{"cells":[{"cell_type":"markdown","metadata":{"id":"qNGJMU5Rflh0"},"source":["# **Wave-U-Net for Background Noise Suppression**\n","\n","**Signal Processing for Artificial Intelligence in Audio - Final Project**\n","\n","Yuval Sheinin\n","\n","A.C\n","\n","O.N\n","\n","---\n","\n","*Note to reader: This notebook was completed collaboratively across several Google Drive environments, for this reason path names are not consistent across cells.*\n"]},{"cell_type":"markdown","metadata":{"id":"7-4VnwhF_c4A"},"source":["# Outline\n","\n","- Dataset:\n","  * Includes all types of noises.\n","  * Noise level set to 20dB only.\n","  * Training dataset composed of train_speech and train_noise; mixing performed.\n","\n","- Model:\n","  * Fine-tuning of U-Net applied to all layers: downsampling and upsampling.\n","\n","\n","- Goals:\n","  * Assess the model's SNR performance. Experiment with reducing background noise on different SNR values and  noise types."]},{"cell_type":"markdown","metadata":{"id":"VpjSoHS4Wv44"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJsK3N2xpsEg"},"outputs":[],"source":["# Imports\n","import zipfile\n","import os\n","import csv\n","import librosa\n","import re\n","import sys\n","import pdb\n","import random\n","import shutil\n","import json\n","import subprocess\n","import numpy as np\n","import soundfile as sf\n","import matplotlib.pyplot as plt\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"44bpF40X_H_f"},"source":["# Setting Up Environment and Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23526,"status":"ok","timestamp":1710866719163,"user":{"displayName":"Omri Newman","userId":"03939389287432353270"},"user_tz":-120},"id":"-lRSwdR2taF2","outputId":"56d0db2f-a286-44c9-d86f-268a5b0a8454"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mount Google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195563,"status":"ok","timestamp":1710868408005,"user":{"displayName":"Omri Newman","userId":"03939389287432353270"},"user_tz":-120},"id":"r3QBhF9i5_Z2","outputId":"c4706b88-1eaf-4513-cdef-e3e54fd6084c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (0.18.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.25.2)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.1)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.10.0)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.2.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (24.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.3.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n","Collecting musdb\n","  Downloading musdb-0.4.2-py2.py3-none-any.whl (13 kB)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from musdb) (1.25.2)\n","Collecting stempeg>=0.2.3 (from musdb)\n","  Downloading stempeg-0.2.3-py3-none-any.whl (963 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyaml (from musdb)\n","  Downloading pyaml-23.12.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from musdb) (4.66.2)\n","Collecting ffmpeg-python>=0.2.0 (from stempeg>=0.2.3->musdb)\n","  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml->musdb) (6.0.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python>=0.2.0->stempeg>=0.2.3->musdb) (0.18.3)\n","Installing collected packages: pyaml, ffmpeg-python, stempeg, musdb\n","Successfully installed ffmpeg-python-0.2.0 musdb-0.4.2 pyaml-23.12.0 stempeg-0.2.3\n","Collecting museval\n","  Downloading museval-0.4.1-py2.py3-none-any.whl (20 kB)\n","Requirement already satisfied: musdb>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from museval) (0.4.2)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from museval) (1.5.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from museval) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from museval) (1.11.4)\n","Collecting simplejson>=3.19.0 (from museval)\n","  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from museval) (0.12.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from museval) (4.19.2)\n","Requirement already satisfied: stempeg>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from musdb>=0.4.0->museval) (0.2.3)\n","Requirement already satisfied: pyaml in /usr/local/lib/python3.10/dist-packages (from musdb>=0.4.0->museval) (23.12.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from musdb>=0.4.0->museval) (4.66.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->museval) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->museval) (2023.4)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->museval) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->museval) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->museval) (0.33.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->museval) (0.18.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->museval) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->museval) (2.21)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->museval) (1.16.0)\n","Requirement already satisfied: ffmpeg-python>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from stempeg>=0.2.3->musdb>=0.4.0->museval) (0.2.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml->musdb>=0.4.0->museval) (6.0.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python>=0.2.0->stempeg>=0.2.3->musdb>=0.4.0->museval) (0.18.3)\n","Installing collected packages: simplejson, museval\n","Successfully installed museval-0.4.1 simplejson-3.19.2\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.2.1+cu121)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision) (12.4.99)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.62.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.6)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.25.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (2.4.0)\n","Collecting resampy\n","  Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from resampy) (1.25.2)\n","Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.10/dist-packages (from resampy) (0.58.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.53->resampy) (0.41.1)\n","Installing collected packages: resampy\n","Successfully installed resampy-0.4.3\n"]}],"source":["# Necessary installation for notebook running\n","!pip3 install future\n","!pip3 install numpy\n","!pip3 install librosa\n","!pip3 install soundfile\n","!pip3 install musdb\n","!pip3 install museval\n","!pip3 install h5py\n","!pip3 install tqdm\n","!pip3 install torch\n","!pip3 install torchvision\n","!pip3 install tensorboard\n","!pip3 install sortedcontainers\n","!pip3 install resampy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":106808,"status":"ok","timestamp":1710691778084,"user":{"displayName":"Yuval Sh","userId":"08473625653578168122"},"user_tz":-120},"id":"61k4NQMWy1ra","outputId":"62c874c1-86a9-4a66-ed7e-3eed2f930325"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","libsndfile1 is already the newest version (1.0.31-2ubuntu0.1).\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 1)) (0.18.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 2)) (1.25.2)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (0.10.1)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 4)) (0.12.1)\n","Collecting musdb (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 5))\n","  Downloading musdb-0.4.2-py2.py3-none-any.whl (13 kB)\n","Collecting museval (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 6))\n","  Downloading museval-0.4.1-py2.py3-none-any.whl (20 kB)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 7)) (3.9.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 8)) (4.66.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9)) (2.2.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 10)) (0.17.1+cu121)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (2.15.2)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 12)) (2.4.0)\n","Collecting resampy (from -r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 13))\n","  Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (3.0.1)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (1.2.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (1.3.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (4.4.2)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (0.58.1)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (1.8.1)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (4.10.0)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (0.3)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (1.0.8)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 4)) (1.16.0)\n","Collecting stempeg>=0.2.3 (from musdb->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 5))\n","  Downloading stempeg-0.2.3-py3-none-any.whl (963 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyaml (from musdb->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 5))\n","  Downloading pyaml-23.12.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from museval->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 6)) (1.5.3)\n","Collecting simplejson>=3.19.0 (from museval->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 6))\n","  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from museval->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 6)) (4.19.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9)) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9)) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9)) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9))\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9))\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9))\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9))\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9))\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9))\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9))\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9))\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9))\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9))\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9)) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9))\n","  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 10)) (9.4.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (1.62.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (3.5.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (2.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (3.0.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 4)) (2.21)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (1.4.0)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (0.41.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->museval->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->museval->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 6)) (2023.4)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (4.2.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (24.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (2024.2.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 3)) (3.3.0)\n","Collecting ffmpeg-python>=0.2.0 (from stempeg>=0.2.3->musdb->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 5))\n","  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (2.1.5)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->museval->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 6)) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->museval->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 6)) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->museval->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 6)) (0.33.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->museval->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 6)) (0.18.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml->musdb->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 5)) (6.0.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 9)) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/requirements.txt (line 11)) (3.2.2)\n","Installing collected packages: simplejson, pyaml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ffmpeg-python, stempeg, resampy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, musdb, museval\n","Successfully installed ffmpeg-python-0.2.0 musdb-0.4.2 museval-0.4.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pyaml-23.12.0 resampy-0.4.3 simplejson-3.19.2 stempeg-0.2.3\n"]}],"source":["### Necessary installations for Wave-U-Net-PyTorch\n","\n","# Install the necessary system packages\n","!apt-get install ffmpeg libsndfile1\n","\n","# Install dependencies from requirements.txt\n","!pip3 install -r \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/requirements.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1351,"status":"ok","timestamp":1710095130782,"user":{"displayName":"Yuval Sh","userId":"08473625653578168122"},"user_tz":-120},"id":"4biKwVD-LfQE","outputId":"4ad341ca-a854-4167-dd20-b023543e9aba"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part\n","fatal: destination path 'MS-SNSD' already exists and is not an empty directory.\n","fatal: destination path 'Wave-U-Net-Pytorch' already exists and is not an empty directory.\n"]}],"source":["### Github Cloning\n","\n","# Change directory to the desired path in Google Drive\n","%cd /content/drive/MyDrive/Reichman University /AI Audio/Final Project\n","\n","# Clone the MS-SNSD GitHub repository\n","!git clone https://github.com/microsoft/MS-SNSD.git\n","\n","# Clone the Wave-U-Net-Pytorch repository from GitHub\n","!git clone https://github.com/f90/Wave-U-Net-Pytorch.git"]},{"cell_type":"markdown","metadata":{"id":"8lky9_GW2X0r"},"source":["# Create Dataset"]},{"cell_type":"markdown","metadata":{"id":"vm_JsoeAvl1e"},"source":[" Run Microsoft code for creating the Noisy files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6igDhfC24dF"},"outputs":[],"source":["noisy_speech_training_dir = r\"/content/drive/My Drive/Reichman University /AI Audio/Final Project/MS-SNSD/NoisySpeech_training\""]},{"cell_type":"markdown","metadata":{"id":"ZAfrcgRhPNK4"},"source":["## Create Training Set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":338,"status":"ok","timestamp":1710233022990,"user":{"displayName":"Yuval Sh","userId":"08473625653578168122"},"user_tz":-120},"id":"BCTPmO5ePLMA","outputId":"faeb3d07-a05e-466e-adb8-ce635b583736"},"outputs":[{"name":"stdout","output_type":"stream","text":["NoisySpeech_training directory found. The script will not be executed.\n"]}],"source":["if os.path.exists(noisy_speech_training_dir):\n","    print(\"NoisySpeech_training directory found. The script will not be executed.\")\n","else:\n","    # Run the noisyspeech_synthesizer script directly by specifying the full path.\n","    # This script utilizes a synthesizer to generate three types of training data: clean_speech_training, noise_training, and noisy_speech_training.\n","    # - clean_speech_training consists of recordings of clean speech used for training.\n","    # - noise_training consists of noise samples used for training. Each clean speech recording is paired with a specified number of noise samples based on the configuration requirements.\n","    #   For example, if the configuration specifies five noise samples for each clean speech recording, then in noisy_speech_training, each clean speech recording will be paired with five noise samples.\n","    #   Therefore, if we have 60 clean speech recordings and a parameter of five, noisy_speech_training will include 300 records, each containing one clean speech recording mixed with five noise samples.\n","    !python \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/MS-SNSD/noisyspeech_synthesizer.py\" --cfg noisyspeech_synthesizer_train.cfg\n"]},{"cell_type":"markdown","metadata":{"id":"_MPclwSEQFLf"},"source":["## Create Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8M7u536QIHl"},"outputs":[],"source":["if os.path.exists(noisy_speech_training_dir):\n","    print(\"NoisySpeech_training directory found. The script will not be executed.\")\n","else:\n","    # Run the noisyspeech_synthesizer script directly by specifying the full path.\n","    # This script utilizes a synthesizer to generate three types of training data: clean_speech_training, noise_training, and noisy_speech_training.\n","    # - clean_speech_training consists of recordings of clean speech used for training.\n","    # - noise_training consists of noise samples used for training. Each clean speech recording is paired with a specified number of noise samples based on the configuration requirements.\n","    #   For example, if the configuration specifies five noise samples for each clean speech recording, then in noisy_speech_training, each clean speech recording will be paired with five noise samples.\n","    #   Therefore, if we have 60 clean speech recordings and a parameter of five, noisy_speech_training will include 300 records, each containing one clean speech recording mixed with five noise samples.\n","    !python \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/MS-SNSD/noisyspeech_synthesizer.py\" --cfg noisyspeech_synthesizer_test.cfg\n"]},{"cell_type":"markdown","metadata":{"id":"WxnLrtIUF-24"},"source":["## Organize the Data to Look Like Musdb18hq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tSDiGgfaaXnX"},"outputs":[],"source":["# Define Constants\n","NUM_OF_SNR_LEVELS = 1  # Number of SNR levels\n","AUDIO_FILE_TYPE = '.wav'  # File extension for audio files\n","\n","def create_sample(sample_num, clean_file, noise_file, dataset_type, dataset_path, clean_speech_path, noise_path, noisy_speech_path):\n","    # Create sample folder\n","    sample_folder = f\"{os.path.splitext(noise_file)[0]}_{os.path.splitext(clean_file)[0]}\"\n","    sample_path = os.path.join(dataset_path, dataset_type, sample_folder)\n","    os.makedirs(sample_path)\n","\n","    # Load clean speech files (loaded as mono by default)\n","    clean_audio, sr = librosa.load(os.path.join(clean_speech_path, clean_file), sr=None, mono=True)\n","    # Convert clean audio to stereo by duplicating across channels\n","    clean_audio_stereo = np.vstack([clean_audio, clean_audio])\n","    # Write the stereo clean audio to the vocals file\n","    sf.write(os.path.join(sample_path, \"vocals.wav\"), clean_audio_stereo.T, sr)\n","\n","    # Create noisy speech filename based on clean and noise filenames\n","    noisy_speech_filename = f\"{noise_file}_{clean_file}\"\n","    noisy_speech_filename = noisy_speech_filename.replace(\".wav\", \"\") + AUDIO_FILE_TYPE\n","\n","    # Load noisy speech files (loaded as mono by default)\n","    noisy_speech, sr = librosa.load(os.path.join(noisy_speech_path, noisy_speech_filename), sr=None, mono=True)\n","    # Convert noisy audio to stereo by duplicating across channels\n","    noisy_audio_stereo = np.vstack([noisy_speech, noisy_speech])\n","    # Write the stereo noisy audio to the mixture file\n","    sf.write(os.path.join(sample_path, \"mixture.wav\"), noisy_audio_stereo.T, sr)\n","\n","    # Load noise file (loaded as mono by default)\n","    noise_audio, sr = librosa.load(os.path.join(noise_path, noise_file), sr=None, mono=True)\n","    # Convert noise audio to stereo by duplicating across channels\n","    noise_audio_stereo = np.vstack([noise_audio, noise_audio])\n","    # Write the stereo noise audio to other, bass, and drums files\n","    sf.write(os.path.join(sample_path, \"other.wav\"), noise_audio_stereo.T, sr)\n","    sf.write(os.path.join(sample_path, \"bass.wav\"), noise_audio_stereo.T, sr)\n","    sf.write(os.path.join(sample_path, \"drums.wav\"), noise_audio_stereo.T, sr)\n","\n","def process_samples(dataset_path, clean_speech_path, noisy_speech_path,  noise_path, dataset_type):\n","\n","    # Check if the dataset folder is empty\n","    if not os.path.exists(os.path.join(dataset_path, dataset_type)):\n","        # Create a folder for train or test based on the argument provided\n","        dataset_type_folder = os.path.join(dataset_path, dataset_type)\n","\n","        # Create a folder for the dataset type\n","        os.makedirs(dataset_type_folder)\n","\n","        clean_files = [file for file in os.listdir(clean_speech_path) if file.startswith(\"clnsp\")]\n","\n","        # Calculate the Total Number of Samples (N)\n","        num_of_clean_samples = len(clean_files)\n","        N = NUM_OF_SNR_LEVELS * num_of_clean_samples\n","        print(\"Total Number of Samples (N):\", N)\n","\n","        # Initialize sample counter\n","        num_of_sample = 1\n","\n","        # Iterate through clean speech files and match with noise files\n","        for clean_file in clean_files:\n","            clean_number_match = re.match(r'clnsp(\\d+).*\\.wav$', clean_file)\n","            clean_number = int(clean_number_match.group(1))\n","            filtered_noise_files = [file for file in os.listdir(noise_path) if file.startswith(f\"noisy{clean_number}_\")]\n","\n","            for noise_file in filtered_noise_files:\n","                create_sample(num_of_sample, clean_file, noise_file, dataset_type,\n","                              dataset_path, clean_speech_path, noise_path, noisy_speech_path)\n","                print(\"Number of Samples Processed:\", num_of_sample)\n","                num_of_sample += 1\n","    else:\n","        print(\"Dataset_type folder already exists. Exiting...\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBcpm5DDttXK"},"outputs":[],"source":["# Define Paths to Source Directories\n","clean_speech_path = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/MS-SNSD/CleanSpeech_training\"\n","noisy_speech_path = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/MS-SNSD/NoisySpeech_training\"\n","noise_path = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/MS-SNSD/Noise_training\"\n","# Define Path to Destination Dataset\n","dataset_path = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/Dataset\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":355245,"status":"ok","timestamp":1710234669758,"user":{"displayName":"Yuval Sh","userId":"08473625653578168122"},"user_tz":-120},"id":"ztkjfr4ZbAas","outputId":"7bfa01f7-fa8d-4887-eefc-c2b34292cd9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Number of Samples (N): 768\n","Number of Samples Processed: 1\n","Number of Samples Processed: 2\n","Number of Samples Processed: 3\n","Number of Samples Processed: 4\n","Number of Samples Processed: 5\n","Number of Samples Processed: 6\n","Number of Samples Processed: 7\n","Number of Samples Processed: 8\n","Number of Samples Processed: 9\n","Number of Samples Processed: 10\n","Number of Samples Processed: 11\n","Number of Samples Processed: 12\n","Number of Samples Processed: 13\n","Number of Samples Processed: 14\n","Number of Samples Processed: 15\n","Number of Samples Processed: 16\n","Number of Samples Processed: 17\n","Number of Samples Processed: 18\n","Number of Samples Processed: 19\n","Number of Samples Processed: 20\n","Number of Samples Processed: 21\n","Number of Samples Processed: 22\n","Number of Samples Processed: 23\n","Number of Samples Processed: 24\n","Number of Samples Processed: 25\n","Number of Samples Processed: 26\n","Number of Samples Processed: 27\n","Number of Samples Processed: 28\n","Number of Samples Processed: 29\n","Number of Samples Processed: 30\n","Number of Samples Processed: 31\n","Number of Samples Processed: 32\n","Number of Samples Processed: 33\n","Number of Samples Processed: 34\n","Number of Samples Processed: 35\n","Number of Samples Processed: 36\n","Number of Samples Processed: 37\n","Number of Samples Processed: 38\n","Number of Samples Processed: 39\n","Number of Samples Processed: 40\n","Number of Samples Processed: 41\n","Number of Samples Processed: 42\n","Number of Samples Processed: 43\n","Number of Samples Processed: 44\n","Number of Samples Processed: 45\n","Number of Samples Processed: 46\n","Number of Samples Processed: 47\n","Number of Samples Processed: 48\n","Number of Samples Processed: 49\n","Number of Samples Processed: 50\n","Number of Samples Processed: 51\n","Number of Samples Processed: 52\n","Number of Samples Processed: 53\n","Number of Samples Processed: 54\n","Number of Samples Processed: 55\n","Number of Samples Processed: 56\n","Number of Samples Processed: 57\n","Number of Samples Processed: 58\n","Number of Samples Processed: 59\n","Number of Samples Processed: 60\n","Number of Samples Processed: 61\n","Number of Samples Processed: 62\n","Number of Samples Processed: 63\n","Number of Samples Processed: 64\n","Number of Samples Processed: 65\n","Number of Samples Processed: 66\n","Number of Samples Processed: 67\n","Number of Samples Processed: 68\n","Number of Samples Processed: 69\n","Number of Samples Processed: 70\n","Number of Samples Processed: 71\n","Number of Samples Processed: 72\n","Number of Samples Processed: 73\n","Number of Samples Processed: 74\n","Number of Samples Processed: 75\n","Number of Samples Processed: 76\n","Number of Samples Processed: 77\n","Number of Samples Processed: 78\n","Number of Samples Processed: 79\n","Number of Samples Processed: 80\n","Number of Samples Processed: 81\n","Number of Samples Processed: 82\n","Number of Samples Processed: 83\n","Number of Samples Processed: 84\n","Number of Samples Processed: 85\n","Number of Samples Processed: 86\n","Number of Samples Processed: 87\n","Number of Samples Processed: 88\n","Number of Samples Processed: 89\n","Number of Samples Processed: 90\n","Number of Samples Processed: 91\n","Number of Samples Processed: 92\n","Number of Samples Processed: 93\n","Number of Samples Processed: 94\n","Number of Samples Processed: 95\n","Number of Samples Processed: 96\n","Number of Samples Processed: 97\n","Number of Samples Processed: 98\n","Number of Samples Processed: 99\n","Number of Samples Processed: 100\n","Number of Samples Processed: 101\n","Number of Samples Processed: 102\n","Number of Samples Processed: 103\n","Number of Samples Processed: 104\n","Number of Samples Processed: 105\n","Number of Samples Processed: 106\n","Number of Samples Processed: 107\n","Number of Samples Processed: 108\n","Number of Samples Processed: 109\n","Number of Samples Processed: 110\n","Number of Samples Processed: 111\n","Number of Samples Processed: 112\n","Number of Samples Processed: 113\n","Number of Samples Processed: 114\n","Number of Samples Processed: 115\n","Number of Samples Processed: 116\n","Number of Samples Processed: 117\n","Number of Samples Processed: 118\n","Number of Samples Processed: 119\n","Number of Samples Processed: 120\n","Number of Samples Processed: 121\n","Number of Samples Processed: 122\n","Number of Samples Processed: 123\n","Number of Samples Processed: 124\n","Number of Samples Processed: 125\n","Number of Samples Processed: 126\n","Number of Samples Processed: 127\n","Number of Samples Processed: 128\n","Number of Samples Processed: 129\n","Number of Samples Processed: 130\n","Number of Samples Processed: 131\n","Number of Samples Processed: 132\n","Number of Samples Processed: 133\n","Number of Samples Processed: 134\n","Number of Samples Processed: 135\n","Number of Samples Processed: 136\n","Number of Samples Processed: 137\n","Number of Samples Processed: 138\n","Number of Samples Processed: 139\n","Number of Samples Processed: 140\n","Number of Samples Processed: 141\n","Number of Samples Processed: 142\n","Number of Samples Processed: 143\n","Number of Samples Processed: 144\n","Number of Samples Processed: 145\n","Number of Samples Processed: 146\n","Number of Samples Processed: 147\n","Number of Samples Processed: 148\n","Number of Samples Processed: 149\n","Number of Samples Processed: 150\n","Number of Samples Processed: 151\n","Number of Samples Processed: 152\n","Number of Samples Processed: 153\n","Number of Samples Processed: 154\n","Number of Samples Processed: 155\n","Number of Samples Processed: 156\n","Number of Samples Processed: 157\n","Number of Samples Processed: 158\n","Number of Samples Processed: 159\n","Number of Samples Processed: 160\n","Number of Samples Processed: 161\n","Number of Samples Processed: 162\n","Number of Samples Processed: 163\n","Number of Samples Processed: 164\n","Number of Samples Processed: 165\n","Number of Samples Processed: 166\n","Number of Samples Processed: 167\n","Number of Samples Processed: 168\n","Number of Samples Processed: 169\n","Number of Samples Processed: 170\n","Number of Samples Processed: 171\n","Number of Samples Processed: 172\n","Number of Samples Processed: 173\n","Number of Samples Processed: 174\n","Number of Samples Processed: 175\n","Number of Samples Processed: 176\n","Number of Samples Processed: 177\n","Number of Samples Processed: 178\n","Number of Samples Processed: 179\n","Number of Samples Processed: 180\n","Number of Samples Processed: 181\n","Number of Samples Processed: 182\n","Number of Samples Processed: 183\n","Number of Samples Processed: 184\n","Number of Samples Processed: 185\n","Number of Samples Processed: 186\n","Number of Samples Processed: 187\n","Number of Samples Processed: 188\n","Number of Samples Processed: 189\n","Number of Samples Processed: 190\n","Number of Samples Processed: 191\n","Number of Samples Processed: 192\n","Number of Samples Processed: 193\n","Number of Samples Processed: 194\n","Number of Samples Processed: 195\n","Number of Samples Processed: 196\n","Number of Samples Processed: 197\n","Number of Samples Processed: 198\n","Number of Samples Processed: 199\n","Number of Samples Processed: 200\n","Number of Samples Processed: 201\n","Number of Samples Processed: 202\n","Number of Samples Processed: 203\n","Number of Samples Processed: 204\n","Number of Samples Processed: 205\n","Number of Samples Processed: 206\n","Number of Samples Processed: 207\n","Number of Samples Processed: 208\n","Number of Samples Processed: 209\n","Number of Samples Processed: 210\n","Number of Samples Processed: 211\n","Number of Samples Processed: 212\n","Number of Samples Processed: 213\n","Number of Samples Processed: 214\n","Number of Samples Processed: 215\n","Number of Samples Processed: 216\n","Number of Samples Processed: 217\n","Number of Samples Processed: 218\n","Number of Samples Processed: 219\n","Number of Samples Processed: 220\n","Number of Samples Processed: 221\n","Number of Samples Processed: 222\n","Number of Samples Processed: 223\n","Number of Samples Processed: 224\n","Number of Samples Processed: 225\n","Number of Samples Processed: 226\n","Number of Samples Processed: 227\n","Number of Samples Processed: 228\n","Number of Samples Processed: 229\n","Number of Samples Processed: 230\n","Number of Samples Processed: 231\n","Number of Samples Processed: 232\n","Number of Samples Processed: 233\n","Number of Samples Processed: 234\n","Number of Samples Processed: 235\n","Number of Samples Processed: 236\n","Number of Samples Processed: 237\n","Number of Samples Processed: 238\n","Number of Samples Processed: 239\n","Number of Samples Processed: 240\n","Number of Samples Processed: 241\n","Number of Samples Processed: 242\n","Number of Samples Processed: 243\n","Number of Samples Processed: 244\n","Number of Samples Processed: 245\n","Number of Samples Processed: 246\n","Number of Samples Processed: 247\n","Number of Samples Processed: 248\n","Number of Samples Processed: 249\n","Number of Samples Processed: 250\n","Number of Samples Processed: 251\n","Number of Samples Processed: 252\n","Number of Samples Processed: 253\n","Number of Samples Processed: 254\n","Number of Samples Processed: 255\n","Number of Samples Processed: 256\n","Number of Samples Processed: 257\n","Number of Samples Processed: 258\n","Number of Samples Processed: 259\n","Number of Samples Processed: 260\n","Number of Samples Processed: 261\n","Number of Samples Processed: 262\n","Number of Samples Processed: 263\n","Number of Samples Processed: 264\n","Number of Samples Processed: 265\n","Number of Samples Processed: 266\n","Number of Samples Processed: 267\n","Number of Samples Processed: 268\n","Number of Samples Processed: 269\n","Number of Samples Processed: 270\n","Number of Samples Processed: 271\n","Number of Samples Processed: 272\n","Number of Samples Processed: 273\n","Number of Samples Processed: 274\n","Number of Samples Processed: 275\n","Number of Samples Processed: 276\n","Number of Samples Processed: 277\n","Number of Samples Processed: 278\n","Number of Samples Processed: 279\n","Number of Samples Processed: 280\n","Number of Samples Processed: 281\n","Number of Samples Processed: 282\n","Number of Samples Processed: 283\n","Number of Samples Processed: 284\n","Number of Samples Processed: 285\n","Number of Samples Processed: 286\n","Number of Samples Processed: 287\n","Number of Samples Processed: 288\n","Number of Samples Processed: 289\n","Number of Samples Processed: 290\n","Number of Samples Processed: 291\n","Number of Samples Processed: 292\n","Number of Samples Processed: 293\n","Number of Samples Processed: 294\n","Number of Samples Processed: 295\n","Number of Samples Processed: 296\n","Number of Samples Processed: 297\n","Number of Samples Processed: 298\n","Number of Samples Processed: 299\n","Number of Samples Processed: 300\n","Number of Samples Processed: 301\n","Number of Samples Processed: 302\n","Number of Samples Processed: 303\n","Number of Samples Processed: 304\n","Number of Samples Processed: 305\n","Number of Samples Processed: 306\n","Number of Samples Processed: 307\n","Number of Samples Processed: 308\n","Number of Samples Processed: 309\n","Number of Samples Processed: 310\n","Number of Samples Processed: 311\n","Number of Samples Processed: 312\n","Number of Samples Processed: 313\n","Number of Samples Processed: 314\n","Number of Samples Processed: 315\n","Number of Samples Processed: 316\n","Number of Samples Processed: 317\n","Number of Samples Processed: 318\n","Number of Samples Processed: 319\n","Number of Samples Processed: 320\n","Number of Samples Processed: 321\n","Number of Samples Processed: 322\n","Number of Samples Processed: 323\n","Number of Samples Processed: 324\n","Number of Samples Processed: 325\n","Number of Samples Processed: 326\n","Number of Samples Processed: 327\n","Number of Samples Processed: 328\n","Number of Samples Processed: 329\n","Number of Samples Processed: 330\n","Number of Samples Processed: 331\n","Number of Samples Processed: 332\n","Number of Samples Processed: 333\n","Number of Samples Processed: 334\n","Number of Samples Processed: 335\n","Number of Samples Processed: 336\n","Number of Samples Processed: 337\n","Number of Samples Processed: 338\n","Number of Samples Processed: 339\n","Number of Samples Processed: 340\n","Number of Samples Processed: 341\n","Number of Samples Processed: 342\n","Number of Samples Processed: 343\n","Number of Samples Processed: 344\n","Number of Samples Processed: 345\n","Number of Samples Processed: 346\n","Number of Samples Processed: 347\n","Number of Samples Processed: 348\n","Number of Samples Processed: 349\n","Number of Samples Processed: 350\n","Number of Samples Processed: 351\n","Number of Samples Processed: 352\n","Number of Samples Processed: 353\n","Number of Samples Processed: 354\n","Number of Samples Processed: 355\n","Number of Samples Processed: 356\n","Number of Samples Processed: 357\n","Number of Samples Processed: 358\n","Number of Samples Processed: 359\n","Number of Samples Processed: 360\n","Number of Samples Processed: 361\n","Number of Samples Processed: 362\n","Number of Samples Processed: 363\n","Number of Samples Processed: 364\n","Number of Samples Processed: 365\n","Number of Samples Processed: 366\n","Number of Samples Processed: 367\n","Number of Samples Processed: 368\n","Number of Samples Processed: 369\n","Number of Samples Processed: 370\n","Number of Samples Processed: 371\n","Number of Samples Processed: 372\n","Number of Samples Processed: 373\n","Number of Samples Processed: 374\n","Number of Samples Processed: 375\n","Number of Samples Processed: 376\n","Number of Samples Processed: 377\n","Number of Samples Processed: 378\n","Number of Samples Processed: 379\n","Number of Samples Processed: 380\n","Number of Samples Processed: 381\n","Number of Samples Processed: 382\n","Number of Samples Processed: 383\n","Number of Samples Processed: 384\n","Number of Samples Processed: 385\n","Number of Samples Processed: 386\n","Number of Samples Processed: 387\n","Number of Samples Processed: 388\n","Number of Samples Processed: 389\n","Number of Samples Processed: 390\n","Number of Samples Processed: 391\n","Number of Samples Processed: 392\n","Number of Samples Processed: 393\n","Number of Samples Processed: 394\n","Number of Samples Processed: 395\n","Number of Samples Processed: 396\n","Number of Samples Processed: 397\n","Number of Samples Processed: 398\n","Number of Samples Processed: 399\n","Number of Samples Processed: 400\n","Number of Samples Processed: 401\n","Number of Samples Processed: 402\n","Number of Samples Processed: 403\n","Number of Samples Processed: 404\n","Number of Samples Processed: 405\n","Number of Samples Processed: 406\n","Number of Samples Processed: 407\n","Number of Samples Processed: 408\n","Number of Samples Processed: 409\n","Number of Samples Processed: 410\n","Number of Samples Processed: 411\n","Number of Samples Processed: 412\n","Number of Samples Processed: 413\n","Number of Samples Processed: 414\n","Number of Samples Processed: 415\n","Number of Samples Processed: 416\n","Number of Samples Processed: 417\n","Number of Samples Processed: 418\n","Number of Samples Processed: 419\n","Number of Samples Processed: 420\n","Number of Samples Processed: 421\n","Number of Samples Processed: 422\n","Number of Samples Processed: 423\n","Number of Samples Processed: 424\n","Number of Samples Processed: 425\n","Number of Samples Processed: 426\n","Number of Samples Processed: 427\n","Number of Samples Processed: 428\n","Number of Samples Processed: 429\n","Number of Samples Processed: 430\n","Number of Samples Processed: 431\n","Number of Samples Processed: 432\n","Number of Samples Processed: 433\n","Number of Samples Processed: 434\n","Number of Samples Processed: 435\n","Number of Samples Processed: 436\n","Number of Samples Processed: 437\n","Number of Samples Processed: 438\n","Number of Samples Processed: 439\n","Number of Samples Processed: 440\n","Number of Samples Processed: 441\n","Number of Samples Processed: 442\n","Number of Samples Processed: 443\n","Number of Samples Processed: 444\n","Number of Samples Processed: 445\n","Number of Samples Processed: 446\n","Number of Samples Processed: 447\n","Number of Samples Processed: 448\n","Number of Samples Processed: 449\n","Number of Samples Processed: 450\n","Number of Samples Processed: 451\n","Number of Samples Processed: 452\n","Number of Samples Processed: 453\n","Number of Samples Processed: 454\n","Number of Samples Processed: 455\n","Number of Samples Processed: 456\n","Number of Samples Processed: 457\n","Number of Samples Processed: 458\n","Number of Samples Processed: 459\n","Number of Samples Processed: 460\n","Number of Samples Processed: 461\n","Number of Samples Processed: 462\n","Number of Samples Processed: 463\n","Number of Samples Processed: 464\n","Number of Samples Processed: 465\n","Number of Samples Processed: 466\n","Number of Samples Processed: 467\n","Number of Samples Processed: 468\n","Number of Samples Processed: 469\n","Number of Samples Processed: 470\n","Number of Samples Processed: 471\n","Number of Samples Processed: 472\n","Number of Samples Processed: 473\n","Number of Samples Processed: 474\n","Number of Samples Processed: 475\n","Number of Samples Processed: 476\n","Number of Samples Processed: 477\n","Number of Samples Processed: 478\n","Number of Samples Processed: 479\n","Number of Samples Processed: 480\n","Number of Samples Processed: 481\n","Number of Samples Processed: 482\n","Number of Samples Processed: 483\n","Number of Samples Processed: 484\n","Number of Samples Processed: 485\n","Number of Samples Processed: 486\n","Number of Samples Processed: 487\n","Number of Samples Processed: 488\n","Number of Samples Processed: 489\n","Number of Samples Processed: 490\n","Number of Samples Processed: 491\n","Number of Samples Processed: 492\n","Number of Samples Processed: 493\n","Number of Samples Processed: 494\n","Number of Samples Processed: 495\n","Number of Samples Processed: 496\n","Number of Samples Processed: 497\n","Number of Samples Processed: 498\n","Number of Samples Processed: 499\n","Number of Samples Processed: 500\n","Number of Samples Processed: 501\n","Number of Samples Processed: 502\n","Number of Samples Processed: 503\n","Number of Samples Processed: 504\n","Number of Samples Processed: 505\n","Number of Samples Processed: 506\n","Number of Samples Processed: 507\n","Number of Samples Processed: 508\n","Number of Samples Processed: 509\n","Number of Samples Processed: 510\n","Number of Samples Processed: 511\n","Number of Samples Processed: 512\n","Number of Samples Processed: 513\n","Number of Samples Processed: 514\n","Number of Samples Processed: 515\n","Number of Samples Processed: 516\n","Number of Samples Processed: 517\n","Number of Samples Processed: 518\n","Number of Samples Processed: 519\n","Number of Samples Processed: 520\n","Number of Samples Processed: 521\n","Number of Samples Processed: 522\n","Number of Samples Processed: 523\n","Number of Samples Processed: 524\n","Number of Samples Processed: 525\n","Number of Samples Processed: 526\n","Number of Samples Processed: 527\n","Number of Samples Processed: 528\n","Number of Samples Processed: 529\n","Number of Samples Processed: 530\n","Number of Samples Processed: 531\n","Number of Samples Processed: 532\n","Number of Samples Processed: 533\n","Number of Samples Processed: 534\n","Number of Samples Processed: 535\n","Number of Samples Processed: 536\n","Number of Samples Processed: 537\n","Number of Samples Processed: 538\n","Number of Samples Processed: 539\n","Number of Samples Processed: 540\n","Number of Samples Processed: 541\n","Number of Samples Processed: 542\n","Number of Samples Processed: 543\n","Number of Samples Processed: 544\n","Number of Samples Processed: 545\n","Number of Samples Processed: 546\n","Number of Samples Processed: 547\n","Number of Samples Processed: 548\n","Number of Samples Processed: 549\n","Number of Samples Processed: 550\n","Number of Samples Processed: 551\n","Number of Samples Processed: 552\n","Number of Samples Processed: 553\n","Number of Samples Processed: 554\n","Number of Samples Processed: 555\n","Number of Samples Processed: 556\n","Number of Samples Processed: 557\n","Number of Samples Processed: 558\n","Number of Samples Processed: 559\n","Number of Samples Processed: 560\n","Number of Samples Processed: 561\n","Number of Samples Processed: 562\n","Number of Samples Processed: 563\n","Number of Samples Processed: 564\n","Number of Samples Processed: 565\n","Number of Samples Processed: 566\n","Number of Samples Processed: 567\n","Number of Samples Processed: 568\n","Number of Samples Processed: 569\n","Number of Samples Processed: 570\n","Number of Samples Processed: 571\n","Number of Samples Processed: 572\n","Number of Samples Processed: 573\n","Number of Samples Processed: 574\n","Number of Samples Processed: 575\n","Number of Samples Processed: 576\n","Number of Samples Processed: 577\n","Number of Samples Processed: 578\n","Number of Samples Processed: 579\n","Number of Samples Processed: 580\n","Number of Samples Processed: 581\n","Number of Samples Processed: 582\n","Number of Samples Processed: 583\n","Number of Samples Processed: 584\n","Number of Samples Processed: 585\n","Number of Samples Processed: 586\n","Number of Samples Processed: 587\n","Number of Samples Processed: 588\n","Number of Samples Processed: 589\n","Number of Samples Processed: 590\n","Number of Samples Processed: 591\n","Number of Samples Processed: 592\n","Number of Samples Processed: 593\n","Number of Samples Processed: 594\n","Number of Samples Processed: 595\n","Number of Samples Processed: 596\n","Number of Samples Processed: 597\n","Number of Samples Processed: 598\n","Number of Samples Processed: 599\n","Number of Samples Processed: 600\n","Number of Samples Processed: 601\n","Number of Samples Processed: 602\n","Number of Samples Processed: 603\n","Number of Samples Processed: 604\n","Number of Samples Processed: 605\n","Number of Samples Processed: 606\n","Number of Samples Processed: 607\n","Number of Samples Processed: 608\n","Number of Samples Processed: 609\n","Number of Samples Processed: 610\n","Number of Samples Processed: 611\n","Number of Samples Processed: 612\n","Number of Samples Processed: 613\n","Number of Samples Processed: 614\n","Number of Samples Processed: 615\n","Number of Samples Processed: 616\n","Number of Samples Processed: 617\n","Number of Samples Processed: 618\n","Number of Samples Processed: 619\n","Number of Samples Processed: 620\n","Number of Samples Processed: 621\n","Number of Samples Processed: 622\n","Number of Samples Processed: 623\n","Number of Samples Processed: 624\n","Number of Samples Processed: 625\n","Number of Samples Processed: 626\n","Number of Samples Processed: 627\n","Number of Samples Processed: 628\n","Number of Samples Processed: 629\n","Number of Samples Processed: 630\n","Number of Samples Processed: 631\n","Number of Samples Processed: 632\n","Number of Samples Processed: 633\n","Number of Samples Processed: 634\n","Number of Samples Processed: 635\n","Number of Samples Processed: 636\n","Number of Samples Processed: 637\n","Number of Samples Processed: 638\n","Number of Samples Processed: 639\n","Number of Samples Processed: 640\n","Number of Samples Processed: 641\n","Number of Samples Processed: 642\n","Number of Samples Processed: 643\n","Number of Samples Processed: 644\n","Number of Samples Processed: 645\n","Number of Samples Processed: 646\n","Number of Samples Processed: 647\n","Number of Samples Processed: 648\n","Number of Samples Processed: 649\n","Number of Samples Processed: 650\n","Number of Samples Processed: 651\n","Number of Samples Processed: 652\n","Number of Samples Processed: 653\n","Number of Samples Processed: 654\n","Number of Samples Processed: 655\n","Number of Samples Processed: 656\n","Number of Samples Processed: 657\n","Number of Samples Processed: 658\n","Number of Samples Processed: 659\n","Number of Samples Processed: 660\n","Number of Samples Processed: 661\n","Number of Samples Processed: 662\n","Number of Samples Processed: 663\n","Number of Samples Processed: 664\n","Number of Samples Processed: 665\n","Number of Samples Processed: 666\n","Number of Samples Processed: 667\n","Number of Samples Processed: 668\n","Number of Samples Processed: 669\n","Number of Samples Processed: 670\n","Number of Samples Processed: 671\n","Number of Samples Processed: 672\n","Number of Samples Processed: 673\n","Number of Samples Processed: 674\n","Number of Samples Processed: 675\n","Number of Samples Processed: 676\n","Number of Samples Processed: 677\n","Number of Samples Processed: 678\n","Number of Samples Processed: 679\n","Number of Samples Processed: 680\n","Number of Samples Processed: 681\n","Number of Samples Processed: 682\n","Number of Samples Processed: 683\n","Number of Samples Processed: 684\n","Number of Samples Processed: 685\n","Number of Samples Processed: 686\n","Number of Samples Processed: 687\n","Number of Samples Processed: 688\n","Number of Samples Processed: 689\n","Number of Samples Processed: 690\n","Number of Samples Processed: 691\n","Number of Samples Processed: 692\n","Number of Samples Processed: 693\n","Number of Samples Processed: 694\n","Number of Samples Processed: 695\n","Number of Samples Processed: 696\n","Number of Samples Processed: 697\n","Number of Samples Processed: 698\n","Number of Samples Processed: 699\n","Number of Samples Processed: 700\n","Number of Samples Processed: 701\n","Number of Samples Processed: 702\n","Number of Samples Processed: 703\n","Number of Samples Processed: 704\n","Number of Samples Processed: 705\n","Number of Samples Processed: 706\n","Number of Samples Processed: 707\n","Number of Samples Processed: 708\n","Number of Samples Processed: 709\n","Number of Samples Processed: 710\n","Number of Samples Processed: 711\n","Number of Samples Processed: 712\n","Number of Samples Processed: 713\n","Number of Samples Processed: 714\n","Number of Samples Processed: 715\n","Number of Samples Processed: 716\n","Number of Samples Processed: 717\n","Number of Samples Processed: 718\n","Number of Samples Processed: 719\n","Number of Samples Processed: 720\n","Number of Samples Processed: 721\n","Number of Samples Processed: 722\n","Number of Samples Processed: 723\n","Number of Samples Processed: 724\n","Number of Samples Processed: 725\n","Number of Samples Processed: 726\n","Number of Samples Processed: 727\n","Number of Samples Processed: 728\n","Number of Samples Processed: 729\n","Number of Samples Processed: 730\n","Number of Samples Processed: 731\n","Number of Samples Processed: 732\n","Number of Samples Processed: 733\n","Number of Samples Processed: 734\n","Number of Samples Processed: 735\n","Number of Samples Processed: 736\n","Number of Samples Processed: 737\n","Number of Samples Processed: 738\n","Number of Samples Processed: 739\n","Number of Samples Processed: 740\n","Number of Samples Processed: 741\n","Number of Samples Processed: 742\n","Number of Samples Processed: 743\n","Number of Samples Processed: 744\n","Number of Samples Processed: 745\n","Number of Samples Processed: 746\n","Number of Samples Processed: 747\n","Number of Samples Processed: 748\n","Number of Samples Processed: 749\n","Number of Samples Processed: 750\n","Number of Samples Processed: 751\n","Number of Samples Processed: 752\n","Number of Samples Processed: 753\n","Number of Samples Processed: 754\n","Number of Samples Processed: 755\n","Number of Samples Processed: 756\n","Number of Samples Processed: 757\n","Number of Samples Processed: 758\n","Number of Samples Processed: 759\n","Number of Samples Processed: 760\n","Number of Samples Processed: 761\n","Number of Samples Processed: 762\n","Number of Samples Processed: 763\n","Number of Samples Processed: 764\n","Number of Samples Processed: 765\n","Number of Samples Processed: 766\n","Number of Samples Processed: 767\n","Number of Samples Processed: 768\n"]}],"source":["# Run the script\n","process_samples(dataset_path, clean_speech_path, noisy_speech_path, noise_path, 'train')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91094,"status":"ok","timestamp":1710234887603,"user":{"displayName":"Yuval Sh","userId":"08473625653578168122"},"user_tz":-120},"id":"PmbMNKV8bCHO","outputId":"c28bbd56-ea7b-4603-8de1-4b989721a8a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Number of Samples (N): 190\n","Number of Samples Processed: 1\n","Number of Samples Processed: 2\n","Number of Samples Processed: 3\n","Number of Samples Processed: 4\n","Number of Samples Processed: 5\n","Number of Samples Processed: 6\n","Number of Samples Processed: 7\n","Number of Samples Processed: 8\n","Number of Samples Processed: 9\n","Number of Samples Processed: 10\n","Number of Samples Processed: 11\n","Number of Samples Processed: 12\n","Number of Samples Processed: 13\n","Number of Samples Processed: 14\n","Number of Samples Processed: 15\n","Number of Samples Processed: 16\n","Number of Samples Processed: 17\n","Number of Samples Processed: 18\n","Number of Samples Processed: 19\n","Number of Samples Processed: 20\n","Number of Samples Processed: 21\n","Number of Samples Processed: 22\n","Number of Samples Processed: 23\n","Number of Samples Processed: 24\n","Number of Samples Processed: 25\n","Number of Samples Processed: 26\n","Number of Samples Processed: 27\n","Number of Samples Processed: 28\n","Number of Samples Processed: 29\n","Number of Samples Processed: 30\n","Number of Samples Processed: 31\n","Number of Samples Processed: 32\n","Number of Samples Processed: 33\n","Number of Samples Processed: 34\n","Number of Samples Processed: 35\n","Number of Samples Processed: 36\n","Number of Samples Processed: 37\n","Number of Samples Processed: 38\n","Number of Samples Processed: 39\n","Number of Samples Processed: 40\n","Number of Samples Processed: 41\n","Number of Samples Processed: 42\n","Number of Samples Processed: 43\n","Number of Samples Processed: 44\n","Number of Samples Processed: 45\n","Number of Samples Processed: 46\n","Number of Samples Processed: 47\n","Number of Samples Processed: 48\n","Number of Samples Processed: 49\n","Number of Samples Processed: 50\n","Number of Samples Processed: 51\n","Number of Samples Processed: 52\n","Number of Samples Processed: 53\n","Number of Samples Processed: 54\n","Number of Samples Processed: 55\n","Number of Samples Processed: 56\n","Number of Samples Processed: 57\n","Number of Samples Processed: 58\n","Number of Samples Processed: 59\n","Number of Samples Processed: 60\n","Number of Samples Processed: 61\n","Number of Samples Processed: 62\n","Number of Samples Processed: 63\n","Number of Samples Processed: 64\n","Number of Samples Processed: 65\n","Number of Samples Processed: 66\n","Number of Samples Processed: 67\n","Number of Samples Processed: 68\n","Number of Samples Processed: 69\n","Number of Samples Processed: 70\n","Number of Samples Processed: 71\n","Number of Samples Processed: 72\n","Number of Samples Processed: 73\n","Number of Samples Processed: 74\n","Number of Samples Processed: 75\n","Number of Samples Processed: 76\n","Number of Samples Processed: 77\n","Number of Samples Processed: 78\n","Number of Samples Processed: 79\n","Number of Samples Processed: 80\n","Number of Samples Processed: 81\n","Number of Samples Processed: 82\n","Number of Samples Processed: 83\n","Number of Samples Processed: 84\n","Number of Samples Processed: 85\n","Number of Samples Processed: 86\n","Number of Samples Processed: 87\n","Number of Samples Processed: 88\n","Number of Samples Processed: 89\n","Number of Samples Processed: 90\n","Number of Samples Processed: 91\n","Number of Samples Processed: 92\n","Number of Samples Processed: 93\n","Number of Samples Processed: 94\n","Number of Samples Processed: 95\n","Number of Samples Processed: 96\n","Number of Samples Processed: 97\n","Number of Samples Processed: 98\n","Number of Samples Processed: 99\n","Number of Samples Processed: 100\n","Number of Samples Processed: 101\n","Number of Samples Processed: 102\n","Number of Samples Processed: 103\n","Number of Samples Processed: 104\n","Number of Samples Processed: 105\n","Number of Samples Processed: 106\n","Number of Samples Processed: 107\n","Number of Samples Processed: 108\n","Number of Samples Processed: 109\n","Number of Samples Processed: 110\n","Number of Samples Processed: 111\n","Number of Samples Processed: 112\n","Number of Samples Processed: 113\n","Number of Samples Processed: 114\n","Number of Samples Processed: 115\n","Number of Samples Processed: 116\n","Number of Samples Processed: 117\n","Number of Samples Processed: 118\n","Number of Samples Processed: 119\n","Number of Samples Processed: 120\n","Number of Samples Processed: 121\n","Number of Samples Processed: 122\n","Number of Samples Processed: 123\n","Number of Samples Processed: 124\n","Number of Samples Processed: 125\n","Number of Samples Processed: 126\n","Number of Samples Processed: 127\n","Number of Samples Processed: 128\n","Number of Samples Processed: 129\n","Number of Samples Processed: 130\n","Number of Samples Processed: 131\n","Number of Samples Processed: 132\n","Number of Samples Processed: 133\n","Number of Samples Processed: 134\n","Number of Samples Processed: 135\n","Number of Samples Processed: 136\n","Number of Samples Processed: 137\n","Number of Samples Processed: 138\n","Number of Samples Processed: 139\n","Number of Samples Processed: 140\n","Number of Samples Processed: 141\n","Number of Samples Processed: 142\n","Number of Samples Processed: 143\n","Number of Samples Processed: 144\n","Number of Samples Processed: 145\n","Number of Samples Processed: 146\n","Number of Samples Processed: 147\n","Number of Samples Processed: 148\n","Number of Samples Processed: 149\n","Number of Samples Processed: 150\n","Number of Samples Processed: 151\n","Number of Samples Processed: 152\n","Number of Samples Processed: 153\n","Number of Samples Processed: 154\n","Number of Samples Processed: 155\n","Number of Samples Processed: 156\n","Number of Samples Processed: 157\n","Number of Samples Processed: 158\n","Number of Samples Processed: 159\n","Number of Samples Processed: 160\n","Number of Samples Processed: 161\n","Number of Samples Processed: 162\n","Number of Samples Processed: 163\n","Number of Samples Processed: 164\n","Number of Samples Processed: 165\n","Number of Samples Processed: 166\n","Number of Samples Processed: 167\n","Number of Samples Processed: 168\n","Number of Samples Processed: 169\n","Number of Samples Processed: 170\n","Number of Samples Processed: 171\n","Number of Samples Processed: 172\n","Number of Samples Processed: 173\n","Number of Samples Processed: 174\n","Number of Samples Processed: 175\n","Number of Samples Processed: 176\n","Number of Samples Processed: 177\n","Number of Samples Processed: 178\n","Number of Samples Processed: 179\n","Number of Samples Processed: 180\n","Number of Samples Processed: 181\n","Number of Samples Processed: 182\n","Number of Samples Processed: 183\n","Number of Samples Processed: 184\n","Number of Samples Processed: 185\n","Number of Samples Processed: 186\n","Number of Samples Processed: 187\n","Number of Samples Processed: 188\n","Number of Samples Processed: 189\n","Number of Samples Processed: 190\n"]}],"source":["# Run the script\n","process_samples(dataset_path, clean_speech_path, noisy_speech_path, noise_path, 'test')"]},{"cell_type":"markdown","metadata":{"id":"PSo0-dwe5kuu"},"source":["## Shuffle\n","\n","Shuffle original data and create new train and test directories. This allows for a more robust training of the model, while avoiding data leakage.  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":362,"status":"ok","timestamp":1710235752830,"user":{"displayName":"Yuval Sh","userId":"08473625653578168122"},"user_tz":-120},"id":"YPNvxBhW5naL","outputId":"b6f7898e-b2df-46e3-b3fd-2df3b06f882f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sorry, the data is protected.\n"]}],"source":["# Constants\n","START_NUM = 769\n","SNRDB_VALUE = 20.0  # Default SNRdb value\n","\n","\n","def rename_test_subdirectories(test_dir, start_num, snrdb_value):\n","    \"\"\"\n","    Rename test subdirectories based on specified naming conventions.\n","    \"\"\"\n","    sorted_subdirs = sorted(os.listdir(test_dir), key=lambda x: int(x.split('noisy')[1].split('_')[0]))\n","\n","    for index, subdir in enumerate(sorted_subdirs, start=start_num):\n","        old_path = os.path.join(test_dir, subdir)\n","        new_dir_name = f\"noisy{index}_SNRdb_{snrdb_value}_clnsp{index}\"\n","        new_path = os.path.join(test_dir, new_dir_name)\n","        os.rename(old_path, new_path)\n","\n","\n","def list_subfolder_names(directory):\n","    \"\"\"\n","    List subfolder names in the specified directory.\n","    \"\"\"\n","    subfolders = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n","    return subfolders\n","\n","\n","def reorganize_folders(src_folder, dest_folder, subdir_list):\n","    \"\"\"\n","    Move subdirectories from source to destination based on the provided list.\n","    \"\"\"\n","    os.makedirs(dest_folder, exist_ok=True)\n","\n","    for item in os.listdir(src_folder):\n","        item_path = os.path.join(src_folder, item)\n","\n","        if os.path.isdir(item_path) and item not in subdir_list:\n","            dest_path = os.path.join(dest_folder, item)\n","            shutil.move(item_path, dest_path)\n","\n","\n","def combine_and_split_dataset(base_directory, test_size=0.2):\n","    \"\"\"\n","    Combine and split dataset into train and test sets.\n","    \"\"\"\n","    train_dir = os.path.join(base_directory, 'train')\n","    test_dir = os.path.join(base_directory, 'test')\n","\n","    rename_test_subdirectories(test_dir, START_NUM, SNRDB_VALUE)\n","\n","    train_subfolders = list_subfolder_names(train_dir)\n","    test_subfolders = list_subfolder_names(test_dir)\n","\n","    all_subfolders = train_subfolders + test_subfolders\n","    all_subfolders_shuffled = shuffle(all_subfolders)\n","\n","    train_subdirs, test_subdirs = train_test_split(all_subfolders_shuffled, test_size=test_size)\n","\n","    reorganize_folders(train_dir, test_dir, train_subdirs)\n","    reorganize_folders(test_dir, train_dir, test_subdirs)\n","\n","\n","if __name__ == \"__main__\":\n","    data_dir = \"/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/Dataset\"\n","    PROTECT = 1\n","    if PROTECT == 1:\n","        print('Sorry, the data is protected.')\n","    else:\n","        combine_and_split_dataset(data_dir, test_size=0.2)"]},{"cell_type":"markdown","metadata":{"id":"tKAFy_dnrQJo"},"source":["# Train (Fine tune)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6210164,"status":"ok","timestamp":1710441125621,"user":{"displayName":"Yuval Sh","userId":"08473625653578168122"},"user_tz":-120},"id":"D_G93babrT9J","outputId":"9bfbec99-d269-4b7d-c752-ec6ae76fec18"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-03-14 16:48:40.923666: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-14 16:48:40.923902: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-14 16:48:41.057195: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-03-14 16:48:41.146371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-03-14 16:48:43.433561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","model:  DataParallel(\n","  (module): Waveunet(\n","    (waveunets): ModuleDict(\n","      (bass): Module(\n","        (downsampling_blocks): ModuleList(\n","          (0): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (1): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (2): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (3): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (4): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","        )\n","        (upsampling_blocks): ModuleList(\n","          (0): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (1): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (2): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (3): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (4): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","        )\n","        (bottlenecks): ModuleList(\n","          (0): ConvLayer(\n","            (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n","            (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n","          )\n","        )\n","        (output_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n","      )\n","      (drums): Module(\n","        (downsampling_blocks): ModuleList(\n","          (0): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (1): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (2): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (3): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (4): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","        )\n","        (upsampling_blocks): ModuleList(\n","          (0): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (1): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (2): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (3): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (4): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","        )\n","        (bottlenecks): ModuleList(\n","          (0): ConvLayer(\n","            (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n","            (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n","          )\n","        )\n","        (output_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n","      )\n","      (other): Module(\n","        (downsampling_blocks): ModuleList(\n","          (0): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (1): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (2): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (3): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (4): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","        )\n","        (upsampling_blocks): ModuleList(\n","          (0): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (1): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (2): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (3): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (4): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","        )\n","        (bottlenecks): ModuleList(\n","          (0): ConvLayer(\n","            (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n","            (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n","          )\n","        )\n","        (output_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n","      )\n","      (vocals): Module(\n","        (downsampling_blocks): ModuleList(\n","          (0): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (1): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (2): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (3): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","          (4): DownsamplingBlock(\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n","              )\n","            )\n","            (downconv): Resample1d()\n","          )\n","        )\n","        (upsampling_blocks): ModuleList(\n","          (0): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (1): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (2): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (3): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","          (4): UpsamplingBlock(\n","            (upconv): Resample1d()\n","            (pre_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n","              )\n","            )\n","            (post_shortcut_convs): ModuleList(\n","              (0): ConvLayer(\n","                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n","                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n","              )\n","            )\n","          )\n","        )\n","        (bottlenecks): ModuleList(\n","          (0): ConvLayer(\n","            (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n","            (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n","          )\n","        )\n","        (output_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n","      )\n","    )\n","  )\n",")\n","parameter count:  70148232\n","Step 132065\n","Loading train set...\n","Loading test set...\n","Continuing training full model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Trainable parameters count:  29713188\n","TRAINING START\n","Training one epoch from iteration 132065\n","97it [00:20,  4.69it/s]\n","Current loss: 0.0074: : 888it [01:16, 11.63it/s]           \n","VALIDATION FINISHED: LOSS: 0.007358191017038393\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109]\n","Validation Loss: [0.007358191017038393]\n","Training one epoch from iteration 132162\n","97it [00:19,  4.90it/s]\n","Current loss: 0.0071: : 888it [01:13, 12.11it/s]           \n","VALIDATION FINISHED: LOSS: 0.007074328591975776\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776]\n","Training one epoch from iteration 132259\n","97it [00:19,  4.86it/s]\n","Current loss: 0.0069: : 888it [01:13, 12.08it/s]           \n","VALIDATION FINISHED: LOSS: 0.006917470385454729\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729]\n","Training one epoch from iteration 132356\n","97it [00:19,  4.87it/s]\n","Current loss: 0.0067: : 888it [01:13, 12.07it/s]           \n","VALIDATION FINISHED: LOSS: 0.0067300539674040255\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255]\n","Training one epoch from iteration 132453\n","97it [00:19,  4.91it/s]\n","Current loss: 0.0066: : 888it [01:13, 12.06it/s]           \n","VALIDATION FINISHED: LOSS: 0.006604816317154504\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504]\n","Training one epoch from iteration 132550\n","97it [00:19,  4.91it/s]\n","Current loss: 0.0065: : 888it [01:13, 12.03it/s]           \n","VALIDATION FINISHED: LOSS: 0.006526375998955784\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784]\n","Training one epoch from iteration 132647\n","97it [00:19,  4.93it/s]\n","Current loss: 0.0064: : 888it [01:14, 11.88it/s]           \n","VALIDATION FINISHED: LOSS: 0.006416785798448785\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785]\n","Training one epoch from iteration 132744\n","97it [00:19,  4.90it/s]\n","Current loss: 0.0063: : 888it [01:14, 11.95it/s]\n","VALIDATION FINISHED: LOSS: 0.0063399013292059075\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075]\n","Training one epoch from iteration 132841\n","97it [00:19,  4.88it/s]\n","Current loss: 0.0063: : 888it [01:13, 12.01it/s]           \n","VALIDATION FINISHED: LOSS: 0.0062716078727143194\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194]\n","Training one epoch from iteration 132938\n","97it [00:19,  4.91it/s]\n","Current loss: 0.0062: : 888it [01:13, 12.05it/s]           \n","VALIDATION FINISHED: LOSS: 0.0062333967902190035\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035]\n","Training one epoch from iteration 133035\n","97it [00:20,  4.84it/s]\n","Current loss: 0.0062: : 888it [01:13, 12.02it/s]\n","VALIDATION FINISHED: LOSS: 0.006204682198192657\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657]\n","Training one epoch from iteration 133132\n","97it [00:19,  4.91it/s]\n","Current loss: 0.0061: : 888it [01:14, 11.98it/s]           \n","VALIDATION FINISHED: LOSS: 0.006149176460206257\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257]\n","Training one epoch from iteration 133229\n","97it [00:19,  4.88it/s]\n","Current loss: 0.0061: : 888it [01:14, 12.00it/s]           \n","VALIDATION FINISHED: LOSS: 0.006088839380567699\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699]\n","Training one epoch from iteration 133326\n","97it [00:20,  4.82it/s]\n","Current loss: 0.0061: : 888it [01:14, 11.96it/s]           \n","VALIDATION FINISHED: LOSS: 0.006050824877797617\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617]\n","Training one epoch from iteration 133423\n","97it [00:19,  4.89it/s]\n","Current loss: 0.0061: : 888it [01:14, 11.95it/s]           \n","VALIDATION FINISHED: LOSS: 0.006055279955035077\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077]\n","Training one epoch from iteration 133520\n","97it [00:19,  4.88it/s]\n","Current loss: 0.0060: : 888it [01:13, 12.03it/s]           \n","VALIDATION FINISHED: LOSS: 0.005991470785669951\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951]\n","Training one epoch from iteration 133617\n","97it [00:19,  4.88it/s]\n","Current loss: 0.0060: : 888it [01:13, 12.03it/s]           \n","VALIDATION FINISHED: LOSS: 0.005958879528292411\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411]\n","Training one epoch from iteration 133714\n","97it [00:19,  4.89it/s]\n","Current loss: 0.0060: : 888it [01:14, 11.97it/s]           \n","VALIDATION FINISHED: LOSS: 0.005980228455784422\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422]\n","Training one epoch from iteration 133811\n","97it [00:19,  4.89it/s]\n","Current loss: 0.0060: : 888it [01:14, 11.94it/s]           \n","VALIDATION FINISHED: LOSS: 0.00595999179816457\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457]\n","Training one epoch from iteration 133908\n","97it [00:19,  4.93it/s]\n","Current loss: 0.0059: : 888it [01:14, 11.86it/s]\n","VALIDATION FINISHED: LOSS: 0.005937366131121529\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529]\n","Training one epoch from iteration 134005\n","97it [00:19,  4.91it/s]\n","Current loss: 0.0059: : 888it [01:14, 11.93it/s]           \n","VALIDATION FINISHED: LOSS: 0.005945601021799654\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654]\n","Training one epoch from iteration 134102\n","97it [00:19,  4.88it/s]\n","Current loss: 0.0059: : 888it [01:14, 11.97it/s]           \n","VALIDATION FINISHED: LOSS: 0.005942237158337408\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408]\n","Training one epoch from iteration 134199\n","97it [00:20,  4.80it/s]\n","Current loss: 0.0059: : 888it [01:14, 11.98it/s]           \n","VALIDATION FINISHED: LOSS: 0.005902864902226782\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782]\n","Training one epoch from iteration 134296\n","97it [00:19,  4.91it/s]\n","Current loss: 0.0059: : 888it [01:14, 11.88it/s]\n","VALIDATION FINISHED: LOSS: 0.005893288331650629\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629]\n","Training one epoch from iteration 134393\n","97it [00:19,  4.89it/s]\n","Current loss: 0.0059: : 888it [01:14, 11.86it/s]\n","VALIDATION FINISHED: LOSS: 0.005907530258965538\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538]\n","Training one epoch from iteration 134490\n","97it [00:19,  4.92it/s]\n","Current loss: 0.0059: : 888it [01:14, 11.88it/s]           \n","VALIDATION FINISHED: LOSS: 0.00585464408884962\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962]\n","Training one epoch from iteration 134587\n","97it [00:19,  4.87it/s]\n","Current loss: 0.0058: : 888it [01:14, 11.94it/s]           \n","VALIDATION FINISHED: LOSS: 0.00583504373286347\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347]\n","Training one epoch from iteration 134684\n","97it [00:19,  4.88it/s]\n","Current loss: 0.0059: : 888it [01:14, 11.92it/s]           \n","VALIDATION FINISHED: LOSS: 0.0058915777578173855\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855]\n","Training one epoch from iteration 134781\n","97it [00:20,  4.83it/s]\n","Current loss: 0.0058: : 888it [01:14, 11.88it/s]           \n","VALIDATION FINISHED: LOSS: 0.005831635488681742\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742]\n","Training one epoch from iteration 134878\n","97it [00:19,  4.87it/s]\n","Current loss: 0.0058: : 888it [01:14, 11.92it/s]           \n","VALIDATION FINISHED: LOSS: 0.00581652874012712\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712]\n","Training one epoch from iteration 134975\n","97it [00:20,  4.81it/s]\n","Current loss: 0.0058: : 888it [01:14, 11.87it/s]\n","VALIDATION FINISHED: LOSS: 0.005829151634266909\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909]\n","Training one epoch from iteration 135072\n","97it [00:19,  4.88it/s]\n","Current loss: 0.0058: : 888it [01:15, 11.74it/s]           \n","VALIDATION FINISHED: LOSS: 0.005846618687805328\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328]\n","Training one epoch from iteration 135169\n","97it [00:20,  4.85it/s]\n","Current loss: 0.0058: : 888it [01:15, 11.80it/s]           \n","VALIDATION FINISHED: LOSS: 0.005832407652961009\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009]\n","Training one epoch from iteration 135266\n","97it [00:20,  4.72it/s]\n","Current loss: 0.0058: : 888it [01:15, 11.82it/s]           \n","VALIDATION FINISHED: LOSS: 0.0058063311203564415\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415]\n","Training one epoch from iteration 135363\n","97it [00:19,  4.88it/s]\n","Current loss: 0.0058: : 888it [01:15, 11.83it/s]           \n","VALIDATION FINISHED: LOSS: 0.0058354524127836975\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975]\n","Training one epoch from iteration 135460\n","97it [00:19,  4.91it/s]\n","Current loss: 0.0058: : 888it [01:15, 11.82it/s]           \n","VALIDATION FINISHED: LOSS: 0.005798247454688996\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996]\n","Training one epoch from iteration 135557\n","97it [00:19,  4.89it/s]\n","Current loss: 0.0058: : 888it [01:15, 11.83it/s]           \n","VALIDATION FINISHED: LOSS: 0.0057736595950077865\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865]\n","Training one epoch from iteration 135654\n","97it [00:20,  4.72it/s]\n","Current loss: 0.0058: : 888it [01:14, 11.85it/s]           \n","VALIDATION FINISHED: LOSS: 0.005772713999298552\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552]\n","Training one epoch from iteration 135751\n","97it [00:20,  4.85it/s]\n","Current loss: 0.0058: : 888it [01:15, 11.79it/s]           \n","VALIDATION FINISHED: LOSS: 0.005772857959167235\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235]\n","Training one epoch from iteration 135848\n","97it [00:19,  4.91it/s]\n","Current loss: 0.0057: : 888it [01:14, 11.85it/s]           \n","VALIDATION FINISHED: LOSS: 0.005746805340603312\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312]\n","Training one epoch from iteration 135945\n","97it [00:20,  4.84it/s]\n","Current loss: 0.0058: : 888it [01:15, 11.75it/s]\n","VALIDATION FINISHED: LOSS: 0.005776995749722516\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516]\n","Training one epoch from iteration 136042\n","97it [00:19,  4.90it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.84it/s]           \n","VALIDATION FINISHED: LOSS: 0.005746402147213968\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968]\n","Training one epoch from iteration 136139\n","97it [00:20,  4.74it/s]\n","Current loss: 0.0058: : 888it [01:14, 11.89it/s]           \n","VALIDATION FINISHED: LOSS: 0.005755973000919467\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467]\n","Training one epoch from iteration 136236\n","97it [00:20,  4.79it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.76it/s]           \n","VALIDATION FINISHED: LOSS: 0.0057235295942781065\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065]\n","Training one epoch from iteration 136333\n","97it [00:19,  4.88it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.77it/s]           \n","VALIDATION FINISHED: LOSS: 0.005725600844540696\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696]\n","Training one epoch from iteration 136430\n","97it [00:19,  4.90it/s]\n","Current loss: 0.0057: : 888it [01:16, 11.68it/s]           \n","VALIDATION FINISHED: LOSS: 0.005734987162393919\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919]\n","Training one epoch from iteration 136527\n","97it [00:19,  4.88it/s]\n","Current loss: 0.0057: : 888it [01:14, 11.91it/s]           \n","VALIDATION FINISHED: LOSS: 0.005722961920459837\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837]\n","Training one epoch from iteration 136624\n","97it [00:19,  4.91it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.83it/s]           \n","VALIDATION FINISHED: LOSS: 0.005732528272772886\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886]\n","Training one epoch from iteration 136721\n","97it [00:20,  4.83it/s]\n","Current loss: 0.0057: : 888it [01:14, 11.93it/s]           \n","VALIDATION FINISHED: LOSS: 0.005712756286779001\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001]\n","Training one epoch from iteration 136818\n","97it [00:20,  4.72it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.77it/s]\n","VALIDATION FINISHED: LOSS: 0.005716260804530962\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962]\n","Training one epoch from iteration 136915\n","97it [00:20,  4.80it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.78it/s]           \n","VALIDATION FINISHED: LOSS: 0.00573229176213342\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847, 0.004739925956603178]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962, 0.00573229176213342]\n","Training one epoch from iteration 137012\n","97it [00:19,  4.87it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.84it/s]\n","VALIDATION FINISHED: LOSS: 0.0057300825673837225\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847, 0.004739925956603178, 0.0047310979799223465]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962, 0.00573229176213342, 0.0057300825673837225]\n","Training one epoch from iteration 137109\n","97it [00:20,  4.81it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.80it/s]           \n","VALIDATION FINISHED: LOSS: 0.005683082046940589\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847, 0.004739925956603178, 0.0047310979799223465, 0.004580064274649107]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962, 0.00573229176213342, 0.0057300825673837225, 0.005683082046940589]\n","Training one epoch from iteration 137206\n","97it [00:19,  4.86it/s]\n","Current loss: 0.0057: : 888it [01:14, 11.91it/s]           \n","VALIDATION FINISHED: LOSS: 0.00567632944961309\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847, 0.004739925956603178, 0.0047310979799223465, 0.004580064274649107, 0.004709284642192814]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962, 0.00573229176213342, 0.0057300825673837225, 0.005683082046940589, 0.00567632944961309]\n","Training one epoch from iteration 137303\n","97it [00:20,  4.83it/s]\n","Current loss: 0.0057: : 888it [01:14, 11.85it/s]           \n","VALIDATION FINISHED: LOSS: 0.0057050256019375335\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847, 0.004739925956603178, 0.0047310979799223465, 0.004580064274649107, 0.004709284642192814, 0.004734931046805661]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962, 0.00573229176213342, 0.0057300825673837225, 0.005683082046940589, 0.00567632944961309, 0.0057050256019375335]\n","Training one epoch from iteration 137400\n","97it [00:19,  4.87it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.78it/s]           \n","VALIDATION FINISHED: LOSS: 0.005744155559876009\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847, 0.004739925956603178, 0.0047310979799223465, 0.004580064274649107, 0.004709284642192814, 0.004734931046805661, 0.0046372314418195604]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962, 0.00573229176213342, 0.0057300825673837225, 0.005683082046940589, 0.00567632944961309, 0.0057050256019375335, 0.005744155559876009]\n","Training one epoch from iteration 137497\n","97it [00:19,  4.89it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.80it/s]           \n","VALIDATION FINISHED: LOSS: 0.005666902164578839\n","MODEL IMPROVED ON VALIDATION SET!\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847, 0.004739925956603178, 0.0047310979799223465, 0.004580064274649107, 0.004709284642192814, 0.004734931046805661, 0.0046372314418195604, 0.0046759699939356485]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962, 0.00573229176213342, 0.0057300825673837225, 0.005683082046940589, 0.00567632944961309, 0.0057050256019375335, 0.005744155559876009, 0.005666902164578839]\n","Training one epoch from iteration 137594\n","97it [00:19,  4.86it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.83it/s]\n","VALIDATION FINISHED: LOSS: 0.005706149676690396\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847, 0.004739925956603178, 0.0047310979799223465, 0.004580064274649107, 0.004709284642192814, 0.004734931046805661, 0.0046372314418195604, 0.0046759699939356485, 0.004602068929558562]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962, 0.00573229176213342, 0.0057300825673837225, 0.005683082046940589, 0.00567632944961309, 0.0057050256019375335, 0.005744155559876009, 0.005666902164578839, 0.005706149676690396]\n","Training one epoch from iteration 137691\n","97it [00:19,  4.89it/s]\n","Current loss: 0.0057: : 888it [01:14, 11.88it/s]\n","VALIDATION FINISHED: LOSS: 0.005699858317916748\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847, 0.004739925956603178, 0.0047310979799223465, 0.004580064274649107, 0.004709284642192814, 0.004734931046805661, 0.0046372314418195604, 0.0046759699939356485, 0.004602068929558562, 0.004581825550856818]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962, 0.00573229176213342, 0.0057300825673837225, 0.005683082046940589, 0.00567632944961309, 0.0057050256019375335, 0.005744155559876009, 0.005666902164578839, 0.005706149676690396, 0.005699858317916748]\n","Training one epoch from iteration 137788\n","97it [00:19,  4.87it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.81it/s]           \n","VALIDATION FINISHED: LOSS: 0.005689926740652067\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847, 0.004739925956603178, 0.0047310979799223465, 0.004580064274649107, 0.004709284642192814, 0.004734931046805661, 0.0046372314418195604, 0.0046759699939356485, 0.004602068929558562, 0.004581825550856818, 0.004587721049583987]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962, 0.00573229176213342, 0.0057300825673837225, 0.005683082046940589, 0.00567632944961309, 0.0057050256019375335, 0.005744155559876009, 0.005666902164578839, 0.005706149676690396, 0.005699858317916748, 0.005689926740652067]\n","Training one epoch from iteration 137885\n","97it [00:19,  4.86it/s]\n","Current loss: 0.0057: : 888it [01:14, 11.90it/s]           \n","VALIDATION FINISHED: LOSS: 0.005710192417412854\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847, 0.004739925956603178, 0.0047310979799223465, 0.004580064274649107, 0.004709284642192814, 0.004734931046805661, 0.0046372314418195604, 0.0046759699939356485, 0.004602068929558562, 0.004581825550856818, 0.004587721049583987, 0.00469991348242176]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962, 0.00573229176213342, 0.0057300825673837225, 0.005683082046940589, 0.00567632944961309, 0.0057050256019375335, 0.005744155559876009, 0.005666902164578839, 0.005706149676690396, 0.005699858317916748, 0.005689926740652067, 0.005710192417412854]\n","Training one epoch from iteration 137982\n","97it [00:19,  4.87it/s]\n","Current loss: 0.0057: : 888it [01:15, 11.82it/s]           \n","VALIDATION FINISHED: LOSS: 0.005703569805920275\n","Saving model...\n","Train Loss: [0.007413007102945109, 0.0066205322233595185, 0.006336741039010975, 0.006267690164584321, 0.006135542824211502, 0.006071505434427065, 0.005838929868744898, 0.005779391819534381, 0.00586161209323165, 0.005636519519612193, 0.005636980055024866, 0.005589815426723475, 0.00543801613948932, 0.005652116885235936, 0.005382218184525666, 0.005330116073984998, 0.005360590483628444, 0.005317595514910507, 0.005278895544272261, 0.005267261309049947, 0.005174378985683098, 0.0052231040689614166, 0.0051724759552186144, 0.005155209732750796, 0.00508797906828833, 0.005127205302189921, 0.005069802785244261, 0.005105610009158011, 0.004913053346903438, 0.005055639050305658, 0.005010282940029637, 0.005042616385947337, 0.004887913684984765, 0.004893747870042229, 0.0049969345696040035, 0.004868328534114684, 0.0048209565653244855, 0.004867442489899311, 0.004896694269828191, 0.004835166165499573, 0.00481150126127891, 0.004806006209655982, 0.0048681482535873335, 0.004680985563252236, 0.004731449016523499, 0.0046148436700867625, 0.004770655539030805, 0.004679685254514064, 0.0047010302546040455, 0.004757677424535847, 0.004739925956603178, 0.0047310979799223465, 0.004580064274649107, 0.004709284642192814, 0.004734931046805661, 0.0046372314418195604, 0.0046759699939356485, 0.004602068929558562, 0.004581825550856818, 0.004587721049583987, 0.00469991348242176, 0.004504870679757567]\n","Validation Loss: [0.007358191017038393, 0.007074328591975776, 0.006917470385454729, 0.0067300539674040255, 0.006604816317154504, 0.006526375998955784, 0.006416785798448785, 0.0063399013292059075, 0.0062716078727143194, 0.0062333967902190035, 0.006204682198192657, 0.006149176460206257, 0.006088839380567699, 0.006050824877797617, 0.006055279955035077, 0.005991470785669951, 0.005958879528292411, 0.005980228455784422, 0.00595999179816457, 0.005937366131121529, 0.005945601021799654, 0.005942237158337408, 0.005902864902226782, 0.005893288331650629, 0.005907530258965538, 0.00585464408884962, 0.00583504373286347, 0.0058915777578173855, 0.005831635488681742, 0.00581652874012712, 0.005829151634266909, 0.005846618687805328, 0.005832407652961009, 0.0058063311203564415, 0.0058354524127836975, 0.005798247454688996, 0.0057736595950077865, 0.005772713999298552, 0.005772857959167235, 0.005746805340603312, 0.005776995749722516, 0.005746402147213968, 0.005755973000919467, 0.0057235295942781065, 0.005725600844540696, 0.005734987162393919, 0.005722961920459837, 0.005732528272772886, 0.005712756286779001, 0.005716260804530962, 0.00573229176213342, 0.0057300825673837225, 0.005683082046940589, 0.00567632944961309, 0.0057050256019375335, 0.005744155559876009, 0.005666902164578839, 0.005706149676690396, 0.005699858317916748, 0.005689926740652067, 0.005710192417412854, 0.005703569805920275]\n","Saving training and validation losses to disk.\n","TESTING\n","Current loss: 0.0056: 100% 245/245 [00:31<00:00,  7.85it/s]\n","TEST FINISHED: LOSS: 0.005622179703121739\n","The test loss value was saved as JSON format.\n"]}],"source":["!python \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/train.py\" --dataset_dir \"/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/Dataset\" --load_model \"/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/model\" --checkpoint_dir \"/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet\" --hdf_dir \"/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/hdf\" --log_dir \"/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/logs/waveunet\" --patience 5 --sr 16000 --channels 2 --cuda\n"]},{"cell_type":"markdown","metadata":{"id":"-wNDLK67rbXF"},"source":["## Loss Graphs (Training and Validation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1u6i9Zu7ili"},"outputs":[],"source":["def plot_loss(train_loss_file, val_loss_file, model_type):\n","    \"\"\"\n","    Plots the training and validation loss from two JSON files.\n","    \"\"\"\n","\n","    # Load train_loss.json\n","    with open(train_loss_file, 'r') as f:\n","        train_losses = json.load(f)\n","\n","    # Load val_loss.json\n","    with open(val_loss_file, 'r') as f:\n","        val_losses = json.load(f)\n","\n","    # Create epochs list\n","    epochs = list(range(1, len(train_losses) + 1))  # Assuming the number of epochs equals the length of the losses list\n","\n","    # Plotting\n","    plt.plot(epochs, train_losses, label='Train Loss')\n","    plt.plot(epochs, val_losses, label='Validation Loss')\n","\n","    # Add labels and title\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title(f'Training and Validation Loss \\n {model_type}')\n","\n","    # Add legend\n","    plt.legend()\n","\n","    # Add grid\n","    plt.grid(True)\n","\n","    # Show plot\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":493},"executionInfo":{"elapsed":538,"status":"ok","timestamp":1710872639973,"user":{"displayName":"Omri Newman","userId":"03939389287432353270"},"user_tz":-120},"id":"lB7ixYqs9JYe","outputId":"3769a5c6-0b03-4f4a-a5c0-f572bee7ce2e"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAlEAAAHcCAYAAAD2uv9FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAClw0lEQVR4nOzdeVxU1fvA8c/MADPsKCiLIqDijhsq7pqiuKRpZWXmlqlp7pm/THOtLE1zyyVz7atpaZmlqWiapuS+7yu4gSugIOvc3x9XJkdQAYERfN6v130xc+659zxzmOLx3HPP1SiKoiCEEEIIIbJEa+kAhBBCCCHyI0mihBBCCCGyQZIoIYQQQohskCRKCCGEECIbJIkSQgghhMgGSaKEEEIIIbJBkighhBBCiGyQJEoIIYQQIhskiRJCCCGEyAZJooQoILp164avr2+2jh0zZgwajSZnA3rOXLx4EY1Gw6JFi/K8bY1Gw5gxY0zvFy1ahEaj4eLFi0891tfXl27duuVoPM/yXRFC/EeSKCFymUajydS2detWS4f6whswYAAajYazZ88+ts6IESPQaDQcPnw4DyPLuqtXrzJmzBgOHjxo6VBM0hLZr7/+2tKhCJEjrCwdgBAF3Q8//GD2fsmSJYSGhqYrL1++/DO1M2/ePIxGY7aOHTlyJB9//PEztV8QdOrUiRkzZrBs2TJGjRqVYZ0ff/yRgIAAKleunO12OnfuzFtvvYVer8/2OZ7m6tWrjB07Fl9fX6pWrWq271m+K0KI/0gSJUQue+edd8ze//vvv4SGhqYrf1R8fDx2dnaZbsfa2jpb8QFYWVlhZSX/OwgKCqJ06dL8+OOPGSZRYWFhXLhwgS+//PKZ2tHpdOh0umc6x7N4lu+KEOI/cjlPiOdA48aNqVSpEvv27aNhw4bY2dnxySefAPDbb7/RunVrvLy80Ov1lCpVivHjx5Oammp2jkfnuTx86eS7776jVKlS6PV6atasyZ49e8yOzWhOlEajoV+/fqxevZpKlSqh1+upWLEi69evTxf/1q1bqVGjBgaDgVKlSjF37txMz7Pavn07HTp0oESJEuj1ery9vRk8eDD3799P9/kcHBy4cuUK7dq1w8HBgSJFijB06NB0fREdHU23bt1wdnbGxcWFrl27Eh0d/dRYQB2NOnnyJPv370+3b9myZWg0Gjp27EhSUhKjRo0iMDAQZ2dn7O3tadCgAVu2bHlqGxnNiVIUhc8++4zixYtjZ2fHSy+9xLFjx9Ide/v2bYYOHUpAQAAODg44OTnRsmVLDh06ZKqzdetWatasCUD37t1Nl4zT5oNlNCcqLi6ODz/8EG9vb/R6PWXLluXrr79GURSzeln5XmTX9evX6dGjB+7u7hgMBqpUqcLixYvT1Vu+fDmBgYE4Ojri5OREQEAA06ZNM+1PTk5m7Nix+Pv7YzAYcHV1pX79+oSGhuZYrOLFJv/0FOI5cevWLVq2bMlbb73FO++8g7u7O6D+wXVwcGDIkCE4ODjw119/MWrUKGJjY5k0adJTz7ts2TLu3r1L79690Wg0TJw4kVdffZXz588/dUTin3/+4ZdffqFv3744Ojoyffp0XnvtNSIiInB1dQXgwIEDtGjRAk9PT8aOHUtqairjxo2jSJEimfrcP//8M/Hx8fTp0wdXV1d2797NjBkzuHz5Mj///LNZ3dTUVEJCQggKCuLrr79m06ZNTJ48mVKlStGnTx9ATUZeeeUV/vnnH95//33Kly/Pr7/+SteuXTMVT6dOnRg7dizLli2jevXqZm3/9NNPNGjQgBIlSnDz5k2+//57OnbsSM+ePbl79y7z588nJCSE3bt3p7uE9jSjRo3is88+o1WrVrRq1Yr9+/fTvHlzkpKSzOqdP3+e1atX06FDB/z8/IiKimLu3Lk0atSI48eP4+XlRfny5Rk3bhyjRo2iV69eNGjQAIC6detm2LaiKLRt25YtW7bQo0cPqlatyoYNG/joo4+4cuUK33zzjVn9zHwvsuv+/fs0btyYs2fP0q9fP/z8/Pj555/p1q0b0dHRDBw4EIDQ0FA6duxI06ZN+eqrrwA4ceIEO3bsMNUZM2YMEyZM4L333qNWrVrExsayd+9e9u/fT7NmzZ4pTiEAUIQQeeqDDz5QHv1Pr1GjRgqgzJkzJ139+Pj4dGW9e/dW7OzslISEBFNZ165dFR8fH9P7CxcuKIDi6uqq3L5921T+22+/KYDy+++/m8pGjx6dLiZAsbGxUc6ePWsqO3TokAIoM2bMMJW1adNGsbOzU65cuWIqO3PmjGJlZZXunBnJ6PNNmDBB0Wg0Snh4uNnnA5Rx48aZ1a1WrZoSGBhoer969WoFUCZOnGgqS0lJURo0aKAAysKFC58aU82aNZXixYsrqampprL169crgDJ37lzTORMTE82Ou3PnjuLu7q68++67ZuWAMnr0aNP7hQsXKoBy4cIFRVEU5fr164qNjY3SunVrxWg0mup98sknCqB07drVVJaQkGAWl6Kov2u9Xm/WN3v27Hns5330u5LWZ5999plZvddff13RaDRm34HMfi8ykvadnDRp0mPrTJ06VQGU//3vf6aypKQkpU6dOoqDg4MSGxurKIqiDBw4UHFyclJSUlIee64qVaoorVu3fmJMQjwLuZwnxHNCr9fTvXv3dOW2tram13fv3uXmzZs0aNCA+Ph4Tp48+dTzvvnmmxQqVMj0Pm1U4vz58089Njg4mFKlSpneV65cGScnJ9OxqampbNq0iXbt2uHl5WWqV7p0aVq2bPnU84P554uLi+PmzZvUrVsXRVE4cOBAuvrvv/++2fsGDRqYfZZ169ZhZWVlGpkCdQ5S//79MxUPqPPYLl++zLZt20xly5Ytw8bGhg4dOpjOaWNjA4DRaOT27dukpKRQo0aNDC8FPsmmTZtISkqif//+ZpdABw0alK6uXq9Hq1X/152amsqtW7dwcHCgbNmyWW43zbp169DpdAwYMMCs/MMPP0RRFP7880+z8qd9L57FunXr8PDwoGPHjqYya2trBgwYwL179/j7778BcHFxIS4u7omX5lxcXDh27Bhnzpx55riEyIgkUUI8J4oVK2b6o/ywY8eO0b59e5ydnXFycqJIkSKmSekxMTFPPW+JEiXM3qclVHfu3MnysWnHpx17/fp17t+/T+nSpdPVy6gsIxEREXTr1o3ChQub5jk1atQISP/5DAZDusuED8cDEB4ejqenJw4ODmb1ypYtm6l4AN566y10Oh3Lli0DICEhgV9//ZWWLVuaJaSLFy+mcuXKpvk2RYoUYe3atZn6vTwsPDwcAH9/f7PyIkWKmLUHasL2zTff4O/vj16vx83NjSJFinD48OEst/tw+15eXjg6OpqVp90xmhZfmqd9L55FeHg4/v7+pkTxcbH07duXMmXK0LJlS4oXL867776bbl7WuHHjiI6OpkyZMgQEBPDRRx8990tTiPxFkighnhMPj8ikiY6OplGjRhw6dIhx48bx+++/ExoaapoDkpnb1B93F5jyyIThnD42M1JTU2nWrBlr167l//7v/1i9ejWhoaGmCdCPfr68uqOtaNGiNGvWjFWrVpGcnMzvv//O3bt36dSpk6nO//73P7p160apUqWYP38+69evJzQ0lCZNmuTq8gFffPEFQ4YMoWHDhvzvf/9jw4YNhIaGUrFixTxbtiC3vxeZUbRoUQ4ePMiaNWtM87latmxpNvetYcOGnDt3jgULFlCpUiW+//57qlevzvfff59ncYqCTSaWC/Ec27p1K7du3eKXX36hYcOGpvILFy5YMKr/FC1aFIPBkOHilE9asDLNkSNHOH36NIsXL6ZLly6m8me5e8rHx4fNmzdz7949s9GoU6dOZek8nTp1Yv369fz5558sW7YMJycn2rRpY9q/cuVKSpYsyS+//GJ2CW706NHZihngzJkzlCxZ0lR+48aNdKM7K1eu5KWXXmL+/Plm5dHR0bi5uZneZ2UFeh8fHzZt2sTdu3fNRqPSLhenxZcXfHx8OHz4MEaj0Ww0KqNYbGxsaNOmDW3atMFoNNK3b1/mzp3Lp59+ahoJLVy4MN27d6d79+7cu3ePhg0bMmbMGN577708+0yi4JKRKCGeY2n/4n/4X/hJSUnMmjXLUiGZ0el0BAcHs3r1aq5evWoqP3v2bLp5NI87Hsw/n6IoZrepZ1WrVq1ISUlh9uzZprLU1FRmzJiRpfO0a9cOOzs7Zs2axZ9//smrr76KwWB4Yuy7du0iLCwsyzEHBwdjbW3NjBkzzM43derUdHV1Ol26EZ+ff/6ZK1eumJXZ29sDZGpph1atWpGamsrMmTPNyr/55hs0Gk2m57flhFatWhEZGcmKFStMZSkpKcyYMQMHBwfTpd5bt26ZHafVak0LoCYmJmZYx8HBgdKlS5v2C/GsZCRKiOdY3bp1KVSoEF27djU9kuSHH37I08smTzNmzBg2btxIvXr16NOnj+mPcaVKlZ76yJFy5cpRqlQphg4dypUrV3BycmLVqlXPNLemTZs21KtXj48//piLFy9SoUIFfvnllyzPF3JwcKBdu3ameVEPX8oDePnll/nll19o3749rVu35sKFC8yZM4cKFSpw7969LLWVtt7VhAkTePnll2nVqhUHDhzgzz//NBtdSmt33LhxdO/enbp163LkyBGWLl1qNoIFUKpUKVxcXJgzZw6Ojo7Y29sTFBSEn59fuvbbtGnDSy+9xIgRI7h48SJVqlRh48aN/PbbbwwaNMhsEnlO2Lx5MwkJCenK27VrR69evZg7dy7dunVj3759+Pr6snLlSnbs2MHUqVNNI2Xvvfcet2/fpkmTJhQvXpzw8HBmzJhB1apVTfOnKlSoQOPGjQkMDKRw4cLs3buXlStX0q9fvxz9POIFZpmbAoV4cT1uiYOKFStmWH/Hjh1K7dq1FVtbW8XLy0sZNmyYsmHDBgVQtmzZYqr3uCUOMrqdnEduuX/cEgcffPBBumN9fHzMbrlXFEXZvHmzUq1aNcXGxkYpVaqU8v333ysffvihYjAYHtML/zl+/LgSHBysODg4KG5ubkrPnj1Nt8w/fHt+165dFXt7+3THZxT7rVu3lM6dOytOTk6Ks7Oz0rlzZ+XAgQOZXuIgzdq1axVA8fT0TLesgNFoVL744gvFx8dH0ev1SrVq1ZQ//vgj3e9BUZ6+xIGiKEpqaqoyduxYxdPTU7G1tVUaN26sHD16NF1/JyQkKB9++KGpXr169ZSwsDClUaNGSqNGjcza/e2335QKFSqYlptI++wZxXj37l1l8ODBipeXl2Jtba34+/srkyZNMltyIe2zZPZ78ai07+Tjth9++EFRFEWJiopSunfvrri5uSk2NjZKQEBAut/bypUrlebNmytFixZVbGxslBIlSii9e/dWrl27Zqrz2WefKbVq1VJcXFwUW1tbpVy5csrnn3+uJCUlPTFOITJLoyjP0T9phRAFRrt27eT2ciFEgSZzooQQz+zRR7ScOXOGdevW0bhxY8sEJIQQeUBGooQQz8zT05Nu3bpRsmRJwsPDmT17NomJiRw4cCDd2kdCCFFQyMRyIcQza9GiBT/++CORkZHo9Xrq1KnDF198IQmUEKJAk5EoIYQQQohskDlRQgghhBDZIEmUEEIIIUQ2SBIlhMg3Fi1ahEaj4eLFi6ayxo0bF/i7ADP63OLpLl68iEajMT2LEdTFYbPySBwhnkSSKCGyKe1/0GmbtbU1bm5u1K1bl08++YSIiAhLhyheMN26dTN7XuCjHBwc6NatW94FJEQBJ0mUEM+oY8eO/PDDD8yfP59PP/2UkiVLMnXqVMqXL8/y5cstHV6Bt3HjRjZu3GjpMEQ+MXLkyHTrmgmRXbLEgRDPqHr16rzzzjtmZeHh4TRv3pyuXbtSvnx5qlSpYqHoCj4bGxtLhyDyESsrK6ys5E+fyBkyEiVELvDx8WHRokUkJSUxceJEs33nz5+nQ4cOFC5cGDs7O2rXrs3atWtN+xVFwc3NjSFDhpjKjEYjLi4u6HQ6oqOjTeVfffUVVlZWpgfepl3OuXLlCu3atcPBwcH0cNvU1FSzOJYvX05gYCCOjo44OTkREBDAtGnTTPtv377N0KFDCQgIwMHBAScnJ1q2bMmhQ4fMzrN161Y0Gg0//fQTY8eOpVixYjg6OvL6668TExNDYmIigwYNomjRojg4ONC9e3cSExPNzqHRaOjXrx9Lly6lbNmyGAwGAgMD2bZt21P7+tE5UQ/H8/nnn1O8eHEMBgNNmzbl7Nmz6Y7/9ttvKVmyJLa2ttSqVYvt27dnep7VwoULadKkCUWLFkWv11OhQgVmz56drp6vry8vv/wy//zzD7Vq1cJgMFCyZEmWLFmSru6xY8do0qQJtra2FC9enM8++wyj0fjUWLIjba7Vtm3b6N27N66urjg5OdGlS5d0D4Heu3cvISEhuLm5YWtri5+fH++++65Zna+//pq6devi6uqKra0tgYGBrFy5Ml27ab/vn3/+mQoVKmBra0udOnU4cuQIAHPnzqV06dIYDAYaN26cbi5Y48aNqVSpEvv27aNu3bqmeObMmfPUz5zRnKi0eFavXk2lSpXQ6/VUrFiR9evXpzt+69at1KhRA4PBQKlSpZg7d67Ms3qBSTouRC6pU6cOpUqVIjQ01FQWFRVF3bp1iY+PZ8CAAbi6urJ48WLatm3LypUrad++PRqNhnr16pklEIcPHyYmJgatVsuOHTto3bo1ANu3b6datWpm82BSU1MJCQkhKCiIr7/+mk2bNjF58mRKlSpFnz59AAgNDaVjx440bdqUr776CoATJ06wY8cOBg4cCKjJ3urVq+nQoQN+fn5ERUUxd+5cGjVqxPHjx/Hy8jL7vBMmTMDW1paPP/6Ys2fPMmPGDKytrdFqtdy5c4cxY8bw77//smjRIvz8/Bg1apTZ8X///TcrVqxgwIAB6PV6Zs2aRYsWLdi9ezeVKlXKcv9/+eWXaLVahg4dSkxMDBMnTqRTp07s2rXLVGf27Nn069ePBg0aMHjwYC5evEi7du0oVKgQxYsXf2obs2fPpmLFirRt2xYrKyt+//13+vbti9Fo5IMPPjCre/bsWV5//XV69OhB165dWbBgAd26dSMwMJCKFSsCEBkZyUsvvURKSgoff/wx9vb2fPfdd9ja2mb582dFv379cHFxYcyYMZw6dYrZs2cTHh5uSkivX79O8+bNKVKkCB9//DEuLi5cvHiRX375xew806ZNo23btnTq1ImkpCSWL19Ohw4d+OOPP0zf2TTbt29nzZo1pn6aMGECL7/8MsOGDWPWrFn07duXO3fuMHHiRN59913++usvs+Pv3LlDq1ateOONN+jYsSM//fQTffr0wcbGJl1ylxn//PMPv/zyC3379sXR0ZHp06fz2muvERERgaurKwAHDhygRYsWeHp6MnbsWFJTUxk3bhxFihTJcnuigLDgw4+FyNfSnkg/adKkx9Z55ZVXFECJiYlRFEVRBg0apADK9u3bTXXu3r2r+Pn5Kb6+vkpqaqqiKIoyadIkRafTKbGxsYqiKMr06dMVHx8fpVatWsr//d//KYqiKKmpqYqLi4syePBg07m6du2qAMq4cePM4qhWrZoSGBhoej9w4EDFyclJSUlJeWzsCQkJpnge/sx6vd7s/Fu2bFEApVKlSkpSUpKpvGPHjopGo1Fatmxpdo46deooPj4+ZmWAAih79+41lYWHhysGg0Fp3769qWzhwoUKoFy4cMFU1qhRI6VRo0bp4ilfvrySmJhoKp82bZoCKEeOHFEURVESExMVV1dXpWbNmkpycrKp3qJFixTA7JyPEx8fn64sJCREKVmypFmZj4+PAijbtm0zlV2/fl3R6/XKhx9+aCpL+37s2rXLrJ6zs3O6z52Rrl27Kvb29o/db29vr3Tt2tX0Pq0/AwMDzX53EydOVADlt99+UxRFUX799VcFUPbs2fPE9h/tj6SkJKVSpUpKkyZNzMoBRa/Xm32euXPnKoDi4eFh+t4riqIMHz48w985oEyePNlUlpiYqFStWlUpWrSo6bOk/Te6cOFCU73Ro0crj/7pAxQbGxvl7NmzprJDhw4pgDJjxgxTWZs2bRQ7OzvlypUrprIzZ84oVlZW6c4pXgxyOU+IXJQ2QnT37l0A1q1bR61atahfv75ZnV69enHx4kWOHz8OQIMGDUhNTWXnzp2A+q/2Bg0a0KBBA7Zv3w7A0aNHiY6OpkGDBunaff/9983eN2jQgPPnz5veu7i4EBcXZzZK9ii9Xo9Wq/4vIjU1lVu3buHg4EDZsmXZv39/uvpdunTB2tra9D4oKAhFUdKNCgQFBXHp0iVSUlLMyuvUqUNgYKDpfYkSJXjllVfYsGFDukuRmdG9e3ez+VJp/ZTWD3v37uXWrVv07NnTbI5Mp06dKFSoUKbaeHiEKCYmhps3b9KoUSPOnz9PTEyMWd0KFSqY/a6KFClC2bJlzX4v69ato3bt2tSqVcusXqdOnTIVT3b16tXL7HfXp08frKysWLduHaB+XwD++OMPkpOTH3ueh/vjzp07xMTE0KBBgwy/L02bNsXX19f0PigoCIDXXnsNR0fHdOUP9xOoc5t69+5tem9jY0Pv3r25fv06+/bte9pHTic4OJhSpUqZ3leuXBknJydTu6mpqWzatIl27dqZjcKWLl2ali1bZrk9UTBIEiVELkqbq5T2RyE8PJyyZcumq1e+fHnTflAnq9vZ2ZkSprQkqmHDhuzdu5eEhATTvocTMgCDwZDu8kKhQoXM5rj07duXMmXK0LJlS4oXL867776bbv6H0Wjkm2++wd/fH71ej5ubG0WKFDFdWnxUiRIlzN47OzsD4O3tna7caDSmO0dGz9krU6YM8fHx3LhxI92+p3k0nrTEKK0f0vq6dOnSZvWsrKzM/rg/yY4dOwgODsbe3h4XFxeKFCnCJ598ApDu8z0aT1pMD/9ewsPDM+yHjL4z2ZXR3J1H23RwcMDT09M0F6lRo0a89tprjB07Fjc3N1555RUWLlyYbm7bH3/8Qe3atTEYDBQuXJgiRYowe/bsZ/6+AOnmaHl5eWFvb29WVqZMGYBsraf1tN/P9evXuX//frrvC6T/DokXhyRRQuSio0ePUrRoUZycnLJ0nLW1NUFBQWzbto2zZ88SGRlJgwYNqF+/PsnJyezatYvt27dTrly5dAmTTqd76vmLFi3KwYMHWbNmDW3btmXLli20bNmSrl27mup88cUXDBkyhIYNG/K///2PDRs2EBoaSsWKFTOc6Py4dh9XruTyYztzu91z587RtGlTbt68yZQpU1i7di2hoaEMHjwYIF0f5UU/GAwGEhMTMzynoigkJCRgMBiyfF6NRsPKlSsJCwujX79+XLlyhXfffZfAwEDTPxS2b99O27ZtMRgMzJo1i3Xr1hEaGsrbb7+dYTwv2vdFFEySRAmRS8LCwjh37hzNmzc3lfn4+HDq1Kl0dU+ePGnan6ZBgwbs3r2bTZs24ebmRrly5ShcuDAVK1Zk+/btbN++nYYNG2Y7PhsbG9q0acOsWbM4d+4cvXv3ZsmSJaY72FauXMlLL73E/Pnzeeutt2jevDnBwcFmdwfmpDNnzqQrO336NHZ2drkycTetrx+9Yy8lJSVTIxm///47iYmJrFmzht69e9OqVSuCg4OfaRK4j49Phv2Q0XfmccenpKRw7ty5dPvOnj1Lamqq2XcszaNt3rt3j2vXrqUbkatduzaff/45e/fuZenSpRw7dsy0FtqqVaswGAxs2LCBd999l5YtWxIcHJypuLPj6tWrxMXFmZWdPn0aINMjiVlRtGhRDAZDhnd4ZlQmXgySRAmRC8LDw+nWrRs2NjZ89NFHpvJWrVqxe/duwsLCTGVxcXF89913+Pr6UqFCBVN5gwYNSExMZOrUqdSvX990GaZBgwb88MMPXL16NcP5UJlx69Yts/darZbKlSsDmC7R6HS6dP8K//nnn7ly5Uq22nyasLAws7kzly5d4rfffqN58+aZGl3Lqho1auDq6sq8efPM5mctXbo03aWjjKTF9HAfxcTEsHDhwmzH1KpVK/799192795tKrtx4wZLly7N1PFpc3NmzpyZbt+3335rVudh3333ndlcp9mzZ5OSkmKqe+fOnXTfhapVqwLm3xeNRmM2f+3ixYusXr06U7FnVUpKCnPnzjW9T0pKYu7cuRQpUsRsbl1O0el0BAcHs3r1aq5evWoqP3v2LH/++WeOtyfyB1niQIhntH//fv73v/9hNBqJjo5mz549rFq1Co1Gww8//GBKTgA+/vhjfvzxR1q2bMmAAQMoXLgwixcv5sKFC6xatco0kRvUidZWVlacOnWKXr16mcobNmxoWosou0nUe++9x+3bt2nSpAnFixcnPDycGTNmULVqVdP8rJdffplx48bRvXt36taty5EjR1i6dCklS5bMVptPU6lSJUJCQsyWOAAYO3ZsrrRnY2PDmDFj6N+/P02aNOGNN97g4sWLLFq0iFKlSj113Z/mzZubRvN69+7NvXv3mDdvHkWLFuXatWvZimnYsGH88MMPtGjRgoEDB5qWOPDx8eHw4cNPPb5q1aq89957TJs2jTNnztCsWTNAXdJi3bp1vPfeexku/JqUlETTpk154403OHXqFLNmzaJ+/fq0bdsWgMWLFzNr1izat29PqVKluHv3LvPmzcPJyYlWrVoB0Lp1a6ZMmUKLFi14++23uX79Ot9++y2lS5fOVOxZ5eXlxVdffcXFixcpU6YMK1as4ODBg3z33Xdmk+Rz0pgxY9i4cSP16tWjT58+pKamMnPmTCpVqsTBgwdzpU3xfJMkSohn9OOPP/Ljjz9iZWWFk5MT/v7+DBo0iPfffz/dZFV3d3d27tzJ//3f/zFjxgwSEhKoXLkyv//+e7p1dOzt7alWrRp79uwxmzyeljh5e3tneGkmM9555x2+++47Zs2aRXR0NB4eHrz55puMGTPGlMh98sknxMXFsWzZMlasWEH16tVZu3YtH3/8cbbafJpGjRpRp04dxo4dS0REBBUqVGDRokVmSWhO69evH4qiMHnyZIYOHUqVKlVYs2YNAwYMeOrcobJly7Jy5UpGjhzJ0KFD8fDwoE+fPhQpUiRb6xQBeHp6smXLFvr378+XX36Jq6sr77//Pl5eXvTo0SNT55g7dy4BAQEsWLCA4cOHm2KdPn16urWr0sycOZOlS5cyatQokpOT6dixI9OnTzclko0aNWL37t0sX76cqKgonJ2dqVWrFkuXLsXPzw+AJk2aMH/+fL788ksGDRqEn5+fKcnJjSSqUKFCLF68mP79+zNv3jzc3d2ZOXMmPXv2zPG20gQGBvLnn38ydOhQPv30U7y9vRk3bhwnTpwwXZIXLxaNIrPmhBAWptFo+OCDDzK8DJXXjEYjRYoU4dVXX2XevHmWDidXLVq0iO7du7Nnzx5q1Khh6XAyrXHjxty8eZOjR49aOhQA2rVrx7FjxzKczyYKNpkTJYR4YSUkJKSb67NkyRJu376dqce+iBfPow8vPnPmDOvWrZPvywtKLucJIV5Y//77L4MHD6ZDhw64urqyf/9+5s+fT6VKlejQoYOlwxPPoZIlS9KtWzdKlixJeHg4s2fPxsbGhmHDhlk6NGEBkkQJIV5Yvr6+eHt7M336dG7fvk3hwoXp0qULX375pdlq50KkadGiBT/++CORkZHo9Xrq1KnDF198keEiqaLgkzlRQgghhBDZIHOihBBCCCGyQZIoIYQQQohskDlRuchoNHL16lUcHR2funCfEEIIIZ4PiqJw9+5dvLy8zBZBfpQkUbno6tWr6Z5ILoQQQoj84dKlSxQvXvyx+yWJykWOjo6A+ktwcnLK1DHJycls3LiR5s2b59qjC4T0c16Rfs470td5Q/o5b1i6n2NjY/H29jb9HX8cSaJyUdolPCcnpywlUXZ2djg5Ocl/oLlI+jlvSD/nHenrvCH9nDeel35+2lQcmVguhBBCCJENkkQJIYQQQmSDJFFCCCGEENkgc6KEEEI8t1JTU0lOTrZ0GCbJyclYWVmRkJBAamqqpcMpsHK7n62trdHpdM98HkmihBBCPHcURSEyMpLo6GhLh2JGURQ8PDy4dOmSrP+Xi/Kin11cXPDw8Him80sSJYQQ4rmTlkAVLVoUOzu75yZhMRqN3Lt3DwcHhycuwiieTW72s6IoxMfHc/36dQA8PT2zfS5JooQQQjxXUlNTTQmUq6urpcMxYzQaSUpKwmAwSBKVi3K7n21tbQG4fv06RYsWzfalPfkGCCGEeK6kzYGys7OzcCSiIEv7fj3LnDtJooQQQjyXnpdLeKJgyonvlyRRQgghhBDZIEmUEEII8Rzz9fVl6tSplg5DZECSKCGEECIHaDSaJ25jxozJ1nn37NlDr169nim2xo0bM2jQoGc6h0hP7s7Lh2ITkrl8+z7lPR1lzoAQQjwnrl27Znq9YsUKRo0axalTp0xlDg4OpteKopCamoqV1dP/DBcpUiRnAxU5Rkai8pmE5FQqj9lIq+nbibn//KziK4QQLzoPDw/T5uzsjEajMb0/efIkjo6O/PnnnwQGBqLX6/nnn384d+4cr7zyCu7u7jg4OFCzZk02bdpkdt5HL+dpNBq+//572rdvj52dHf7+/qxZs+aZYl+1ahUVK1ZEr9fj6+vL5MmTzfbPmjULf39/DAYD7u7uvP7666Z9K1euJCAgAFtbW1xdXQkODiYuLu6Z4skvZCQqnzFY63Bz0HPzXiKX79zHxc7G0iEJIUSuUxSF+8l5/5gVW2tdjo74f/zxx3z99deULFmSQoUKcenSJVq1asXnn3+OXq9nyZIltGnThlOnTlGiRInHnmfs2LFMnDiRSZMmMWPGDDp16kR4eDiFCxfOckz79u3jjTfeYMyYMbz55pvs3LmTvn374urqSrdu3di7dy8DBgzghx9+oG7duty+fZvt27cD6uhbx44dmThxIu3bt+fu3bts374dRVGy3Uf5iSRR+VDxQrYPkqh4KhVztnQ4QgiR6+4np1Jh1IY8b/f4uBDsbHLuT+W4ceNo1qyZ6X3hwoWpUqWK6f348eP59ddfWbNmDf369Xvsebp160bHjh0B+OKLL5g+fTq7d++mRYsWWY5pypQpNG3alE8//RSAMmXKcPz4cSZNmkS3bt2IiIjA3t6el19+GUdHR3x8fKhWrRqgJlEpKSm8+uqr+Pj4ABAQEJDlGPIruZyX3yTeo5vyG5OtZ3H5dryloxFCCJEFNWrUMHt/7949hg4dSvny5XFxccHBwYETJ04QERHxxPNUrlzZ9Nre3h4nJyfTY0yy6sSJE9SrV8+srF69epw5c4bU1FSaNWuGj48PJUuWpHPnzixdupT4ePXvT5UqVWjatCkBAQF06NCBefPmcefOnWzFkR/JSFR+o7Ph5VsLsNIlMy3yDFDK0hEJIUSus7XWcXxciEXazUn29vZm74cOHUpoaChff/01pUuXxtbWltdff52kpKQnnsfa2trsvUajwWg05misaRwdHdm/fz9bt25l48aNjBo1ijFjxrBnzx5cXFwIDQ1l586dbNy4kRkzZjBixAh27dqFn59frsTzPJGRqPzGyoZop3IA2F4/aNlYhBAij2g0GuxsrPJ8y+07oHfs2EG3bt1o3749AQEBeHh4cPHixVxt81Hly5dnx44d6eIqU6aM6ZlyVlZWBAcHM3HiRA4fPszFixf566+/APV3U69ePcaOHcuBAwewsbHh119/zdPPYCkyEpUPJbpXh5gjFIk9ZulQhBBCPAN/f39++eUX2rRpg0aj4dNPP821EaUbN25w8OBBszJPT08+/PBDatasyfjx43nzzTcJCwtj5syZzJo1C4A//viD8+fP07BhQwoVKsS6deswGo2ULVuWXbt2sXnzZpo3b07RokXZtWsXN27coHz58rnyGZ43MhKVD1mXUK+p+yWeeGHugBBCiIJoypQpFCpUiLp169KmTRtCQkKoXr16rrS1bNkyqlWrZrbNmzeP6tWr89NPP7F8+XIqVarEqFGjGDduHN26dQPAxcWFX375hSZNmlC+fHnmzJnDjz/+SMWKFXFycmLbtm20atWKMmXKMHLkSCZPnkzLli1z5TM8b2QkKh9y9q8Nm6A8F7lzN47CTg5PP0gIIUSe6datmykJAXXF8Iz+0evr62u6LJbmgw8+MHv/6OW9jM4THR39xHi2bt36xP2vvfYar732Wob76tev/9jjy5cvz/r165947oJMRqLyIX1Rf2JwQK9J5tb5A5YORwghhHghSRKVH2k0nLcpC0Dixd0WDkYIIYR4MUkSlU9FOVYCwDpSRqKEEEIIS5AkKp+KK6IutFb4zmELRyKEEEK8mCSJyqe0xdQ79IokhkNCjIWjEUIIIV48kkTlU0U8ihNhLKK+uSqX9IQQQoi89lwkUd9++y2+vr4YDAaCgoLYvfvJk6V//vlnypUrh8FgICAggHXr1pntVxSFUaNG4enpia2tLcHBwZw5c8a0f+vWrWg0mgy3PXv2mOodPnyYBg0aYDAY8Pb2ZuLEiTn7wZ9B8UK2HFLUR74ol/daOBohhBDixWPxJGrFihUMGTKE0aNHs3//fqpUqUJISMhjH6S4c+dOOnbsSI8ePThw4ADt2rWjXbt2HD161FRn4sSJTJ8+nTlz5rBr1y7s7e0JCQkhISEBgLp163Lt2jWz7b333sPPz8/0cMjY2FiaN2+Oj48P+/btY9KkSYwZM4bvvvsu9zslEzxdDBxUSgOQFLHnKbWFEEIIkdMsnkRNmTKFnj170r17dypUqMCcOXOws7NjwYIFGdafNm0aLVq04KOPPqJ8+fKMHz+e6tWrM3PmTEAdhZo6dSojR47klVdeoXLlyixZsoSrV6+yevVqAGxsbPDw8DBtrq6u/Pbbb3Tv3t30nKSlS5eSlJTEggULqFixIm+99RYDBgxgypQpedIvT6O30nHZVl1WX3tlH8jK5UIIIUSesmgSlZSUxL59+wgODjaVabVagoODCQsLy/CYsLAws/oAISEhpvoXLlwgMjLSrI6zszNBQUGPPeeaNWu4desW3bt3N2unYcOG2NjYmLVz6tQp7ty5k/UPmwvuFa5AiqLF+v4NiL1i6XCEEELkgMaNGzNo0CDTe19fX6ZOnfrEYzQajWmg4Fnk1HleFBZ97MvNmzdJTU3F3d3drNzd3Z2TJ09meExkZGSG9SMjI03708oeV+dR8+fPJyQkhOLFi5u14+fnl+4cafsKFSqU7jyJiYkkJiaa3sfGxgKQnJxMcnJyhm0/Kq1eZuq7ubhw8loJKmkukhKxG6Wc+1OPEaqs9LPIPunnvFOQ+jo5ORlFUTAajbn2MN7sSnvkSlp8D2vbti3Jycn8+eef6Y7bvn07jRs35sCBA1SuXDlT7aSdP21aytP6Iiv9NXbsWH777Tf2799vVn7lyhUKFSqUq/2+aNEihgwZwu3btx9b50n9nFOMRiOKopCcnIxOpzPbl9n/jl74Z+ddvnyZDRs28NNPPz3zuSZMmMDYsWPTlW/cuBE7O7ssnSs0NPSpdRJuajlkLEUl7UUubP+Z4+d1Tz1GmMtMP4tnJ/2cdwpCX1tZWeHh4cG9e/dISkqydDgZunv3brqyjh070qVLF06cOEGxYsXM9s2bN49q1arh6+tr+gf246SkpJCUlGSqp9frSUlJeepx9+/ff2qdNImJiaSmpqarb2dnl25AIKclJCSgKEqmYs2on3NKUlIS9+/fZ9u2baSkpJjti4+Pz9Q5LJpEubm5odPpiIqKMiuPiorCw8Mjw2M8PDyeWD/tZ1RUFJ6enmZ1qlatmu58CxcuxNXVlbZt22aqnYfbeNTw4cMZMmSI6X1sbCze3t40b94cJyenDI95VHJyMqGhoTRr1gxra+sn1o3bd4UDv5eiE5spZYjGt1WrTLUhstbPIvukn/NOQerrhIQELl26hIODAwaDwdLhmFEUhbt37+Lo6GiaQ5umQ4cOfPjhh/zyyy+MGDHCVH7v3j1+++03vvrqK5KTk+nfvz/bt2/nzp07lCpVio8//piOHTua6ltZWWFjY2P6u1GyZEkGDhzIwIEDAThz5gw9e/Zk9+7dlCxZkm+++QYAW1tb0zEff/wxq1ev5vLly3h4ePD222/z6aefYm1tzaJFi/jqq68ATFdV5s+fT7du3dDpdKxatYp27doBcOTIEQYPHkxYWBh2dna8+uqrTJ48GQcH9cH33bt3Jzo6mvr16zNlyhSSkpJ48803+eabbx77PTQYDGg0msf+XYyIiGDAgAH89ddfaLVaQkJCmD59uulq0KFDhxgyZAh79+5Fo9Hg7+/P7NmzqVGjBuHh4fTv358dO3aQlJSEr68vX331Fa0y+PuYkJCAra0tDRs2TPc9y2wyatEkysbGhsDAQDZv3mz6hRmNRjZv3ky/fv0yPKZOnTps3rzZ7HpxaGgoderUAcDPzw8PDw82b95sSppiY2PZtWsXffr0MTuXoigsXLiQLl26pPtl16lThxEjRpCcnGzaFxoaStmyZTO8lAfqvxb0en26cmtr6yz/Ty0zx/i6ObDQqN6hp712CK1OC1oZjcqK7PxuRNZJP+edgtDXqampaDQatFotWu2DqbuKAsmZGx3IUdZ28FCylHZpKS2+h9nY2NClSxcWL17MyJEjTUnWqlWrSE1NpVOnTty7d48aNWrw8ccf4+TkxNq1a+natSv+/v7UqlXLdK5Hz5/23mg08vrrr+Pu7s6uXbuIiYkx/T18uL+cnJxYtGgRXl5eHDlyhJ49e+Lk5MSwYcPo2LEjx48fZ/369WzatAlQ5w6nHZt2nri4OFq2bEmdOnXYs2cP169f57333mPAgAEsWrTIFNfWrVvx8vJiy5YtnD17ljfffJNq1arRs2fPDLv04XYeZTQaad++PQ4ODvzxxx/o9Xr69+9Px44d2bp1KwCdO3emWrVqzJ49G51Ox8GDB9Hr9Wi1Wvr3709SUhLbtm3D3t6e48eP4+TklGFbWq0WjUaT4X8zmf1vyOKX84YMGULXrl2pUaMGtWrVYurUqcTFxZkmeXfp0oVixYoxYcIEAAYOHEijRo2YPHkyrVu3Zvny5ezdu9e09IBGo2HQoEF89tln+Pv74+fnx6effoqXl5cpUUvz119/ceHCBd577710cb399tuMHTuWHj168H//938cPXqUadOmmTL+50HxQnacU7yIUwzYJ8fBjZPgXtHSYQkhRM5LjocvvPK+3U+ugo19pqu/++67TJo0ib///pvGjRsD6hWP1157DWdnZ5ydnRk6dKipfv/+/U1TSh5Ooh5n06ZNnDx5kg0bNuDlpfbHF198QcuWLc3qjRw50vTa19eXoUOHsnz5coYNG4atrS0ODg6my6aPs2zZMhISEliyZAn29mofzJw5kzZt2vDVV1+ZRoYKFSrEzJkz0el0lCtXjtatW7N58+bHJlFPsnnzZo4cOcK5c+dwdnbGycmJJUuWULFiRfbs2UPNmjWJiIjgo48+oly5cgD4+/ubjo+IiOC1114jICAAUEfxcpPFlzh48803+frrrxk1ahRVq1bl4MGDrF+/3vTLiYiI4Nq1a6b6devWZdmyZXz33XdUqVKFlStXsnr1aipVqmSqM2zYMPr370+vXr2oWbMm9+7dY/369emG6+bPn0/dunVNv4iHOTs7s3HjRi5cuEBgYCAffvgho0aNolevXrnUE1nn6WIAjZZDxgdfkiv7LBuQEEK84MqVK0fdunVNy/ScPXuW7du306NHD0AdZRs/fjwBAQEULlwYBwcHNmzYQERERKbOf+LECby9vU0JFGC6EvOwFStWUK9ePTw8PHBwcGDkyJGZbuPhtqpUqWJKoADq1auH0Wjk1KlTprKKFSuaTcz29PR87FqPmWnT29sbb29vU1mFChVwcXHhxIkTgDr48t577xEcHMyXX37JuXPnTHUHDBjAZ599Rr169Rg9ejSHD+fu82UtPhIF0K9fv8devksbvntYhw4d6NChw2PPp9FoGDduHOPGjXtiu8uWLXvi/sqVK7N9+/Yn1rEka50WT2dbDt0rRV2Oq0lU9S6WDksIIXKetZ06KmSJdrOoR48e9O/fn2+//ZaFCxdSqlQpGjVqBMCkSZOYNm0aU6dOJSAgAHt7ewYNGpSjE+jDwsLo1KkTY8eOJSQkBGdnZ5YvX87kyZNzrI2HPXrpS6PR5OrdfWPGjOHtt99m7dq1/Pnnn4wePZrly5fTvn173nvvPUJCQli7di0bN25kwoQJTJ48mf79++dKLBYfiRLPplghWw4+mBfFZRmJEkIUUBqNelktr7dHJo9nxhtvvIFWq2XZsmUsWbKEd9991zQ/aseOHbzyyiu88847VKlShZIlS3L69OlMn7t8+fJcunTJ7ArNv//+a1Zn586d+Pj4MGLECGrUqIG/vz/h4eFmdWxsbEhNTX1qW4cOHSIuLs5UtmPHDrRaLWXLls10zFmR9vkuXbpkKjt+/DjR0dFUqFDBVFamTBkGDx7Mxo0befXVV1m4cKFpn7e3N++//z6//PILH374IfPmzcuVWEGSqHzPu5AdB43qM/S4fhyS4p58gBBCiFzl4ODAm2++yfDhw7l27RrdunUz7fP39yc0NJSdO3dy4sQJevfune5O8CcJDg6mTJkydO3alUOHDrF9+3azOwHT2oiIiGD58uWcO3eO6dOn8+uvv5rV8fX15cKFCxw8eJCbN29muKRBp06dMBgMdO3alaNHj7Jlyxb69+9P586d063FmFWpqakcPHjQbDtx4gTBwcEEBATQuXNnDh06xO7du+nSpQuNGjWiRo0a3L9/n379+rF161bCw8PZsWMHe/bsoXx59QkegwYNYsOGDVy4cIH9+/ezZcsW077cIElUPle8kC1RFCbGyg2UVLiWu9d/hRBCPF2PHj24c+cOISEhZvOXRo4cSfXq1QkJCaFx48Z4eHiku+npSbRaLb/++iv379+nVq1avPfee3z++edmddq2bcvgwYPp168fVatWZefOnXz66admdV577TVatGjBSy+9RJEiRfjxxx/TtWVnZ8eGDRu4ffs2NWvW5PXXX6dp06amx6w9i3v37lGtWjWzrU2bNmg0Gn777TdcXFxo3bo1zZs3p2TJkqxYsQIAnU7HrVu36NKlC2XKlOGNN96gZcuWpjUaU1NT+eCDDyhfvjwtWrSgTJkyzJo165njfRyNoshD13JLbGwszs7OxMTEZGmdqHXr1tGqVatM3WL5895LfLTyMD8X+paa93dA88+gbu5c+y1IstrPInukn/NOQerrhIQELly4gJ+f33O3TpTRaCQ2Nvaxt82LnJEX/fyk71lm/37LNyCf8y6sTno8kCp36AkhhBB5SZKofK54IVsAtsf7qgUyuVwIIYTIE5JE5XMeTgZ0Wg0HUnxQ0EBMBNzL3vocQgghhMg8SaLyOSudFk9nA/ewI8HlwVIHV/Y/+SAhhBBCPDNJogoA70LqvKjrTg9Wbb+y14LRCCFEzpD7nkRuyonvlyRRBUDavKjzNg8WP5PJ5UKIfCzt7sL4eAs8cFi8MNK+X89yN+tz8dgX8WyKPxiJOqyU5iVQkyhFydZKu0IIYWk6nQ4XFxfT89fs7OxMK35bmtFoJCkpiYSEBFniIBflZj8rikJ8fDzXr1/HxcXF7Ll/WSVJVAHgXVgdidqX4AlWBkiIgVvnwK20hSMTQojs8fDwAMj2g2xzi6Io3L9/H1tb2+cmsSuI8qKfXVxcTN+z7JIkqgBIG4kKj04GzypwaZc6GiVJlBAin9JoNHh6elK0aFGSk5MtHY5JcnIy27Zto2HDhvl+UdPnWW73s7W19TONQKWRJKoASJsTdTX6PsZKgWjTkqgqb1o4MiGEeDY6nS5H/tjlFJ1OR0pKCgaDQZKoXJRf+lku6BYA7k4GrHUaklMVogtXVgvlDj0hhBAiV0kSVQDotBq8XNTRqEu2D55WHXkEUtI/lVsIIYQQOUOSqAIi7ZLeuWRXsHOF1CSIPGrhqIQQQoiCS5KoAqK4izq5/NKdBCgWqBbKelFCCCFErpEkqoBIW+bg8p14KFZDLZQkSgghhMg1kkQVEGnLHFy+c/+hkSiZXC6EEELkFkmiCoi0OVGX7sRDsepq4a2zEHfLglEJIYQQBZckUQVE2kjUtZgEUvQu6qKbAIdXWC4oIYQQogCTJKqAKOqox0anJdWoEBmbANW7qjv2LVSfoyeEEEKIHCVJVAGh1WooVihtcvl9COgA1vZw8zSE77RwdEIIIUTBI0lUAWKaF3U7HgxOEPC6umPvAgtGJYQQQhRMkkQVIGZ36AHU6K7+PLFGJpgLIYQQOUySqAKk+MOX8wC8qqlbahIcXGrByIQQQoiCR5KoAsRsmYM0gQ9Go/YtBKPRAlEJIYQQBZMkUQVI2uW8K2kjUQCVXgO9E9w+Dxe3WSgyIYQQouCRJKoASXv0y7WY+ySnPhh10jtA5TfU13sXWigyIYQQouCRJKoAKeKgR2+lxahAZEzCfzvSLumd/APuRlkmOCGEEKKAkSSqANFo/lsr6tLth+ZFeVSC4rXAmAIH/2eh6IQQQoiCRZKoAsb70WUO0qQtd7BvkUwwF0IIIXKAJFEFzH/LHMSb76jYHgzOEB0B5/6yQGRCCCFEwSJJVAGTdofepUdHoqxtoUpH9fU+mWAuhBBCPCtJogqYx45EwX8TzE/9CbFX8zAqIYQQouCRJKqA8S78mDlRAEXLQYm6oKTC/h/yODIhhBCiYJEkqoBJG4mKjE0gKSWDCeRpE8z3LwFjah5GJoQQQhQskkQVMK72Ntha61AUuBqdwWhU+bZgWxhiL8OZ0LwPUAghhCggJIkqYDQaTfoHET/M2gBV31Zf712Qh5EJIYQQBYskUQXQEyeXw38TzM+GQvSlPIpKCCGEKFgkiSqA/lvm4DFJlFtp8GsIilGdGyWEEEKILJMkqgB64uW8NIEPTTBPTc6DqIQQQoiCRZKoAuiJyxykKfcy2BeFe5Gwe14eRSaEEEIUHJJEFUBPnRMFYGUDTUaor7d+Cfdu5EFkQgghRMEhSVQBlDYnKio2kYTkJ6wFVa0zeFaBxBj4a1weRSeEEEIUDJJEFUCF7Kxx1FsBcOFm3OMranXQcqL6ev8PcGV/HkQnhBBCFAySRBVAGo2GgOLOAByIiH5y5RK1ofKbgAJ//h8YM1jlXAghhBDpSBJVQAX6FAJgX/idp1cOHgvW9nB5Nxz5KZcjE0IIIQoGSaIKqOoPkqj9EZlIopw8oeFQ9XXoaEi8m4uRCSGEEAWDxZOob7/9Fl9fXwwGA0FBQezevfuJ9X/++WfKlSuHwWAgICCAdevWme1XFIVRo0bh6emJra0twcHBnDlzJt151q5dS1BQELa2thQqVIh27dqZ7ddoNOm25cuXP/PnzSvVvdUk6sLNOG7HJT39gDofQCE/dcmDbV/ncnRCCCFE/mfRJGrFihUMGTKE0aNHs3//fqpUqUJISAjXr1/PsP7OnTvp2LEjPXr04MCBA7Rr14527dpx9OhRU52JEycyffp05syZw65du7C3tyckJISEhARTnVWrVtG5c2e6d+/OoUOH2LFjB2+//Xa69hYuXMi1a9dM26OJ1vPM2c6a0kUdANifmUt6Vnpo8aX6OuxbuHUuF6MTQggh8j+LJlFTpkyhZ8+edO/enQoVKjBnzhzs7OxYsCDjB+NOmzaNFi1a8NFHH1G+fHnGjx9P9erVmTlzJqCOQk2dOpWRI0fyyiuvULlyZZYsWcLVq1dZvXo1ACkpKQwcOJBJkybx/vvvU6ZMGSpUqMAbb7yRrj0XFxc8PDxMm8FgyLW+yA2BJR7Mi8rMJT2AMiFQOhiMybB+eC5GJoQQQuR/FkuikpKS2LdvH8HBwf8Fo9USHBxMWFhYhseEhYWZ1QcICQkx1b9w4QKRkZFmdZydnQkKCjLV2b9/P1euXEGr1VKtWjU8PT1p2bKl2WhWmg8++AA3Nzdq1arFggULUBTlmT93XsrS5HIAjUYdjdJawZkNcHpjLkYnhBBC5G9Wlmr45s2bpKam4u7ublbu7u7OyZMnMzwmMjIyw/qRkZGm/Wllj6tz/vx5AMaMGcOUKVPw9fVl8uTJNG7cmNOnT1O4cGEAxo0bR5MmTbCzs2Pjxo307duXe/fuMWDAgMd+psTERBITE03vY2NjAUhOTiY5OXPPp0url9n6TxLgpV7OO3w5mviERKx1mciZnX3R1uqN7t9vUdb/Hykl6oHO5pljed7kZD+Lx5N+zjvS13lD+jlvWLqfM9uuxZIoSzE+WAdpxIgRvPbaa4A696l48eL8/PPP9O7dG4BPP/3UdEy1atWIi4tj0qRJT0yiJkyYwNixY9OVb9y4ETs7uyzFGRoamqX6GTEqYKfTEZ9sZP6q9ZRwyNxxVqmVaWrljOH2eU4vGcxZ99bPHMvzKif6WTyd9HPekb7OG9LPecNS/Rwf/4THpj3EYkmUm5sbOp2OqKgos/KoqCg8PDwyPMbDw+OJ9dN+RkVF4enpaVanatWqAKbyChUqmPbr9XpKlixJRETEY+MNCgpi/PjxJCYmotfrM6wzfPhwhgwZYnofGxuLt7c3zZs3x8nJ6bHnflhycjKhoaE0a9YMa2vrTB3zJKtv7+fv0zex9a5Iqzo+mT5O45MMv/ejws0/KPP6p+CY8e8kv8rpfhYZk37OO9LXeUP6OW9Yup/TriQ9jcWSKBsbGwIDA9m8ebPprjej0cjmzZvp169fhsfUqVOHzZs3M2jQIFNZaGgoderUAcDPzw8PDw82b95sSppiY2PZtWsXffr0ASAwMBC9Xs+pU6eoX78+oP6yLl68iI/P45OMgwcPUqhQoccmUKAmYxntt7a2zvKXIDvHZKSGT2H+Pn2TQ1fuZu181TrB/oVoruzD+u8voP3sZ47leZRT/SyeTPo570hf5w3p57xhqX7ObJsWvZw3ZMgQunbtSo0aNahVqxZTp04lLi6O7t27A9ClSxeKFSvGhAkTABg4cCCNGjVi8uTJtG7dmuXLl7N3716+++47QF3badCgQXz22Wf4+/vj5+fHp59+ipeXlylRc3Jy4v3332f06NF4e3vj4+PDpEmTAOjQoQMAv//+O1FRUdSuXRuDwUBoaChffPEFQ4cOzeMeenZpk8sztczBw7RaaDkJvm8Ch5ZBlbegZKNciFAIIYTInyyaRL355pvcuHGDUaNGERkZSdWqVVm/fr1pYnhERARa7X+ToevWrcuyZcsYOXIkn3zyCf7+/qxevZpKlSqZ6gwbNoy4uDh69epFdHQ09evXZ/369WbLE0yaNAkrKys6d+7M/fv3CQoK4q+//qJQITXhsLa25ttvv2Xw4MEoikLp0qVNyzHkN1W8XdBq4Er0fSJjEvBwzsIyDcUDoca7sHcBrO4LfXeCwTn3ghVCCCHyEYtPLO/Xr99jL99t3bo1XVmHDh1MI0YZ0Wg0jBs3jnHjxj22jrW1NV9//TVff53xytwtWrSgRYsWTw48n7DXW1He04ljV2PZH3GHVgGeTz/oYc3Gw7ktcOeC+oDi9nNyJ1AhhBAin7H4Y19E7qteIovrRT1M7wDt54JGC4d+hONrcjg6IYQQIn+SJOoFkOVFNx9VIgjqDVJf/z4Q7kY9sboQQgjxIpAk6gWQlkQduxpDQnJq9k7SeDi4B8D92/D7AMhnq7cLIYQQOU2SqBdA8UK2uDnoSU5VOHolJnsnsbKBV+eqq5efXg8HfsjZIIUQQoh8RpKoF4BGoyHQxwV4hkt6AO4VoclI9fX64XD7wrMHJ4QQQuRTkkS9IJ55XlSaOv2gRF1Iuger+4Axm5cHhRBCiHxOkqgXhGnRzYholGeZz6TVqauX2zhARBiEzcyhCIUQQoj8RZKoF0RFL2esdRpu3kvk0u37z3ayQr7QQl1Fnr8+g8ijzxyfEEIIkd9IEvWCMFjrqFRMXW18X8TtZz9htc5QpiWkJsGvvSEl8dnPKYQQQuQjkkS9QAKfZdHNR2k00HY62LlC1FHYOuHZzymEEELkI5JEvUCqmx5GHJ0zJ3QoCm2mqa93TIMr+3PmvEIIIUQ+IEnUCyRtcvnJyFjuJabkzEnLt4FKr4NihHVDwWjMmfMKIYQQzzlJol4g7k4GirnYYlTg0KXonDtxyOdg4whX9skinEIIIV4YkkS9YExLHeTEvKg0jh7w0nD19aYxEJ8DE9eFEEKI55wkUS+Y6iVcANgXkYNJFECtXlCkvPpsvb/G5+y5hRBCiOeQJFEvmECfwoA6EmU05uBDhHXW0Ppr9fXehXD1QM6dWwghhHgOSRL1ginn6YittY7YhBTO3biXsyf3rQ8BHQAF1sokcyGEEAWbJFEvGGudlsrF1UU39+f0JT2AZuMfTDLfCwf/l/PnF0IIIZ4TkkS9gHLsYcQZcfKExh+rr0NHyyRzIYQQBZYkUS+gXE2iAIJ6PzTJ/LPcaUMIIYSwMEmiXkDVHjz+5dyNOKLjk3K+AZ01tJqkvt67QCaZCyGEKJAkiXoBFba3oaSbPQAHIqJzpxG/BupK5jLJXAghRAElSdQLqnpuX9IDaP4Z2DjIJHMhhBAFkiRRL6hcnxcF5pPMZSVzIYQQBYwkUS+o6g/mRR26HE1yai5eagt6H4qUg/hbMslcCCFEgSJJ1AvKv6gDheysiU9K5dPVR1GUHFy9/GE6a2iVtpL5Aji0InfaEUIIIfKYJFEvKK1WwxftA9BqYPmeS4z/40TuJVJ+DaDme4ACv/aGPfNzpx0hhBAiD0kS9QJrGeDJV69VBmDBjgt8s+lMLjY2SX1IMQqsHQI7puVeW0IIIUQekCTqBdehhjdj2lQAYPrmM3y37VzuNKTVQsuJ0OBD9X3oKHWOVG6NfgkhhBC5TJIoQbd6fnwUUhaAL9adZOmu8NxpSKOBpqOg6Wj1/bZJsH64JFJCCCHyJUmiBAAfvFSaPo1LATBy9VFWH7iSe401GPLfZPNds2FNfzCm5l57QgghRC6QJEqYDAspS5c6PigKfPjzITYci8y9xmr1hHazQaOFAz/Aqh6QkguPoBFCCCFyiSRRwkSj0TCmTUVeq16cVKNC/2UH2H7mRu41WPVt6LAItNZw7FdY8Q4k38+99oQQQogcJEmUMKPVavjqtQBaVvIgKdVIryX72HMxF1car/AKdFwOVrZwZgMs7QApibnXnhBCCJFDJIkS6VjptEx7qxqNyhThfnIq7y7aw/XYhNxr0D8Y3lkFNo5wcTuEzcy9toQQQogcIkmUyJCNlZY57wRSqZgTdxNS+P6fC7nboG89eHmK+nrb1xCTixPbhRBCiBwgSZR4LFsbHYODywCw9N9wYuKTc7fBgA7gHQTJ8bBpdO62JYQQQjwjSaLEEzUpV5RyHo7EJaWyOOxi7jam0agLcqKBIz9DeFjutieEEEI8A0mixBNpNBrT+lELd1wgPikldxv0qgqBXdXXf34k60cJIYR4bkkSJZ6qdYAnJQrbcSc+meW7L+V+g00+BYMzRB6BfYtyvz0hhBAiGySJEk9lpdPSu1FJAOZtP09SijF3G7R3g5dGqK//Gg/xubjEghBCCJFNkkSJTHmtenGKOOq5FpPA6oN5cOdcjR5QpDzcvwNbvsj99oQQQogskiRKZIrBWsd79f0AmPP3OVKNufzQYJ0VtJqovt47HyKP5m57QgghRBZJEiUyrVNtH5wMVpy/EcfG3HyuXhq/hlChHShG+PP/QMnlxE0IIYTIAkmiRKY56K3oWtcXgFlbz6HkRVLTfLz6SJjwf9Tn6wkhhBDPCUmiRJZ0q+uLwVrLkSsx/HP2Zu436FIC6g9WX2/8FJLicr9NIYQQIhMkiRJZ4uqg562aJQCYteVc3jRabwA4l4DYy/DPN3nTphBCCPEUkkSJLOvZsCRWWg1h529xIOJO7jdobQshn6uvd0yH27n8HD8hhBAiEySJEllWzMWWdtWKAercqDxRvg34NYLURNg4Mm/aFEIIIZ5AkiiRLe83KoVGA6HHozgddTf3G9RooOVXoNHByT/g39m536YQQgjxBBZPor799lt8fX0xGAwEBQWxe/fuJ9b/+eefKVeuHAaDgYCAANatW2e2X1EURo0ahaenJ7a2tgQHB3PmzJl051m7di1BQUHY2tpSqFAh2rVrZ7Y/IiKC1q1bY2dnR9GiRfnoo49IScnl58blI6WLOhBSwQOAOXk1GlW0PLw0XH29/mPYPS9v2hVCCCEyYNEkasWKFQwZMoTRo0ezf/9+qlSpQkhICNevX8+w/s6dO+nYsSM9evTgwIEDtGvXjnbt2nH06H8LMU6cOJHp06czZ84cdu3ahb29PSEhISQkJJjqrFq1is6dO9O9e3cOHTrEjh07ePvtt037U1NTad26NUlJSezcuZPFixezaNEiRo0alXudkQ/1fUl9MPFvh65y6XZ83jTaYCjUG6S+XjcU9i7Mm3aFEEKIRykWVKtWLeWDDz4wvU9NTVW8vLyUCRMmZFj/jTfeUFq3bm1WFhQUpPTu3VtRFEUxGo2Kh4eHMmnSJNP+6OhoRa/XKz/++KOiKIqSnJysFCtWTPn+++8fG9e6desUrVarREZGmspmz56tODk5KYmJiZn+fDExMQqgxMTEZPqYpKQkZfXq1UpSUlKmj7GkTvP+VXz+7w/l09VH8q5Ro1FR1n+iKKOd1G3/D1k+RX7r5/xK+jnvSF/nDennvGHpfs7s32+LjUQlJSWxb98+goODTWVarZbg4GDCwsIyPCYsLMysPkBISIip/oULF4iMjDSr4+zsTFBQkKnO/v37uXLlClqtlmrVquHp6UnLli3NRrPCwsIICAjA3d3drJ3Y2FiOHTv27B++AOnbWB2NWrHnEnfikvKmUY0Gmn8GQe+r73/rB4eW503bQgghxANWlmr45s2bpKammiUqAO7u7pw8eTLDYyIjIzOsHxkZadqfVva4OufPnwdgzJgxTJkyBV9fXyZPnkzjxo05ffo0hQsXfmw7D7eRkcTERBITE03vY2NjAUhOTiY5Ofmxxz0srV5m61tajRJOlCnqwOnr99hyMpI2lT3zrvGm49EmJ6LbvxBldR9SFVAqvpapQ/NbP+dX0s95R/o6b0g/5w1L93Nm27VYEmUpRqMRgBEjRvDaa+of3IULF1K8eHF+/vlnevfune1zT5gwgbFjx6Yr37hxI3Z2dlk6V2hoaLbjyGvFdVpOo2XF1kPoLh/I28aVRlRxPY/vrb/Rru7DvoNHuFqoVqYPz0/9nJ9JP+cd6eu8If2cNyzVz/HxmZvna7Ekys3NDZ1OR1RUlFl5VFQUHh4eGR7j4eHxxPppP6OiovD09DSrU7VqVQBTeYUKFUz79Xo9JUuWJCIiwnSeR+8STGv3cbEBDB8+nCFDhpjex8bG4u3tTfPmzXFycnrscQ9LTk4mNDSUZs2aYW1tnaljLM3xzE3+WrKfiCQ7WrZsgEajydsAlFYY/xiI9vCP1IiYQ2qNWihlWz3xkPzYz/mR9HPekb7OG9LPecPS/Zx2JelpLJZE2djYEBgYyObNm03LCxiNRjZv3ky/fv0yPKZOnTps3ryZQYMGmcpCQ0OpU6cOAH5+fnh4eLB582ZT0hQbG8uuXbvo06cPAIGBgej1ek6dOkX9+vUB9Zd18eJFfHx8TO18/vnnXL9+naJFi5racXJyMku+HqXX69Hr9enKra2ts/wlyM4xllK7dBFsdFquxSRwJTYZPzf7vA+i3beAEc3hFVj90gPe/B+UbfHUw/JTP+dn0s95R/o6b0g/5w1L9XNm27ToEgdDhgxh3rx5LF68mBMnTtCnTx/i4uLo3r07AF26dGH48OGm+gMHDmT9+vVMnjyZkydPMmbMGPbu3WtKujQaDYMGDeKzzz5jzZo1HDlyhC5duuDl5WVK1JycnHj//fcZPXo0Gzdu5NSpU6YEq0OHDgA0b96cChUq0LlzZw4dOsSGDRsYOXIkH3zwQYZJ0ovOzsaK6j4uAHnzUOKMaHXwyiyo9BoYk2FFJ/j1fYiSGwGEEELkDovOiXrzzTe5ceMGo0aNIjIykqpVq7J+/XrTJO6IiAi02v/yvLp167Js2TJGjhzJJ598gr+/P6tXr6ZSpUqmOsOGDSMuLo5evXoRHR1N/fr1Wb9+PQaDwVRn0qRJWFlZ0blzZ+7fv09QUBB//fUXhQoVAkCn0/HHH3/Qp08f6tSpg729PV27dmXcuHF51DP5T/3Sbvx7/jY7ztykc20fywShs4L236mrmh/5CQ79qG6lm6kPMfZtoN7ZJ4QQQuQAjaIoiqWDKKhiY2NxdnYmJiYmS3Oi1q1bR6tWrfLVUPGBiDu0n7UTJ4MVB0Y1R6e1cLJyZZ/6sOITa0BRbybAqxrUHQDl25JsVPJlP+c3+fX7nB9JX+cN6ee8Yel+zuzfb4s/9kUUDAHFnHE0WBGbkMLRKzGWDgeKBcIbi6H/Pqj5HlgZ4OoBWNkdZgai3TsfnTHx6ecRQgghHuOFW+JA5A4rnZY6JV3ZeDyKf87epIq3i6VDUhUuCa0nQ+Ph6rP2dn8Hdy6i2/B/tNRYoT07AgzOYHACvdMjr13AvQKUbSWXAYUQQqQjSZTIMfX93dh4PIodZ2/ywUulLR2OOXs39eHF9QbCwaUoO2eii74IcdfV7UmajVfnVAkhhBAPkSRK5Jh6pd0A2HvxDveTUrG10Vk4ogzY2EGtnqRU6cyWNUt5qU51rFPiICEWEmMhIebB6xi4E67OqfprPJR6CTwCLB29EEKI54gkUSLHlHSzx9PZwLWYBPaG36aBfxFLh/R4Wh33bdzAvRI8btKiosDyTnBqLazqCb22gLVt3sYphBDiuSUTy0WO0Wg0ptEoi60XlZM0Gmg7HeyLwo0TsCn9I32EEEK8uCSJEjmq/oMkakdBSKJAnUvVbpb6etdsOLvZsvEIIYR4bkgSJXJU3dKuABy7GsvtuCQLR5ND/JtBzZ7q69V9If62ZeMRQgjxXJAkSuSooo4Gyro7oigQdu6WpcPJOc3GgVsZuBcJvw9Q50sJIYR4oWUribp06RKXL182vd+9ezeDBg3iu+++y7HARP5VoOZFpbGxg1fngdYKTvwOB5dZOiIhhBAWlq0k6u2332bLli0AREZG0qxZM3bv3s2IESPk+XKC+v7qJb0CMy8qjVdVeGmE+vrPYXD7vEXDEUIIYVnZSqKOHj1KrVq1APjpp5+oVKkSO3fuZOnSpSxatCgn4xP5UC0/V6y0GiJux3Ppdrylw8lZ9QaCTz1Iuge/9IbUFEtHJIQQwkKylUQlJyej1+sB2LRpE23btgWgXLlyXLt2LeeiE/mSg96Kqg8e+1LgRqO0Omg/R30szOXd8M8US0ckhBDCQrKVRFWsWJE5c+awfft2QkNDadGiBQBXr17F1dU1RwMU+VOBnBeVxqWE+jw+gK1fwuV95vsVRV31/PZ5uLQHTv0J4TtlMroQQhQw2Vqx/KuvvqJ9+/ZMmjSJrl27UqVKFQDWrFljuswnXmz1/d2YtvkMO8/dwmhU0GoL2AN8AzrA6fVwdBWseAeKlIX4mxB3S/2ZmsHyDrV6QcuJ8jBjIYQoILKVRDVu3JibN28SGxtLoUKFTOW9evXCzs4ux4IT+VdVbxfsbXTcjkviRGQsFb2cLR1SztJo1NGoiH8h9grcvZq+jrU92LuCbWG4dgh2fwcaLbT4UhIpIYQoALKVRN2/fx9FUUwJVHh4OL/++ivly5cnJCQkRwMU+ZO1TktQSVf+OnmdHWdvFrwkCsC2EHT9Hc5sBIOLurq5neuDn27qsghp9v8Aa/rBrjmg0UHI55JICSFEPpetOVGvvPIKS5YsASA6OpqgoCAmT55Mu3btmD17do4GKPKv/+ZFFaBFNx/lWgpq94GqHdWVzYtVV+dM2TwyIlu9M7SZpr7+91sI/VTmSAkhRD6XrSRq//79NGjQAICVK1fi7u5OeHg4S5YsYfr06TkaoMi/0p6jt/vCLRJTUi0czXMgsBu8/I36eucM2DRGEikhhMjHspVExcfH4+joCMDGjRt59dVX0Wq11K5dm/Dw8BwNUORfZdwdcHPQk5BsZH94tKXDeT7UeBdafa2+3jEVNo+TREoIIfKpbCVRpUuXZvXq1Vy6dIkNGzbQvHlzAK5fv46Tk1OOBijyL41GQ/3SBXT18mdRqye0nKS+/mcKbPlcEikhhMiHspVEjRo1iqFDh+Lr60utWrWoU6cOoI5KVatWLUcDFPlbgV4v6lkE9VLv0gPYNkldb0oIIUS+kq27815//XXq16/PtWvXTGtEATRt2pT27dvnWHAi/0tLog5fjibmfjLOttYWjug5UrsPKEbY8An8/aW6/EGjYXLXnhBC5BPZGokC8PDwoFq1aly9epXLly8DUKtWLcqVK5djwYn8z8vFlpJF7DEq8O/5AnyXXnbV+QCajVdfb/0CpleFrV/BHZlbKIQQz7tsJVFGo5Fx48bh7OyMj48PPj4+uLi4MH78eIxGY07HKPK5tLv0ZF7UY9QbAC2+AhtHuHNRTaamVYZFL8PBZZB4z9IRCiGEyEC2LueNGDGC+fPn8+WXX1KvXj0A/vnnH8aMGUNCQgKff/55jgYp8rd6pd1YEhYu86KepPb76lpSJ/6AQ8vg/N9wcbu6rR0KFdtB1behRF3QPvJvH6NRfcxMaiKkJoOVHvSOFvkYQgjxIslWErV48WK+//572rZtayqrXLkyxYoVo2/fvpJECTO1S7qi1cD5G3H8dvAKr1QtZumQnk829lDlTXWLvgSHl6sjUbfPw8Gl6mbnClorNWlKeZA4GVPMz6PRqkspNBmprqouhBAiV2Trct7t27cznPtUrlw5bt++/cxBiYLF2daaVgGeAAxcfpCByw8QE59s4aiecy7e0PAj6L8f3t0A1buol/vib8G9KLh/B5Lj0idQoE5W3/M9zKgBB5aqI1VCCCFyXLZGoqpUqcLMmTPTrU4+c+ZMKleunCOBiYLlmzerUqqIAzO3nOW3g1fZfeE2kztUoe6D+VLiMTQaKFFb3Vp8BTdOqiNRVnrQWYNO/+C1zX9b+A5YN1St+1tf2L9YXeDTU/7bFEKInJStJGrixIm0bt2aTZs2mdaICgsL49KlS6xbty5HAxQFg7VOy+BmZWhctgiDVxzk4q143v5+Fz3q+/FRSFkM1jpLh/j8s7FTn833NH4N4P1/4N/Z6vpTl3bBd42gZk9oMgIMBfBh0EIIYQHZupzXqFEjTp8+Tfv27YmOjiY6OppXX32VY8eO8cMPP+R0jKIAqVaiEOsGNuDtoBIAzP/nAq/M3MHxq7EWjqyA0Vmrd/312wMV26uX+HbPVS/xHVouK6QLIUQOyPY6UV5eXnz++eesWrWKVatW8dlnn3Hnzh3mz5+fk/GJAsjOxoov2gcwv2sN3BxsOBV1l3bf7mDu3+dINcof9xzlXAw6LILOv4JraYi7Dr/2hoWt4OYZS0cnhBD5WraTKCGeVdPy7qwf1JDg8u4kpRqZ8OdJ2s/awVfrT7Lm0FXOXr8rSVVOKdUE+uyEpqPAyhYidsKc+rBzBhhTLR2dEELkS9maEyVETnFz0DOvSyA/7b3E2N+Pc/hyDIcvx5j26620lPVwpLyHE+U9HSnv6UTFYs446OWrm2VWemjwIQR0gN8Hwrm/YONIOPE7vDIL3EpbOkIhhMhX5C+RsDiNRsObNUvQwL8Im09e58S1WE5ci+XktbvcT05Nl1i5Oej5qXdtShZxsGDU+ZhLCXjnF9i/BDaMUCeez6kHTT5Vn+enlUn+QgiRGVlKol599dUn7o+Ojn6WWMQLzsvFls61fUzvjUaF8NvxHL8aa0qsDl6K5ua9RCauP8WczoEWjDaf02ggsKt6mW9Nfzi/BTaOeDAq9a2MSgkhRCZkKYlydn7yrdHOzs506dLlmQISIo1Wq8HPzR4/N3taV1YX6zwddZcWU7ex/lgkByLuUK2ErMj9TFy81Unn+xfDhpFw6V91VKrpKAh6X0alhBDiCbKURC1cuDC34hAiU8q4O/Jq9eKs3HeZr9af5MeetdFoNJYOK3/TaCCwG5RqCmv6wfmtsOETOLISPCqBleHBgp76/16bfupBo1PPodGm2zRGI253j0NKU7C2tvQnFUKIHCVzokS+M7hZGdYcusq/52/z9+kbNC5b1NIhFQwu3tB5NexbpE44v7pf3Z6BFVAPUKZ/B1U6qo+vKZr+kVFCCJEfSRIl8p1iLrZ0qe3D9/9cYOL6UzT0L4JWK6NROUKjgRrdoXQwnPwDku6pDzpOSYCUxPQ/UxPVhTwVo7qAp+m1uhmNqSRdP4fh/m3491t1K15LnY9VoR3o5eYAIUT+JUmUyJf6vlSaFXsucfxaLL8fvsorVYtZOqSCxcVbvVPvGaUmJ7Nx7e+0KqPH6vAyOPUnXN6tbn9+DAGvqaNTXtXVBE4IIfIRSaJEvlTY3oZeDUsyOfQ0kzeepmUlT2ysZO3Y55Gi0aH4N4cKreFuFBxapi6vcPu8eulw3yJwKwuFS4JtIbB1UX8aXMzf2xYC+yKgd5SESwjxXJAkSuRbPRr4sTgsnIjb8SzfE0GXOr6WDkk8jaM71B8M9QbBxX/UZOr4b3DzlLplhpUtOBQBB3ewLwoOD22OXuBbHwxO2YsvIQYOrYDTf6oruWut1E1nrd6pqLV+8N5KnWjv4g2FS4FrKSjkpz4kWgjxwpAkSuRbdjZWDGxamk9/O8b0zWd4rXpx7GUl8/xBowG/BurWaiJc2A7xtyAhGu5Hw/07D17fefA+Gu7ffjBH6z5ER6hbRqxsoVwrqPymug6WLhN3BV49AHvmw9FVkByf/c/lVEwdUXMtpSZXbv5QsjFY22b/nEKI55b8xRH52lu1SvD9PxcIvxXP/H8uMKCpv6VDElllWwgqtM1c3aR49SHK927AvSh1i0t7fR2un4Db59Rk6OgqsHODSq+pCVWxR+ZdJcWrdfYuML8LsUg5dZ6WfVEwpoAxWf2ZmvLg/YOy5PtwJ1xt79Y5NemLvaJuF7f/dz77olC3H9R4V70UKYQoMCSJEvmatU7Lh83LMuDHA3y37Tydgkrg6qC3dFgit9jYgY0vFPLNeL+iqKNKh3+CoyvVBGv3XHUrXEpNpnzrw4k1cPBHSHzwOCGtNVR4BWr2gBJ1sjfnKv62mkzdPge3zqqvL+2G2MsQOgr++QZq94VavdR5XkKIfE+SKJHvvRzgydy/z3Hsaiyztp7j05crWDokYSkajTriVKw6NP9MfZzN4RVw4g81udn6hXl9Fx91SYeq76jzrJ6FXWF18675X1lqsprQbZ+str/lc9g5Q02kavcFe9dna1MIYVGSRIl8T6vVMKxFObou2M0PYeF0r+dL8UIywfeFp7MC/2bqlnhXTaQOr4DLe9W5WDV6qHOmtLl4V6fOGqp1gipvwbFfYdvXcOMEbP8a/p2lXuKr2x8MzhBzBWIuQczlB9ul/97fu6E+z9A7CLxrqWttOReXuxSFsDBJokSB0NDfjTolXQk7f4tvQs8w+Y0qlg5JPE/0jlC1o7pZglYHAa9DxVfh1FrYNgmuHYKwmWoypRiffo6rB9Rt1xz1vaOXmlB511KTK7fyufsZhBDpSBIlCgSNRsP/tSxHu2938MuBy/RqWJKyHjKJVzxntFoo3wbKvQxnN8HfE9WFRwGs7dUlE5yLP7R5q5tdYYg6ps6xurwbrh2Gu1fh+Gp1A6x0NoRobLG6MEqdO2ZtC9Z26mZj999rh6Lg6KEmYY4e4OSlTu6XUS0hsuy5SKK+/fZbJk2aRGRkJFWqVGHGjBnUqlXrsfV//vlnPv30Uy5evIi/vz9fffUVrVq1Mu1XFIXRo0czb948oqOjqVevHrNnz8bf/787t3x9fQkPDzc774QJE/j4448BuHjxIn5+funaDgsLo3bt2s/6kUUuqOrtQstKHvx5NJJJG07yfdeaTz9ICEvQaNTLjKWD1ct1egd1cdEnJTJFy6ujWQBJceqo1KVdamJ1aTea+7cxkATRMVmPR6d/kFh5qj8NzmDjADb2j9kc1YdT29hn6+M/l4xGOBsKRco+/sYFIR5h8SRqxYoVDBkyhDlz5hAUFMTUqVMJCQnh1KlTFC2a/sGyO3fupGPHjkyYMIGXX36ZZcuW0a5dO/bv30+lSpUAmDhxItOnT2fx4sX4+fnx6aefEhISwvHjxzEYDKZzjRs3jp49e5reOzqmH7nYtGkTFStWNL13dZWJoM+zoSFl2Xg8ik0nrrNq32VeCyxu6ZCEeDyNRh19yiobe/UuQ9/66ntFIfnmef7ZtI4GtQOxMiaqSzAkxz/Y7quJV9I9dSmIu5Fw95q6xd9Sn4EYHa5umWVlUOeUlXsZyrZUR8vyq7ib8GtvdXTQ2g5aTYKqnWR0TjyVxZOoKVOm0LNnT7p37w7AnDlzWLt2LQsWLDCNCj1s2rRptGjRgo8++giA8ePHExoaysyZM5kzZw6KojB16lRGjhzJK6+8AsCSJUtwd3dn9erVvPXWW6ZzOTo64uHh8cT4XF1dn1pHPD9KFXGgZ4OSzPn7HB+tPISdjY6WAZ6WDkuI3KXRgEsJYu1KoBSvCdaZWGA0TUrig6QqUr1EeDdKnYifdE9NvJLj/3udlojF3VQTsFPr1E2jA5+6Dy5VtlYvRT6r1GS4cRJunAKvauoCprkhfCes7KF+dlA/728fwPm/4eUpsraXeCKLJlFJSUns27eP4cOHm8q0Wi3BwcGEhYVleExYWBhDhgwxKwsJCWH16tUAXLhwgcjISIKDg037nZ2dCQoKIiwszCyJ+vLLLxk/fjwlSpTg7bffZvDgwVhZmXdJ27ZtSUhIoEyZMgwbNoy2bTO5KKCwmGEhZbl1L5Gf911mwPIDfGet46Vy6Uc1hRCAlR4K+ahbZimKOkfr5B/qXY9RR9QFRi9uhz+HqUlPudZQtOJ/j+SxLwrWhozPZzTCrTPqJcor+9WfkYchJUHdr9GpNwU0HJa1OJ/EaIQd38Bfn4OSCm5l4PUFcHoDbPkCjvwEV/ZBh4XgKTeqiIxZNIm6efMmqampuLu7m5W7u7tz8uTJDI+JjIzMsH5kZKRpf1rZ4+oADBgwgOrVq1O4cGF27tzJ8OHDuXbtGlOmTAHAwcGByZMnU69ePbRaLatWraJdu3asXr36sYlUYmIiiYmJpvexsbEAJCcnk5yc/NT+SKv78E+RPePblic+KYW1RyLp/b99fN+5GnVK/ncpVvo5b0g/550872vXslCvLNT7EO5cRHt6HZpT69Bc2oUm7U7CRyh6R7AvgmJfVH2YtMEF7pxHc+0QmqR7Gdd39kFz/Sgc+B/KoRUYq76Dsd4QcHqGEea4m+jW9EV7/i8AjJU6kNpykjoPrE45NMWD0K3ujeb2OZTvgzE2HYuxxnug0eRdPyffR3N+C2itUDwCwMHjhbq8aOn/d2S2XYtfzrOUh0ezKleujI2NDb1792bChAno9Xrc3NzM6tSsWZOrV68yadKkxyZREyZMYOzYsenKN27ciJ1d1tYtCg0NzVJ9kV5TO4gopOXIHXhv8V76VkjF75GReennvCH9nHcs19e+4NYXvXMn3GMO4B57GNvk2+iTY9CnxKBTUtAk3oXEu2hun093dIrGhhg7H6LtShJt50e0nR/39O6g0VLI5Szlrq2i6N1j6PYvhAP/46JbE864v0yitXOWonS9d5LAi7OxTr5DisaGI96dibBqCJu2mdWz9hlBtYh5eMYcQLdxONd3reRAiR4kWzkAudTPikLhuNOUuP0PXnd2Y2W8b9qVYOVEjK0PMXY+6k9bH+L0RUGTi+ucPQcs9X2Oj8/cMzQtmkS5ubmh0+mIiooyK4+KinrsPCQPD48n1k/7GRUVhaenp1mdqlWrPjaWoKAgUlJSuHjxImXLln1snSf9QocPH26WeMXGxuLt7U3z5s1xcsrcU+WTk5MJDQ2lWbNmWGdlXoPIUPMUI+8vPcA/Z2/x/RkD/3u3BhW9nKSf84j0c955vvrafD0uo6JgTIyFuOto4m5A3A00925A/E0UlxIonlXBrQxOWiucgBIZnnMAKeE70P49Ad2lfyl1YwMlo7djrNkTY+1+6jINT6IY0e6Yivbgl2gUI4qrP8qrC6hUtDyVHntMB1L3fo9282g8Y/bhER5FYptZbDgenbP9fOci2iMr0B75Cc1Dk/sVZ2/1JoKbpzGkxGK4ewT3u0f+22/jgOJeCaXCqxgDuxeokSpLf5/TriQ9jUWTKBsbGwIDA9m8eTPt2rUDwGg0snnzZvr165fhMXXq1GHz5s0MGjTIVBYaGkqdOnUA8PPzw8PDg82bN5uSptjYWHbt2kWfPn0eG8vBgwfRarUZ3hH4cJ2HE7NH6fV69Pr0z22ztrbO8pcgO8eI9KytYV6XmnRduJvdF27TffE+lveqQ0lXw4P90s95Qfo57zy3fW3jBo5uz3aO0o2hVCM49xds+RzNlX3odk5Dt28huFdSFzXVaNWfWit1LlVaWewVdY4TQJWOaFp9jbXe4elt1u0LvnVhZXc0t8+jX9aeBnZ+GK5NRmtMUe9sTE2ClCT1Z2qy+lPv+N+SEY7uD71+8NPOVZ28fuhHiHhoDrCNI1R8RY2xRF11bbHk+xB1HK4dVOeKXTsM14+jSbqH5tK/cOlfdBe2QLtZ+fsuyQxY6vuc2TYtfjlvyJAhdO3alRo1alCrVi2mTp1KXFyc6W69Ll26UKxYMSZMmADAwIEDadSoEZMnT6Z169YsX76cvXv38t133wHqoouDBg3is88+w9/f37TEgZeXlylRCwsLY9euXbz00ks4OjoSFhbG4MGDeeeddyhUSP3XzOLFi7GxsaFatWoA/PLLLyxYsIDvv/8+j3tIPCtbGx0LutWk0/e7OHQpmk7f7+LH92pYOiwhRHZoNFC6qbq8wun16sTwqCMQsfPpx1rZQuvJ6qN4ssKrKvTeBn8MQXPkJwrHnYW4pxwTnwjxN9XYnkoDpV6CKh3VJSNsHpn+YW0LxQPVLU1qCtw8ra5t9dfncPpPmNtQnRzv/fh1FnNV8n01MbSyAZ/66s8CzuJJ1JtvvsmNGzcYNWoUkZGRVK1alfXr15smhkdERKB96NlWdevWZdmyZYwcOZJPPvkEf39/Vq9ebVojCmDYsGHExcXRq1cvoqOjqV+/PuvXrzetEaXX61m+fDljxowhMTERPz8/Bg8enO6uv/HjxxMeHo6VlRXlypVjxYoVvP7663nQKyKnOeitWNK9Fm/N+5cT12LpsnAfvXLpjmkhRB7QaNT1qfxD1AQq/hYYU9S77pRUMKY++JmivgY18cruUgl6R3j1O1Kqd2fftvUE1qqNld4OdDbqYqU6a/VOR501aK0hIea/9bjuRT60jMQ19ee9KHAtrT5XsfKb6srxWaGzAvcK6layMfzcDW6fh4UtIXgM1OmXN5f3UpPh3BY4ugpOroWku2q53gn8m6t3afo3K7BLRWgURVEsHURBFRsbi7OzMzExMVmaE7Vu3TpatWr1fA7J53M37yXy5twwzt2Iw02vsHbwS7i7FKBVl58z8n3OO9LXeSPH+llRcjbJSYiF3wfCsV/U92Va5t7lPWMqhO9QE6fjv8H9O//tcyoOxmQ1SUyj06uJXvmX1bgcijy1CUt/nzP799viI1FC5CU3Bz1L36tNhzk7uXTnPp+vO8X0t6tbOiwhxIsmp0eJDE7qpTzf+rB++EOX9xaCdw48Ais5QZ1TduJ3OParOrqWxr4oVGwPlV6D4g/aurJXrXvyD3WE7MwGddNowbu2upaY1YNRPNPPByN5Oj0ajY5idw6gORQDyoN5ZikJ6uKwj/585VuLTaqXJEq8cDycDUx9ozKvz/2XNYev0a56FE3KuT/9QCGEeJ5pNFCzBxSv8dDlvRbZu7wXe+2hZzPugmuH1BGmNAYXqNBWTZx86quXFx/mXUvdmo1TV54/8Qec/F09T8TOp85hswJqAFzMRKwvT7XY/CtJosQLqXJxZ17yVPjrmoYRvx5l4+DCOBrkEogQogDwrAK9/v7v8t7GkfDvbHWBU7vC6nIQtg9+pr03uEB0xH+JU0xE+vPaF1Uvy1V6TZ1flpnERaNRH55dtDw0+giiL8GpPyH28kN3NCY+eJ2ozrFKScSYksCtW3dwdfdCa21Qn9VoZVBHq6wMattp7y1IkijxwmrpbeRcogPht+P58s+TfN4+wNIhCSFEznj08l7sFXXLLI0W3CuCd9CDrRa4+Dz7ZTMXbwjq9dRqqcnJ7HwwJ0r7HM/xkyRKvLBsdPB5uwq8s2AvS3dF8HJlL+qUcn36gUIIkR+kXd6r0A5unVUngN+/A/dvqz/jb5uX2Rf5L2EqFlhg76jLSZJEiRdakF9h3g4qwbJdEQz/5TB/DmyIrY3O0mEJIUTOsXdVN5HjCvZDd4TIhOEty+HpbODirXi+2XTa0uEIIYTIJySJEi88R4M1n7dXF2v9fvt5Dl2KtmxAQggh8gVJooQAmpRz55WqXhgV+L9Vh0lKMVo6JCGEEM85SaKEeGB0m4oUtrfhZORdZm89Z+lwhBBCPOckiRLigcL2NoxpWxGAmVvOcCry7lOPiYlP5kr0/dwOTQghxHNI7s4T4iFtKnuy5uBVNp2IYtiqw/zSpy467X/rotxNSGb3hduEnbtF2PlbHL8Wi6JA8wruDG9VHj83eQ6fEEK8KCSJEuIhGo2Gz9pVYtf5Wxy6FM2cv89RqZizKWk6eiWGVKPyyDGw8XgUW05dp0sdXwY08cfZ7vldHE4IIUTOkCRKiEd4OBsY0bo8H/9yhEkbTqXb7+tqR51SrtQu6Uqdkq7E3E/mi3Un2HLqBvP/ucCq/ZcZ1NSfTrV9sNbJFXMhhCioJIkSIgNv1vRm/bFItp66QTEXW+qUUhOmOqVc8XKxNatb1MnAwu612Hb6Bp+tPc7pqHuM+f04S/4NZ0Sr8jQpVxSNhZ4wLoQQIvdIEiVEBjQaDfO61CA6Phk3B5tMJUENyxRhXakGrNh7iSkbT3P+Rhw9Fu+lXmlXRrSqQAUvpzyIXAghRF6Raw1CPIa1TksRR32WRpGsdFo6Bfmw9aPGvN+oFDY6LTvO3qLtzH/YH3EnF6MVQgiR1ySJEiIXOBqs+bhlOTZ/2Ii6pVxJMSrM/OuspcMSQgiRgySJEiIXeRe24/P2AWg08NfJ65yJevraU0IIIfIHSaKEyGV+bvaEVPAAYN728xaORgghRE6RJEqIPNCrUUkAVh+4yvXYBAtHI4QQIidIEiVEHqheohA1fAqRlGpk4c6Llg5HCCFEDpAkSog80quhOhq19N9w7iWmWDgaIYQQz0qSKCHySHB5d0q62RObkMKKPZcsHY4QQohnJEmUEHlEq9XwXgN1NGrBPxdITjVaOCIhhBDPQpIoIfLQq9WL4eZgw5Xo+6w7cs3S4QghhHgGkkQJkYcM1jq61PEF4Ltt51EUxbIBCSGEyDZJooTIY51r+2BrrePY1VjCzt2ydDhCCCGySZIoIfJYIXsb3qhRHIC522TxTSGEyK8kiRLCAnrUL4lWA3+fvsHJyNhMHXPjbiLbz9zAaJRLgEII8TyQJEoICyjhakfLSp4AzNt24Yl1FUVh5b7LNJm8lc7zdzPyt6Myl0oIIZ4DkkQJYSE9Hyy+uebQFSJjMn4UzPXYBN5bvJehPx/iboK6QOeyXRFM3ng6z+IUQgiRMUmihLCQqt4u1PIrTHKqwsId5qNRiqKw+sAVmn2zjc0nr2Ot0/BRSFnGv1IRgJlbzvK9PMxYCCEsysrSAQjxIuvdsCS7L9xm2a4I+jUpjaPBmut3Exjx61FCj0cBEFDMma87VKGshyMAsQkpTNpwis/WnsDFzobXA4tb8iMIIcQLS5IoISzopbJFKVXEnnM34li++xJFnfSMXnOM6PhkrHUaBjTx5/3GpbDW/Tdo3LdxKe7EJfH9Pxf4v1WHcTJY0byihwU/hRBCvJjkcp4QFqTVakwPJp644SQDlx8kOj6ZCp5OrOlXn/5N/c0SKACNRsOI1uV5PbA4qUaFfj8ekPWmhBDCAiSJEsLCXqlaDDcHPcmpClZaDYODy/Bbv3qU93R67DEajYYvXw2gWQV3klKM9FyylyOXY/IwaiGEEJJECWFhBmsd37xZhVerFeO3fvUYGJx+9CkjVjotMzpWo3bJwtxLTKHrwt2cu3EvDyIWQggBkkQJ8Vxo4F+EKW9WpaKXc5aOM1jrmNelBgHFnLkdl0Tn73dxNfp+LkUphBDiYZJECZHPORqsWdS9JiXd7Lkak0Dn+bsyvQq6EEKI7JMkSogCwNVBzw/vBeHpbODcjThaTN3OB8v2cybqrqVDE0KIAkuSKCEKiGIutvzUuw6tAtTlDtYevkbzqdsY8OMBzl6XuVJCCJHTJIkSogDxLmzHrE6B/DmwASEV3VEUWHPoKs2/+ZtByw9wXiaeCyFEjpHFNoUogMp7OjG3cw2OXY1h6qYzhB6PYvXBq6w5dJV21YrRq2FJrLRabsclcTsuiTvxSf+9jkvidnwSKakKjcsWoW1VL4o6Giz9kYQQ4rkjSZQQBVhFL2fmdanBkcsxTN10ms0nr/PL/iv8sv9Kpo7/5+xNJvx5kgb+brSvVozmFTywtdHlctRCCJE/SBIlxAsgoLgz87vV5NClaKZuOs22Mzdx0FtR2N6GQnbWFLa3UV/b21DYTv0Zl5jCbwevcvBSNFtP3WDrqRs46K1oUcmDV6sXo7afK1qtxtIfTQghLEaSKCFeIFW8XVjYvRaKoqDRPD0B6l7Pj/M37rH6wBV+OXCFy3fus3LfZVbuu4yXs4HXA4vTr4k/NlYyvVII8eKRJEqIF1BmEqg0JYs4MKR5WQYFl2Fv+B1+PXCZPw5f42pMAtP/OgvAkOZlcytUIYR4bsk/H4UQmaLVaqjlV5gJr1Zmz4hg/q9FOQCW7Y4gMSXVwtEJIUTekyRKCJFlBmsd7zXww8PJwM17Saw9fM3SIQkhRJ57LpKob7/9Fl9fXwwGA0FBQezevfuJ9X/++WfKlSuHwWAgICCAdevWme1XFIVRo0bh6emJra0twcHBnDlzxqyOr68vGo3GbPvyyy/N6hw+fJgGDRpgMBjw9vZm4sSJOfOBhSgArHVa3qldAoDFOy9aNhghhLAAiydRK1asYMiQIYwePZr9+/dTpUoVQkJCuH79eob1d+7cSceOHenRowcHDhygXbt2tGvXjqNHj5rqTJw4kenTpzNnzhx27dqFvb09ISEhJCQkmJ1r3LhxXLt2zbT179/ftC82NpbmzZvj4+PDvn37mDRpEmPGjOG7777LnY4QIh96q1YJbHRaDl2O4UDEHUuHI4QQecriSdSUKVPo2bMn3bt3p0KFCsyZMwc7OzsWLFiQYf1p06bRokULPvroI8qXL8/48eOpXr06M2fOBNRRqKlTpzJy5EheeeUVKleuzJIlS7h69SqrV682O5ejoyMeHh6mzd7e3rRv6dKlJCUlsWDBAipWrMhbb73FgAEDmDJlSq71hRD5jZuDnpcrewKwJCzcwtEIIUTesmgSlZSUxL59+wgODjaVabVagoODCQsLy/CYsLAws/oAISEhpvoXLlwgMjLSrI6zszNBQUHpzvnll1/i6upKtWrVmDRpEikpKWbtNGzYEBsbG7N2Tp06xZ078i9uIdJ0resLwB+Hr3LjbqJlgxFCiDxk0SUObt68SWpqKu7u7mbl7u7unDx5MsNjIiMjM6wfGRlp2p9W9rg6AAMGDKB69eoULlyYnTt3Mnz4cK5du2YaaYqMjMTPzy/dOdL2FSpUKF1siYmJJCb+90ckNjYWgOTkZJKTkx/TC+bS6mW2vsge6eecU8HDnirFnTl0OYb/hV2g30ulTPukn/OO9HXekH7OG5bu58y2+8KuEzVkyBDT68qVK2NjY0Pv3r2ZMGECer0+W+ecMGECY8eOTVe+ceNG7OzssnSu0NDQbMUgskb6OWdUNmg4hI5F28/iE3cK3SNj3NLPeUf6Om9IP+cNS/VzfHx8pupZNIlyc3NDp9MRFRVlVh4VFYWHh0eGx3h4eDyxftrPqKgoPD09zepUrVr1sbEEBQWRkpLCxYsXKVu27GPbebiNRw0fPtwsOYuNjcXb25vmzZvj5OT02LYflpycTGhoKM2aNcPa2jpTx4isk37OWcEpRv6cvI2b95LQ+lSnVYD634j0c96Rvs4b0s95w9L9nHYl6WksmkTZ2NgQGBjI5s2badeuHQBGo5HNmzfTr1+/DI+pU6cOmzdvZtCgQaay0NBQ6tSpA4Cfnx8eHh5s3rzZlDTFxsaya9cu+vTp89hYDh48iFarpWjRoqZ2RowYQXJysukXGBoaStmyZTO8lAeg1+szHMWytrbO8pcgO8eIrJN+zhnW1vB2kA/TN5/hf7su0a669yP7C1Y/K4rC2ev3KFnEAd1z9vzAgtbXzyvp57xhqX7ObJsWvztvyJAhzJs3j8WLF3PixAn69OlDXFwc3bt3B6BLly4MHz7cVH/gwIGsX7+eyZMnc/LkScaMGcPevXtNSZdGo2HQoEF89tlnrFmzhiNHjtClSxe8vLxMiVpYWBhTp07l0KFDnD9/nqVLlzJ48GDeeecdU4L09ttvY2NjQ48ePTh27BgrVqxg2rRpZiNNQoj/dAoqgZVWw97wOxy9EmPpcHJNqlFhyE+HaPbNNqZvPvP0A4QQBZbF50S9+eab3Lhxg1GjRhEZGUnVqlVZv369aRJ3REQEWu1/uV7dunVZtmwZI0eO5JNPPsHf35/Vq1dTqVIlU51hw4YRFxdHr169iI6Opn79+qxfvx6DwQCoI0bLly9nzJgxJCYm4ufnx+DBg80SJGdnZzZu3MgHH3xAYGAgbm5ujBo1il69euVRzwiRv7g7GWgZ4Mnvh66yJOwiE1+vYumQclyqUeHDnw6y+uBVABbtvEjvRiWxs7H4/0qFEBbwXPyX369fv8devtu6dWu6sg4dOtChQ4fHnk+j0TBu3DjGjRuX4f7q1avz77//PjWuypUrs3379qfWE0KoutX14fdDV/nt4FWGtyyPg83zdanrWTycQFlpNTjbWnMrLolf9l/hndo+lg5PCGEBFr+cJ4QoOKqXKESlYk4kphhZvudSpo7ZcfYmH/18iN0XbudydNn3aAI18+1q9GtSGoCFOy5gNCoWjlAIYQmSRAkhcoxGo6FrHV8A/vdvOCmpxsfWTUxJ5bM/jtPp+138vO8yb8wNo/+PB7gafT+Pos2cjBKoFpU8eT2wOA56K87diGPbmRvZOrc6Qf0uiSmpORy1ECIvSBIlhMhRbap4UcjOmivR9/nrVMbJxemou7wycwff/3MBgCC/wmg08PuhqzSZvJXpm8+QkGz5xOJxCRSAo8GaN2qodyEu2HExW+f/fvsFgqdso96XW5i++Qy37smK70LkJ5JECSFylMFax1u1SgDww78RZvsURWHRjgu0mfEPJyPv4mpvw/yuNVjRuw5/9K9PLd/CJCQbmRJ6mqaT/2bdkWsoimUulT0pgUrTra4vGg1sO32Ds9fvZun8UbEJfLPpNAA37yUyJfQ0db/8i+G/HMnyuYQQliFJlBAix71T2wetBv69cIerDxb+vXE3ke6L9jDm9+MkphhpXLYI6wc1pGl59U7cil7OrOhdmxkdq+HlbOBK9H36Lt3P2/N2cTIycwvf5ZTMJFAAJVztaPYg/oVZHI2auP4U8UmpVPV2YdpbVQko5kxiipEfd0cQPGUb3Rbu5p8zNy2WRAohnk6SKCFEjivmYkvzCuqq5f9Eavnr1A1aTN3G1lM30FtpGdu2Igu71aSIo/nitBqNhjZVvNj8YWMGNPVHb6Ul7PwtWk3bzqerj3LsakyuJxWZTaDSvFtffcbmqv2XiY5PylQbBy9Fs2r/ZQBGt6nAK1WLsaZfPX7qXYfmFdzRaGDrqRu8M38XLadt56e9l0h+wvwyIYRlSBIlhMgVXev6AhB2XUPv/x3gVlwS5Twc+b1/fbrW9UWjefzyB7Y2OoY0K8OmIY1oFeCBUYEf/g2n9fR/qD1hM8NWHmLdkWvE3M/5h5NO3XQ60wkUqPO5Kng6kZBs5MfdT78jUVEUxv5+DIBXqxWjWgl1gV+NRkMtv8J816UGWz5sTLe6vtjZ6DgZeZdhKw/Td+l+UuUuQCGeK5JECSFyRe2ShSlT1AGjoiZLPRv48Vu/epRxd8z0ObwL2zGrUyDLegYRXL4ottY6omIT+WnvZfou3U/18aG8MSeMb7eczZFRqvM37jH37/MAfN2hylMTKFCTn7TRqCVhF586YvTbwasciIjGzkbHsBblMqzj62bPmLYVCfu4KR+3LIeNlZbQ41F8tvZ4Fj+RECI3PReLbQohCh6NRsNnr1Rg/Mp/GdK2Ji+Vz/jB3ZlRt5QbdUu5kZiSyu4Lt9l66gZbT13n3I04dl+8ze6Lt5m04RSezgamvVWNWn6Fs9yGoiiMXnOMpFR1vtYrVb0yfWybKp58+ecJrsUksP5oJG2qZHxsfFIKX/55EoAPXiqNh7Phied1trPm/UalKF7Iln7LDrBwx0V8CtvRrZ5f5j+YECLXyEiUECLXVCvhwnvljNQv7Zoj59Nb6WjgX4RPX67A5g8bs33YS4xvV8k0SnUtJoEPlu3ndlzm5iY9bMOxSLafuYmNTsuYNhWfeLkxo7g6Bamrli/YceGx9eZsPUdkbALFC9nSo37mE6GXK3sxrEVZAMb9cZxNx6MyfawQIvdIEiWEyLe8C9vRubYP33etyb5Pg/Ev6sCNu4kMW3k4S5f24pNSGP/HCQB6NyqJr5t9lmPpVLsENjotByKi2R9xJ93+y3fimbtNvVQ4olV5DNa6LJ2/T6NSvFXTG6MC/X88wJHLBfchz0LkF5JECSEKBDsbK6a9VQ0bnZZNJ6JYtjvi6Qc98O2Ws1yJvk8xF1v6Ni6drfaLOhpMl/EyWu5gwp8nSUwxEuRXmBaVsn5pU6PRML5dJRr4u3E/OZV3F+/hynO2ursQLxpJooQQBUYFLyfTZa/xfxzn7PV7Tz3m/I17zNumXoIb1aYCtjZZGyF6WPd6vgCsO3KNazH/JTi7L9xm7eFraDVqG1m5VPgwa52WbztVp6y7IzfuJtJj0R7uJuT8HYpCiMyRJEoIUaC8W8+PBv5uJCQbGbj8AEkpj79bTlEUxvx+3DSZvHkF92dqu1IxZ4L8CpNqVPghLBxQ151KW9LgrVolqOjl/ExtOBmsWdBdXWPrZORd+i7dL2tICWEhkkQJIQoUrVbD1x2qUMjOmmNXY5m88dRj6244FsW20zeyNZn8cdKWO1i2O4L7Sams3HeJY1djcTRY8WGzMs98flAXM13QtSa21jq2n7nJqN+OysrmQliAJFFCiALH3cnAV69VBmDutvPsOHszXZ37SamM/0Nddym7k8kzElzeHe/CtkTHJ/PDvxeZtEFN4gY29cfVQf+UozMvoLgzMzpWQ6OBH3df4rvtF3Ps3EKIzJEkSghRIDWv6MHbQeqDkD/86RB3Hln2ICcmk2dEp9XQtY4voE4mv3kviZJF7OnyoCwnBVdwZ9TLFQD4OvQMC09rmbX1PKHHo4i4FY8xF1Y4///27j0uqjr/H/jrDMwMd+Q+XBWRi6JigiKamUqilrcstR+7S+33l6thX83a0lZTu+nmZqZrmpVddkuMVLaLqITJpuIFBERRFOWm3FEYBLnO5/sHOe0smDACA/h6Ph7zkDnnc8685/2Ax7w85zPn5JRVY1P8pTbNNyPq7XixTSLqtVY8OhDHr5TjSmk1Xt2bjg/Ch0OSJGSXVWP7L5cbuNfJ5K2ZM8Id78VdRHV9EwBg5aODoDDunP+zPjPGE3nXa/Dp0RyklsuQGp+lXWemMIK3kyV8nSzgq7KCn8oSgX1t2n15BQAorLyFTfFZiE7KR6NG4F+p13BgyUMwNuL/xen+xRBFRL2WmcIYm+Y9gFkfHEXs2SJ8nZSPOUHu2iuTj/O598nkrbEykWPOCHd8ejQH43wcMN7PscNf4z+99tggjPexQ3T8Schs3HCppBpZJTdRU9+EtPwKpOVXaMdamhjjsaHOmD3cDYF9be46D6z8Zh0+OHwZ/zieq52kbyyTcLm0GntSrmFOkHtnvjWibo0hioh6tcGu1nhxki/WxV7A6m8zcKOm4dfJ5NM7ZjJ5a14O84OvkyWmDLn7/ffulSRJCOlvhxsXBKZOHQK5XI7GJg1yymtwsbgKF4qqcLGoCmlXK1BYWYudJ/Ox82Q++tmZ4fHhbpj1gCvcbc109ll5qwEf/3wFnxzJRs0vR9RG9rPFS2G+SMuvwFv7zmNj3EVMD3DR68gWUW/AEEVEvd78sf2RkFmKxCvl2nvX/Wlcf3h20GTy1pgqjDBvpEen7f9ujI1kGOBogQGOFpj6S5DTaASOZ5djd/I1xJ4tRE55DTbEXcSGuIsI9rTF7EA3POzrgOikq/gw4TLUtY0AgCGu1ngpzBcPedtDkiQMdbPGJ0eyUVBZiy9P5LXrFjZEvQlDFBH1ejKZhA1zAzB548+ovNXQ4ZPJewqZTNLezPn1Gf7Yf7YIu09fReKVcpzIvo4T2dd1xns7WuDFST4I81fpHLEzkRthSag3lu1Jx5afsjB3hDsslPw4ofsPZwQS0X3B2doU788bBj+VJdY/MbTDJ5P3NOZKY8wOdMNXz47CkVcm4KVJPtojcx62ZnhvbgD2L3kIkwc7t3rK84lAN/S3N8f16np88vOdb7pM1Jvxvw5EdN942NcRD/t27iTvnsi1jykWTfBG5PgBKFbXwc5CAfldvnVnbCTD0kk+WPRVCj76+Qp+H9IXtuaKLqqYqHvgkSgiIgLQPEFdZW1y1wB129TBzvB3scLNukZ88FPW3Tcg6mUYooiISC8ymYSXJ/sBAL44nouCilt32YKod2GIIiIivT3kbY9gT1vUN2qwKf6Socsh6lIMUUREpDdJ+vVo1NdJ+bhcytvB0P2DIYqIiO5JYF8bhA50hEYAGw5eNHQ5RF2GIYqIiO7ZS2G+kCTgh/RCpF+tNHQ5RF2CIYqIiO6Zn8oKM4e5AgDeOXDBwNUQdQ2GKCIi6hAvhPrAWCbh50tlSLxcbuhyiDodQxQREXUIDzszPPXL/QLfOXABQggDV0TUuRiiiIiowzw/YQBM5UZIyavAjqM5UNc26L2v6rpGnLhSfk/7IOpMvO0LERF1GEcrEzwzph8+OHwZb3yfgbf3nUeghw3G+TrgIW8H+LtYQSZreS8+AKhtaMLpvBtIvFyOY5fLkZZfgUaNQH97c0TNHwVHK5MufjdEv40hioiIOtT/TvSGAHDgXBGulFbjZM51nMy5jvUHMmFnrsBYb3uM83XAaC97XL1Rg2NZzaEpOe8G6hs1OvuSG0m4UlaNeR8dR9SzDFLUvTBEERFRhzKRG+GVyX54ZbIf8q/XIOFiKRIuluJYVhnKq+sRk1qAmNSCVrd1tFRitJcdRnvZI8TLDkIAT310HFdKGaSo+2GIIiKiTuNua4bfjeqL343qi/pGDU7n3WgOVZmlyChUw9ZcgVH9bRHiZY/RXnbob28OSdI93bfz2VEMUtQtMUQREVGXUBjLMKq/HUb1t8Mrk/1QXdcIU7nRHedI3eZhZ8YgRd0Sv51HREQGYa40vmuAuu12kHLtY6oNUiXq2k6ukOi3MUQREVGPwCBF3Q1DFBER9RgMUtSdMEQREVGP0lqQSsuvQFUnXpRTCIED54oQ8WkSDlyVeDX2buDN7zPwdVI+btU3GawGTiwnIqIe578nm8/YchQAYGMmh4etGdxtzeDxHw93WzO42Zi2+Obf3QghcDizFBviLiL9WuUvS43gHpeFZVMHtnt/1DHyr9fg4yPZkCRgtJcd3BRmBqmDIYqIiHokDzszRM0fhVf3piOjQI3y6nrcqGnAjZpKpF2tbDFeZWWCSf5OCPNXYaSnLeRGdz4ZI4TAkawybIi7iJS8CgCAmcII47ztEXuuGB/+nA0ThTFeeMSns94e/YbopHwAwIMD7OFmY5gABTBEERFRD+Zua4Z//E8wAOBmXSPyr9cg73qN9t/bj6vXb6FIXYsvEnPxRWIurE3lmOjniEn+KozzcYCpwki7z+NXyrHh4EWczLkOADCRyxAR0g/zH+oPK6UMf/44FjG5Rng//hIUxjJEjh/Qrpo1GoGDGcVwszHFYFfrjmvGfaJJI/BN8lUAwJNB7gathSGKiIh6BQulMQY6W2Ggs1WLdbUNTTh2uQwHzxUjLqMY5dX12JNyDXtSrsFELsND3g4Y6+OA/WcLcTSrHEDzda3Cgz2w8GEvOFo2X5OqoaEB410EvH29sf7gJaw/kAm5kYT5D3m1qcaskptYtvsMknJvAAAeGeSEFyf5wE/VsmZq3ZGsMhRU1sLaVI5Jg5wMWgtDFBER9XomciNM8HPCBD8nvDVLIDn3Bg6cK8KBc0W4euMWDmYU42BGMYDm+/XNG+GB58Z7wdnatNX9zR/rCY2Q8G7cRby97wLkRjI8M8bzjq/f0KTBhwmXsSk+C/VNGpjKjVDX2IS4jGL8eL4Y04a64IVHfOBpb94p7783+fpU86m8WQ+4wkRudJfRnYshioiI7itGMgkjPW0x0tMWKx4diIxCNQ6cK0bi5TJ4O1niuYe92jTP5vmJ3qhv0mDzoSys+S4DciMZfjeqb4txZ65W4OVvzuBCURUA4GFfB7w1awhu1TfivbhL+CG9EN+mFeCH9EI8MdwN/xvqDdc+rYe3+9316noczCgCAMwx8Kk8gCGKiIjuY5Ikwd/FGv4u1oAek8SXPuKD+iYNPky4ghUxZ6EwkmHOiOYP91v1Tdj440V89PMVaETzNwdXTfPHjGEu2m/1bQkfjoXXKrEh7iIOXSjBrqR87E25hv8X3Hwk7PZpRGoWk3INDU0Cg12tMMjF8KdAu0WI2rJlC9avX4+ioiIEBARg8+bNGDly5B3HR0dHY+XKlcjJyYG3tzf++te/YurUqdr1QgisWrUKH330ESoqKjBmzBhs3boV3t7eLfZVV1eH4OBgpKWlISUlBcOGDQMA5OTkwNOz5aHZxMREjBo16t7fNBER9XiSJGHZZD80NArsOJqNV/acgbGRBJW1CZbvSUdueQ0AYHqAC1ZNGwQ7C2WLfQx2tcaOp0cgOfc6/nbgIhKvlOOzYznYdSofw/v2gbFMBmOZBCOZBLmRDEYyCcZGEoxlEoyNZBjjZY+pQ1QGu9xCdV0jTuZcR+LlcpzIvo5BzpZ4e9aQDq9HCIGvf/lW3txucBQK6AYhateuXVi6dCm2bduG4OBgbNy4EWFhYcjMzISjo2OL8ceOHcNTTz2FtWvX4rHHHsNXX32FmTNn4vTp0xg8eDAA4J133sGmTZvw+eefw9PTEytXrkRYWBgyMjJgYqKb6l9++WW4uLggLS2t1fp+/PFH+Pv7a5/b2dl14LsnIqKeTpIkrHxsIBqaNPjH8Vy8GJ2G29fiVFmZ4K1ZgzFx4N0nQAf2tcXO+aNwLKsM6w9mIiWvQjvJ/bd8dSIPk/1VeGvW4FZDWkera2xCSl4FjmWV4djlcqTmV6BR8+vFR9PyK/CAu432iFxHOXO1EheKqqA0lmH6MNcO3be+DB6iNmzYgGeffRbPPPMMAGDbtm344YcfsGPHDixbtqzF+Pfffx+TJ0/Gn//8ZwDAG2+8gbi4OPz973/Htm3bIITAxo0bsWLFCsyYMQMA8MUXX8DJyQkxMTGYN2+edl+xsbE4ePAgdu/ejdjY2Fbrs7Ozg0ql6ui3TUREvYgkSVgz3R+NGg12nmw+WhIe7IFXpvjBykTern2NHmCPPV52OJVzA4WVt9DYJNCkEWjQaNCkEWhsEmjUaNCoEShR1+HLE7nYf64ISbk3sO7xIQjthG+s1TY0IepkHuIvlOBUznXUNmh01rvZmGKMlz0kCYg6lY83f8jAw34OHXo6ctcvR6EmD1bB2rR9Pe0sBg1R9fX1SE5OxvLly7XLZDIZQkNDkZiY2Oo2iYmJWLp0qc6ysLAwxMTEAACys7NRVFSE0NBQ7Xpra2sEBwcjMTFRG6KKi4vx7LPPIiYmBmZmd55AOH36dNTW1sLHxwcvv/wypk+fru/bJSKiXkwmk/DWzCEY6WmLvnbmGO5ho/e+JKl58ntbPBnkhhd2peJi8U38/y+SMDfIHSunDYKF8t4/4oUQ+O5MIf4aewHXKm5pl9tbKDHayw6jvewwZoA93G2bP0cbmzQ4V6BG+rVKrP72HD4ID7znGoDm+WXfpRYA6D6n8gADh6iysjI0NTXByUk3NTs5OeHChQutblNUVNTq+KKiIu3628vuNEYIgaeffhoLFixAUFAQcnJyWryOhYUF3n33XYwZMwYymQy7d+/GzJkzERMTc8cgVVdXh7q6Ou1ztVoNoPm6Ig0Nbbun0+1xbR1P+mGfuwb73HXY667Rlj4/NtjprmM6ko+DGfb8KRjvxWdhx7Fc7ErKx9GsUvx19mCM7Ne2INaa5NwbeHt/Js5cbf4sc7JS4n/G9MODXnYY4GiuM+fpP9/rmzMG4vFtJ7AvvQj70q7hkUEtp+bczX/3+fu0AlTVNcLNxhSB7lad3tu27t/gp/MMYfPmzaiqqtI5Avbf7O3tdY54jRgxAgUFBVi/fv0dQ9TatWuxZs2aFssPHjz4m0e7WhMXF9eu8aQf9rlrsM9dh73uGt2xz0MBLBoIfHnZCFcravG7T05hvLPAVA8N5He+w00LZbXAt7kypF1v3kghEwh11WC8czUUFedwKRm4dJd9jFfJ8GOBDMu+SUHlsCaY6Zk2bvf5w3NGACQMtbiJ/ftbn37TkWpqato0zqAhyt7eHkZGRiguLtZZXlxcfMd5SCqV6jfH3/63uLgYzs7OOmNuf/Pu0KFDSExMhFKpOwEvKCgI4eHh+Pzzz1t97eDg4N/8w1m+fLlO8FKr1XB3d8ekSZNgZdW2r2I2NDQgLi4OjzzyCOTy7nHOtzdin7sG+9x12Ouu0RP6/ExtI96OzcQ3p6/hUKGEq01WiHy4P5ytTeBgqYSDhQLKVi5SWVHTgC2HL+PLM/loaBKQScCTgW5YPMELDpbtm7A+oaEJ07YkIqe8BqmiH96cOqhd2/9nnwvUDchKPAJJApbNHQ9n686/7MPtM0l3Y9AQpVAoEBgYiPj4eMycORMAoNFoEB8fj0WLFrW6TUhICOLj47FkyRLtsri4OISEhAAAPD09oVKpEB8frw1NarUaJ06cwMKFCwEAmzZtwptvvqndvqCgAGFhYdi1axeCg4PvWG9qaqpOMPtvSqWyRTADALlc3u4/Nn22ofZjn7sG+9x12Ouu0Z37bCuX429zhmGSvwrL96TjYslNLP76jM4YKxNjOFgq4WhpAkcrJSxNjPFdWiEqbzWfxhrn44BXpw6Er8pSrxrkcjnWzR6KeduPY1fSVcwa7oZR/dv/7Xa5XI49qXkAgIe8HeBhr189+rxuWxj8dN7SpUsRERGBoKAgjBw5Ehs3bkR1dbX223p/+MMf4OrqirVr1wIAFi9ejHHjxuHdd9/Fo48+iqioKCQlJWH79u0AmifjLVmyBG+++Sa8vb21lzhwcXHRBjUPDw+dGiwsLAAAXl5ecHNzAwB8/vnnUCgUeOCBBwAAe/bswY4dO/Dxxx93ek+IiIju1SR/FYb3tcGGuIvIKFCjtKoOpVV1qG/SQF3bCHVtIy6XVuts46eyxKtTB+IhH4d7fv1R/e3w1EgP7DyZh+V70hG7eGy7b9PS2KTB7tPNNxue28GXTOgIBg9Rc+fORWlpKV577TUUFRVh2LBh2L9/v3ZieF5eHmSyX0/mjh49Gl999RVWrFiBV199Fd7e3oiJidFeIwpovvZTdXU15s+fj4qKCjz44IPYv39/i2tE3c0bb7yB3NxcGBsbw8/PD7t27cITTzzRMW+ciIiok9lbKPH2rCHa50IIqG81oqSqFqVVdSj5JViV3ayDr8oSM4a5wkjWcRfJXD7VD/Hni5FdVo334y/hlcl+7dr+56xyFKvrYGMmx8SB7Z+g3tkMHqIAYNGiRXc8fXf48OEWy5588kk8+eSTd9yfJEl4/fXX8frrr7fp9fv16wchhM6yiIgIREREtGl7IiKinkCSJFibyWFtJoe3U+efGrMykeONmYPxp38kY/u/r+Cxoc7Nt9hpo+jkawCAWQ+4QWls2JsNt6Yd8/WJiIiI2ifMX4WpQ1Ro0gi8svsMGps0d98IgLoe+CmzFED3PJUHMEQRERFRJ1s93R9WJsY4e02NT45kt2mbpDIJjRqBAPc+ek9w72wMUURERNSpHC1NsOLR5sscbIi7iJyy6t8cL4TA8ZLmiDInyK3T69MXQxQRERF1uieD3DDayw51jRpEfHoSr/3rLL4+lY+z1ypR36h7ii81vxLFtySYyGWYFuBioIrvrltMLCciIqLeTZIkrH18CB7bfAS55TX4IjFXu05uJMHb0RL+LlYY7GqNI5ea50JN8Xdq9w2cuxJDFBEREXWJvnbmiF86Dkcvl+HcNTXOFahxrqAS6tpGZBSqkVGoRnTyVe342cNdDVjt3TFEERERUZdxtDLBrAfcMKv5WtYQQuDqjVvaQHWuQI2Mgko4GN3CyH42hi32LhiiiIiIyGAkSYK7rRncbc0weXDz/W8bGhqwb98+SFLHXfizM3BiOREREZEeGKKIiIiI9MAQRURERKQHhigiIiIiPTBEEREREemBIYqIiIhIDwxRRERERHpgiCIiIiLSA0MUERERkR4YooiIiIj0wBBFREREpAeGKCIiIiI9MEQRERER6YEhioiIiEgPxoYuoDcTQgAA1Gp1m7dpaGhATU0N1Go15HJ5Z5V232Ofuwb73HXY667BPncNQ/f59uf27c/xO2GI6kRVVVUAAHd3dwNXQkRERO1VVVUFa2vrO66XxN1iFulNo9GgoKAAlpaWkCSpTduo1Wq4u7sjPz8fVlZWnVzh/Yt97hrsc9dhr7sG+9w1DN1nIQSqqqrg4uICmezOM594JKoTyWQyuLm56bWtlZUV/0C7APvcNdjnrsNedw32uWsYss+/dQTqNk4sJyIiItIDQxQRERGRHhiiuhmlUolVq1ZBqVQaupRejX3uGuxz12Gvuwb73DV6Sp85sZyIiIhIDzwSRURERKQHhigiIiIiPTBEEREREemBIaqb2bJlC/r16wcTExMEBwfj5MmThi6pR/v3v/+NadOmwcXFBZIkISYmRme9EAKvvfYanJ2dYWpqitDQUFy6dMkwxfZga9euxYgRI2BpaQlHR0fMnDkTmZmZOmNqa2sRGRkJOzs7WFhYYPbs2SguLjZQxT3T1q1bMXToUO21c0JCQhAbG6tdzx53jnXr1kGSJCxZskS7jL2+d6tXr4YkSToPPz8/7fqe0GOGqG5k165dWLp0KVatWoXTp08jICAAYWFhKCkpMXRpPVZ1dTUCAgKwZcuWVte/88472LRpE7Zt24YTJ07A3NwcYWFhqK2t7eJKe7aEhARERkbi+PHjiIuLQ0NDAyZNmoTq6mrtmBdeeAHfffcdoqOjkZCQgIKCAjz++OMGrLrncXNzw7p165CcnIykpCRMmDABM2bMwLlz5wCwx53h1KlT+PDDDzF06FCd5ex1x/D390dhYaH2ceTIEe26HtFjQd3GyJEjRWRkpPZ5U1OTcHFxEWvXrjVgVb0HALF3717tc41GI1QqlVi/fr12WUVFhVAqlWLnzp0GqLD3KCkpEQBEQkKCEKK5r3K5XERHR2vHnD9/XgAQiYmJhiqzV7CxsREff/wxe9wJqqqqhLe3t4iLixPjxo0TixcvFkLw97mjrFq1SgQEBLS6rqf0mEeiuon6+nokJycjNDRUu0wmkyE0NBSJiYkGrKz3ys7ORlFRkU7Pra2tERwczJ7fo8rKSgCAra0tACA5ORkNDQ06vfbz84OHhwd7raempiZERUWhuroaISEh7HEniIyMxKOPPqrTU4C/zx3p0qVLcHFxQf/+/REeHo68vDwAPafHvHdeN1FWVoampiY4OTnpLHdycsKFCxcMVFXvVlRUBACt9vz2Omo/jUaDJUuWYMyYMRg8eDCA5l4rFAr06dNHZyx73X7p6ekICQlBbW0tLCwssHfvXgwaNAipqanscQeKiorC6dOncerUqRbr+PvcMYKDg/HZZ5/B19cXhYWFWLNmDcaOHYuzZ8/2mB4zRBFRh4qMjMTZs2d15jZQx/H19UVqaioqKyvxzTffICIiAgkJCYYuq1fJz8/H4sWLERcXBxMTE0OX02tNmTJF+/PQoUMRHByMvn374uuvv4apqakBK2s7ns7rJuzt7WFkZNTimwfFxcVQqVQGqqp3u91X9rzjLFq0CN9//z1++uknuLm5aZerVCrU19ejoqJCZzx73X4KhQIDBgxAYGAg1q5di4CAALz//vvscQdKTk5GSUkJhg8fDmNjYxgbGyMhIQGbNm2CsbExnJyc2OtO0KdPH/j4+CArK6vH/D4zRHUTCoUCgYGBiI+P1y7TaDSIj49HSEiIASvrvTw9PaFSqXR6rlarceLECfa8nYQQWLRoEfbu3YtDhw7B09NTZ31gYCDkcrlOrzMzM5GXl8de3yONRoO6ujr2uANNnDgR6enpSE1N1T6CgoIQHh6u/Zm97ng3b97E5cuX4ezs3HN+nw09s51+FRUVJZRKpfjss89ERkaGmD9/vujTp48oKioydGk9VlVVlUhJSREpKSkCgNiwYYNISUkRubm5Qggh1q1bJ/r06SP+9a9/iTNnzogZM2YIT09PcevWLQNX3rMsXLhQWFtbi8OHD4vCwkLto6amRjtmwYIFwsPDQxw6dEgkJSWJkJAQERISYsCqe55ly5aJhIQEkZ2dLc6cOSOWLVsmJEkSBw8eFEKwx53pP7+dJwR73RFefPFFcfjwYZGdnS2OHj0qQkNDhb29vSgpKRFC9IweM0R1M5s3bxYeHh5CoVCIkSNHiuPHjxu6pB7tp59+EgBaPCIiIoQQzZc5WLlypXBychJKpVJMnDhRZGZmGrboHqi1HgMQn376qXbMrVu3xHPPPSdsbGyEmZmZmDVrligsLDRc0T3QH//4R9G3b1+hUCiEg4ODmDhxojZACcEed6b/DlHs9b2bO3eucHZ2FgqFQri6uoq5c+eKrKws7fqe0GNJCCEMcwyMiIiIqOfinCgiIiIiPTBEEREREemBIYqIiIhIDwxRRERERHpgiCIiIiLSA0MUERERkR4YooiIiIj0wBBFREREpAeGKCKiTiRJEmJiYgxdBhF1AoYoIuq1nn76aUiS1OIxefJkQ5dGRL2AsaELICLqTJMnT8ann36qs0ypVBqoGiLqTXgkioh6NaVSCZVKpfOwsbEB0HyqbevWrZgyZQpMTU3Rv39/fPPNNzrbp6enY8KECTA1NYWdnR3mz5+Pmzdv6ozZsWMH/P39oVQq4ezsjEWLFumsLysrw6xZs2BmZgZvb298++232nU3btxAeHg4HBwcYGpqCm9v7xahj4i6J4YoIrqvrVy5ErNnz0ZaWhrCw8Mxb948nD9/HgBQXV2NsLAw2NjY4NSpU4iOjsaPP/6oE5K2bt2KyMhIzJ8/H+np6fj2228xYMAAnddYs2YN5syZgzNnzmDq1KkIDw/H9evXta+fkZGB2NhYnD9/Hlu3boW9vX3XNYCI9CeIiHqpiIgIYWRkJMzNzXUeb731lhBCCABiwYIFOtsEBweLhQsXCiGE2L59u7CxsRE3b97Urv/hhx+ETCYTRUVFQgghXFxcxF/+8pc71gBArFixQvv85s2bAoCIjY0VQggxbdo08cwzz3TMGyaiLsU5UUTUq40fPx5bt27VWWZra6v9OSQkRGddSEgIUlNTAQDnz59HQEAAzM3NtevHjBkDjUaDzMxMSJKEgoICTJw48TdrGDp0qPZnc3NzWFlZoaSkBACwcOFCzJ49G6dPn8akSZMwc+ZMjB49Wq/3SkRdiyGKiHo1c3PzFqfXOoqpqWmbxsnlcp3nkiRBo9EAAKZMmYLc3Fzs27cPcXFxmDhxIiIjI/G3v/2tw+sloo7FOVFEdF87fvx4i+cDBw4EAAwcOBBpaWmorq7Wrj969ChkMhl8fX1haWmJfv36IT4+/p5qcHBwQEREBP75z39i48aN2L59+z3tj4i6Bo9EEVGvVldXh6KiIp1lxsbG2snb0dHRCAoKwoMPPogvv/wSJ0+exCeffAIACA8Px6pVqxAREYHVq1ejtLQUzz//PH7/+9/DyckJALB69WosWLAAjo6OmDJlCqqqqnD06FE8//zzbarvtddeQ2BgIPz9/VFXV4fvv/9eG+KIqHtjiCKiXm3//v1wdnbWWebr64sLFy4AaP7mXFRUFJ577jk4Oztj586dGDRoEADAzMwMBw4cwOLFizFixAiYmZlh9uzZ2LBhg3ZfERERqK2txXvvvYeXXnoJ9vb2eOKJJ9pcn0KhwPLly5GTkwNTU1OMHTsWUVFRHfDOiaizSUIIYegiiIgMQZIk7N27FzNnzjR0KUTUA3FOFBEREZEeGKKIiIiI9MA5UUR03+JsBiK6FzwSRURERKQHhigiIiIiPTBEEREREemBIYqIiIhIDwxRRERERHpgiCIiIiLSA0MUERERkR4YooiIiIj0wBBFREREpIf/A4iYx0MctUbjAAAAAElFTkSuQmCC","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["train_loss_file = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/logs/waveunet/Train_Val_Test_FT_ALL/train_loss.json\"\n","val_loss_file = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/logs/waveunet/Train_Val_Test_FT_ALL/val_loss.json\"\n","\n","plot_loss(train_loss_file, val_loss_file, 'Downsampling and Upsampling')"]},{"cell_type":"markdown","source":["Here, we observe fine-tuning applied to all weights across every layer within the specified Wave-U-Net model."],"metadata":{"id":"J8tZ5VV4qUw-"}},{"cell_type":"markdown","metadata":{"id":"FucSFwZTsfeR"},"source":["## Predict Results for Fine Tuning\n","\n","Run prediction code on entire test folder:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vwTqDlMspMv","outputId":"a07b43c3-f6a5-470e-ffbd-cbc09297ad30"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/predict.py\", line 4, in <module>\n","    import data.utils\n","  File \"/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/data/utils.py\", line 4, in <module>\n","    import torch\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 237, in <module>\n","    from torch._C import *  # noqa: F403\n","KeyboardInterrupt\n","^C\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/predict.py\", line 4, in <module>\n","  File \"/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/data/utils.py\", line 4, in <module>\n","    import torch\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1477, in <module>\n","    from .functional import *  # noqa: F403\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/functional.py\", line 9, in <module>\n","    import torch.nn.functional as F\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n","    from .modules import *  # noqa: F403\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/__init__.py\", line 2, in <module>\n","    from .linear import Identity, Linear, Bilinear, LazyLinear\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 7, in <module>\n","    from .. import functional as F\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 25, in <module>\n","    from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_jit_internal.py\", line 44, in <module>\n","    import torch.distributed.rpc\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/rpc/__init__.py\", line 74, in <module>\n","    from .server_process_global_profiler import (\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/rpc/server_process_global_profiler.py\", line 6, in <module>\n","    from torch.autograd.profiler_legacy import profile\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 19, in <module>\n","    from .function import Function, NestedIOFunction\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 12, in <module>\n","    from torch._functorch.autograd_function import custom_function_call\n","  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n","KeyboardInterrupt\n","^C\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/predict.py\", line 7, in <module>\n","    from test import predict_song\n","  File \"/content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/test.py\", line 1, in <module>\n","    import museval\n","  File \"/usr/local/lib/python3.10/dist-packages/museval/__init__.py\", line 11, in <module>\n","    from . import metrics\n","  File \"/usr/local/lib/python3.10/dist-packages/museval/metrics.py\", line 51, in <module>\n","    from scipy.signal import fftconvolve\n","  File \"/usr/local/lib/python3.10/dist-packages/scipy/signal/__init__.py\", line 324, in <module>\n","    from ._bsplines import *\n","  File \"/usr/local/lib/python3.10/dist-packages/scipy/signal/_bsplines.py\", line 12, in <module>\n","    from scipy.interpolate import BSpline\n","  File \"/usr/local/lib/python3.10/dist-packages/scipy/interpolate/__init__.py\", line 167, in <module>\n","    from ._interpolate import *\n","  File \"/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_interpolate.py\", line 12, in <module>\n","    from . import _fitpack_py\n","  File \"/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack_py.py\", line 10, in <module>\n","    from ._bsplines import BSpline\n","  File \"/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_bsplines.py\", line 9, in <module>\n","    from scipy.optimize import minimize_scalar\n","  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/__init__.py\", line 410, in <module>\n","    from ._minimize import *\n","  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\", line 27, in <module>\n","    from ._trustregion_constr import _minimize_trustregion_constr\n","  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_trustregion_constr/__init__.py\", line 4, in <module>\n","    from .minimize_trustregion_constr import _minimize_trustregion_constr\n","  File \"/usr/local/lib/python3.10/dist-packages/scipy/optimize/_trustregion_constr/minimize_trustregion_constr.py\", line 10, in <module>\n","    from .equality_constrained_sqp import equality_constrained_sqp\n","  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 945, in _find_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1439, in find_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1411, in _get_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1578, in find_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1532, in _get_spec\n","  File \"<frozen importlib._bootstrap_external>\", line 1040, in __init__\n","KeyboardInterrupt\n","^C\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","move model to gpu\n","Loading model from checkpoint /content/drive/MyDrive/Semester 1/AudioProcAndAI/Final Project/Practical Part/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\n","Step 137012\n"]}],"source":["# Define the model path\n","model_path = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/checkpoint_137012\"\n","\n","# Define the test directory path\n","test_dir = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/Dataset/test\"\n","\n","# Define the output directory path\n","output_dir = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/Predict\"\n","\n","# Check if the output directory is empty\n","if not os.listdir(output_dir):\n","    # Iterate over the test files in the test directory\n","    for test_file in os.listdir(test_dir):\n","\n","        test_file_name = 'mixture.wav'\n","\n","        # Extract the test file name\n","        test_file_path = os.path.join(test_dir, test_file, test_file_name)\n","\n","        # Create directory for the current test file's predictions\n","        test_output_dir = os.path.join(output_dir, test_file + \"_predict\")\n","        os.makedirs(test_output_dir, exist_ok=True)\n","\n","        # Construct the command\n","        command = f\"python \\\"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/predict.py\\\" --load_model \\\"{model_path}\\\" --input \\\"{test_file_path}\\\" --output \\\"{test_output_dir}\\\" --sr 16000 --cuda\"\n","\n","        # Execute the command\n","        !{command}\n","\n","else:\n","    print(\"Predict directory is not empty. Please clear the directory before running predictions.\")\n"]},{"cell_type":"markdown","metadata":{"id":"dgkHrSLhDH6Y"},"source":["## Evaluate Model Using SNR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DkdcrGae-07K"},"outputs":[],"source":["def calc_SNR(speech, noise):\n","    \"\"\"\n","    Calculate Signal-to-Noise Ratio (SNR) between speech and noise signals.\n","\n","    Args:\n","    - speech (numpy.ndarray): Array containing speech signal.\n","    - noise (numpy.ndarray): Array containing noise signal.\n","\n","    Returns:\n","    - snr (float): Signal-to-Noise Ratio (SNR) in decibels (dB).\n","    \"\"\"\n","\n","    # Calculate the power of the noise signal\n","    power_noise = np.mean(noise ** 2)\n","\n","    # Print the power of the noise signal (optional)\n","    # print(power_noise)\n","\n","    # Define the threshold for detecting non-silent segments in the speech audio\n","    silence_threshold = 1.1 * np.median(np.abs(noise))  # Adjust this threshold as needed\n","\n","    # Find indices of non-silent segments in the speech audio\n","    non_silent_indices = np.where(speech ** 2 > silence_threshold)\n","\n","    # Calculate the power of non-silent speech segments\n","    power_non_silent_speech = np.mean(speech[non_silent_indices] ** 2)\n","\n","    # Print the power of non-silent speech segments (optional)\n","    # print(power_non_silent_speech)\n","\n","    # Calculate SNR in decibels (dB)\n","    snr = 10 * np.log10(power_non_silent_speech / power_noise)\n","\n","    # Print SNR (optional)\n","    # print(snr)\n","\n","    return snr"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1363,"status":"ok","timestamp":1710692159696,"user":{"displayName":"Yuval Sh","userId":"08473625653578168122"},"user_tz":-120},"id":"apiO6RpWDQHE","outputId":"73c99d74-ab00-4aaa-e74f-267cbe8a8217"},"outputs":[{"name":"stdout","output_type":"stream","text":["Progress: 1/46 (2.17%)\n","Progress: 2/46 (4.35%)\n","Progress: 3/46 (6.52%)\n","Progress: 4/46 (8.70%)\n","Progress: 5/46 (10.87%)\n","Progress: 6/46 (13.04%)\n","Progress: 7/46 (15.22%)\n","Progress: 8/46 (17.39%)\n","Progress: 9/46 (19.57%)\n","Progress: 10/46 (21.74%)\n","Progress: 11/46 (23.91%)\n","Progress: 12/46 (26.09%)\n","Progress: 13/46 (28.26%)\n","Progress: 14/46 (30.43%)\n","Progress: 15/46 (32.61%)\n","Progress: 16/46 (34.78%)\n","Progress: 17/46 (36.96%)\n","Progress: 18/46 (39.13%)\n","Progress: 19/46 (41.30%)\n","Progress: 20/46 (43.48%)\n","Progress: 21/46 (45.65%)\n","Progress: 22/46 (47.83%)\n","Progress: 23/46 (50.00%)\n","Progress: 24/46 (52.17%)\n","Progress: 25/46 (54.35%)\n","Progress: 26/46 (56.52%)\n","Progress: 27/46 (58.70%)\n","Progress: 28/46 (60.87%)\n","Progress: 29/46 (63.04%)\n","Progress: 30/46 (65.22%)\n","Progress: 31/46 (67.39%)\n","Progress: 32/46 (69.57%)\n","Progress: 33/46 (71.74%)\n","Progress: 34/46 (73.91%)\n","Progress: 35/46 (76.09%)\n","Progress: 36/46 (78.26%)\n","Progress: 37/46 (80.43%)\n","Progress: 38/46 (82.61%)\n","Progress: 39/46 (84.78%)\n","Progress: 40/46 (86.96%)\n","Progress: 41/46 (89.13%)\n","Progress: 42/46 (91.30%)\n","Progress: 43/46 (93.48%)\n","Progress: 44/46 (95.65%)\n","Progress: 45/46 (97.83%)\n","Progress: 46/46 (100.00%)\n","\n","\n","THE MEAN SNR IS:  23.433081481767736\n","THE MAX SNR IS:  26.873691082000732\n","THE MIN SNR IS:  20.07560968399048\n","THE MEDIAN SNR IS:  23.65769863128662\n"]}],"source":["# Define the folder path containing audio files\n","predict_folder_path = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/Predict\"\n","test_dataset_path = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/Dataset/test\"\n","\n","# Initialize total SNR, max SNR, min SNR, and length of the folder\n","total_SNR = 0\n","max_SNR = float('-inf')\n","min_SNR = float('inf')\n","SNR_values = []\n","\n","length = len(os.listdir(predict_folder_path))\n","progress = 0\n","\n","# Iterate through each audio file in the folder\n","for folder_name in os.listdir(predict_folder_path):\n","    # Remove the \"_predict\" part\n","    folder_name_without_predict = folder_name[:-8]\n","\n","    # Load the audio files\n","    predicted_speech, sample_rate_predict = librosa.load(os.path.join(predict_folder_path, folder_name, 'mixture_vocals.wav'), sr=None)\n","    test_clean_speech, sample_rate_test = librosa.load(os.path.join(test_dataset_path, folder_name_without_predict, 'vocals.wav'), sr=None)\n","\n","    # Calculate estimated noise by subtracting original clean speech from predicted speech\n","    est_result_noise = predicted_speech - test_clean_speech\n","\n","    # Calculate SNR for the current audio file\n","    curr_SNR = calc_SNR(test_clean_speech, est_result_noise)\n","\n","    # Accumulate SNR for mean calculation\n","    total_SNR += curr_SNR\n","\n","    # Update max and min SNR\n","    max_SNR = max(max_SNR, curr_SNR)\n","    min_SNR = min(min_SNR, curr_SNR)\n","\n","    # Append current SNR to the list\n","    SNR_values.append(curr_SNR)\n","\n","    progress += 1  # Increment progress counter\n","    progress_percentage = (progress / length) * 100  # Calculate progress percentage\n","    print(f\"Progress: {progress}/{length} ({progress_percentage:.2f}%)\")  # Print progress\n","\n","# Calculate mean SNR\n","mean_SNR = total_SNR / length\n","\n","# Calculate median SNR\n","median_SNR = np.median(SNR_values)\n","\n","# Print the mean, max, min, and median SNR\n","print('\\n')\n","print('THE MEAN SNR IS: ', mean_SNR)\n","print('THE MAX SNR IS: ', max_SNR)\n","print('THE MIN SNR IS: ', min_SNR)\n","print('THE MEDIAN SNR IS: ', median_SNR)"]},{"cell_type":"markdown","metadata":{"id":"C8YNmjibuc9-"},"source":["# Experimenting with the Model"]},{"cell_type":"markdown","metadata":{"id":"XAfOvsLAltA1"},"source":["## Experiment One: SNR Robustness Check\n","Start by generating data with different SNR values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Zxfjh9TZ_3f"},"outputs":[],"source":["# generate SNR 10\n","!python3 \"/content/drive/MyDrive/SoundAI - Final/MS-SNSD/noisyspeech_synthesizer_10_SNR.py\" --cfg noisyspeech_synthesizer_10_SNR.cfg\n","\n","# generate SNR 5\n","!python3 \"/content/drive/MyDrive/SoundAI - Final/MS-SNSD/noisyspeech_synthesizer_5_SNR.py\" --cfg noisyspeech_synthesizer_5_SNR.cfg\n","\n","# generate SNR 0\n","!python3 \"/content/drive/MyDrive/SoundAI - Final/MS-SNSD/noisyspeech_synthesizer_0_SNR.py\" --cfg noisyspeech_synthesizer_0_SNR.cfg\n"]},{"cell_type":"markdown","metadata":{"id":"t8Lym6qfznua"},"source":["Edit the MS-SNSD data to look like the MUSDB18 data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYx-AyQFzlTH"},"outputs":[],"source":["def run_process_samples_for_snr_levels(snr_dict, dataset_type):\n","    for snr_level, paths in snr_dict.items():\n","        # Extract paths for the current SNR level\n","        noisy_speech_path, clean_speech_path, noise_path, dataset_path = paths\n","\n","        print(f\"Processing {snr_level}: clean_path={clean_speech_path}, noisy_path={noisy_speech_path}, noise_path={noise_path}, dataset_path={dataset_path}\")\n","\n","        # Call process_samples with the extracted paths and the dataset type for the current SNR level\n","        process_samples(dataset_path, clean_speech_path, noisy_speech_path, noise_path, dataset_type)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFEPO2eAz1Ly"},"outputs":[],"source":["destination_base_path = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset\"\n","data_base_path = \"/content/drive/MyDrive/SoundAI - Final/MS-SNSD\"\n","\n","snr_dirs = [os.path.join(data_base_path, snr) for snr in ['SNR_0', 'SNR_5', 'SNR_10']]\n","destination_paths = [os.path.join(destination_base_path, snr) for snr in ['SNR_0', 'SNR_5', 'SNR_10']]\n","\n","# Generate SNR paths\n","snr_paths = [[os.path.join(snr_dir, dir) for dir in os.listdir(snr_dir)] + [destination] for snr_dir, destination in zip(snr_dirs, destination_paths)]\n","\n","# Creating a dictionary to map SNR levels to their respective paths\n","snr_dict = dict(zip(['SNR_0', 'SNR_5', 'SNR_10'], snr_paths))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710855915314,"user":{"displayName":"Omri Newman","userId":"03939389287432353270"},"user_tz":-120},"id":"xbeYeQWT0JTD","outputId":"8cffd73b-7677-4b80-a40c-b5240fc28462"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing SNR_0: clean_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/SNR_0/CleanSpeech_training, noisy_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/SNR_0/NoisySpeech_training, noise_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/SNR_0/Noise_training, dataset_path=/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset/SNR_0\n","Dataset_type folder already exists. Exiting...\n","Processing SNR_5: clean_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/SNR_5/CleanSpeech_training, noisy_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/SNR_5/NoisySpeech_training, noise_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/SNR_5/Noise_training, dataset_path=/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset/SNR_5\n","Dataset_type folder already exists. Exiting...\n","Processing SNR_10: clean_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/SNR_10/CleanSpeech_training, noisy_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/SNR_10/NoisySpeech_training, noise_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/SNR_10/Noise_training, dataset_path=/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset/SNR_10\n","Dataset_type folder already exists. Exiting...\n"]}],"source":["run_process_samples_for_snr_levels(snr_dict, 'test')"]},{"cell_type":"markdown","metadata":{"id":"19h6IIlp22Wa"},"source":[" Predict each SNR value using FT_ALL, trained on 20SNR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T30s9nXO1E9R"},"outputs":[],"source":["def run_prediction(path_to_predict, model_path, test_file_path, test_output_dir):\n","    command = [\n","        \"python\", path_to_predict,\n","        \"--load_model\", model_path,\n","        \"--input\", test_file_path,\n","        \"--output\", test_output_dir,\n","        \"--sr\", \"16000\"\n","        # \"--cuda\"  # Uncomment if CUDA is to be used\n","    ]\n","    subprocess.run(command, check=True)\n","\n","def predict_snr_level(path_to_predict, model_path, test_dir, output_dir):\n","    \"\"\"\n","    Process an SNR level by running predictions on the test files and saving the results in the output directory.\n","\n","    :param model_path: Path to the model checkpoint.\n","    :param test_dir: Directory containing test files for an SNR level.\n","    :param output_dir: Directory where predictions will be saved for an SNR level.\n","    \"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","    for test_file in os.listdir(test_dir):\n","        test_file_path = os.path.join(test_dir, test_file, 'mixture.wav')\n","        if os.path.exists(test_file_path):\n","            test_output_dir = os.path.join(output_dir, test_file + \"_predict\")\n","            os.makedirs(test_output_dir, exist_ok=True)\n","            try:\n","                run_prediction(path_to_predict, model_path, test_file_path, test_output_dir)\n","            except subprocess.CalledProcessError as e:\n","                print(f\"Error during processing {test_file_path}: {e}\")\n","        else:\n","            print(f\"Skipping {test_file_path}, file not found.\")\n","\n","def process_all_snr_levels(path_to_predict, model_path, snr_levels, base_dataset_dir, base_output_dir):\n","    \"\"\"\n","    Process all given SNR levels.\n","\n","    :param model_path: Path to the model checkpoint.\n","    :param snr_levels: List of SNR levels to process.\n","    :param base_dataset_dir: Base directory containing SNR subdirectories for the dataset.\n","    :param base_output_dir: Base directory where SNR subdirectories for predictions will be created.\n","    \"\"\"\n","    for snr in snr_levels:\n","        test_dir = os.path.join(base_dataset_dir, snr, \"test\")\n","        output_dir = os.path.join(base_output_dir, snr)\n","        predict_snr_level(path_to_predict, model_path, test_dir, output_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iiu3Odsu3BdD"},"outputs":[],"source":["model_path = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/checkpoints/FT_ALL/model\"\n","base_dataset_dir = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset\"\n","base_output_dir = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Predict\"\n","path_to_predict = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/predict.py\"\n","snr_levels = ['SNR_0', 'SNR_5', 'SNR_10']\n","\n","process_all_snr_levels(path_to_predict, model_path, snr_levels, base_dataset_dir, base_output_dir)"]},{"cell_type":"markdown","metadata":{"id":"FO-RVLQUDOpp"},"source":["Evaluate the models performance on lower SNR values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgMNhXCH9Z6y"},"outputs":[],"source":["def calc_SNR(speech, noise):\n","    \"\"\"\n","    Calculate Signal-to-Noise Ratio (SNR) between speech and noise signals.\n","\n","    Args:\n","    - speech (numpy.ndarray): Array containing speech signal.\n","    - noise (numpy.ndarray): Array containing noise signal.\n","\n","    Returns:\n","    - snr (float): Signal-to-Noise Ratio (SNR) in decibels (dB).\n","    \"\"\"\n","\n","    # Calculate the power of the noise signal\n","    power_noise = np.mean(noise ** 2)\n","\n","    # Print the power of the noise signal (optional)\n","    # print(power_noise)\n","\n","    # Define the threshold for detecting non-silent segments in the speech audio\n","    silence_threshold = 1.1 * np.median(np.abs(noise))  # Adjust this threshold as needed\n","\n","    # Find indices of non-silent segments in the speech audio\n","    non_silent_indices = np.where(speech ** 2 > silence_threshold)\n","\n","    # Calculate the power of non-silent speech segments\n","    power_non_silent_speech = np.mean(speech[non_silent_indices] ** 2)\n","\n","    # Print the power of non-silent speech segments (optional)\n","    # print(power_non_silent_speech)\n","\n","    # Calculate SNR in decibels (dB)\n","    snr = 10 * np.log10(power_non_silent_speech / power_noise)\n","\n","    # Print SNR (optional)\n","    # print(snr)\n","\n","    return snr\n","\n","def process_SNR_calculation(predict_base_path, test_base_path, snr_values):\n","    snr_results = {}\n","\n","    for snr in snr_values:\n","        predict_folder_path = os.path.join(predict_base_path, snr)\n","        test_folder_path = os.path.join(test_base_path, snr, 'test')\n","\n","        total_SNR, max_SNR, min_SNR = 0, float('-inf'), float('inf')\n","        SNR_values, progress = [], 0\n","\n","        for folder_name in os.listdir(predict_folder_path):\n","            predicted_file_path = os.path.join(predict_folder_path, folder_name, 'mixture_vocals.wav')\n","            test_file_path = os.path.join(test_folder_path, folder_name.replace('_predict', ''), 'vocals.wav')\n","\n","            if os.path.exists(predicted_file_path) and os.path.exists(test_file_path):\n","                predicted_speech, _ = librosa.load(predicted_file_path, sr=None)\n","                test_clean_speech, _ = librosa.load(test_file_path, sr=None)\n","\n","                est_noise = predicted_speech - test_clean_speech\n","                curr_SNR = calc_SNR(test_clean_speech, est_noise)\n","\n","                total_SNR += curr_SNR\n","                max_SNR = max(max_SNR, curr_SNR)\n","                min_SNR = min(min_SNR, curr_SNR)\n","                SNR_values.append(curr_SNR)\n","\n","                progress += 1\n","                print(f\"Processing {folder_name} in {snr}: SNR = {curr_SNR:.2f} dB\")\n","\n","        # Aggregate results for current SNR\n","        mean_SNR = total_SNR / progress if progress else 0\n","        median_SNR = np.median(SNR_values) if SNR_values else 0\n","\n","        # Store results in dictionary\n","        snr_results[snr] = {\n","            'mean_SNR': mean_SNR,\n","            'max_SNR': max_SNR,\n","            'min_SNR': min_SNR,\n","            'median_SNR': median_SNR\n","        }\n","\n","    return snr_results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9591,"status":"ok","timestamp":1710858075147,"user":{"displayName":"Omri Newman","userId":"03939389287432353270"},"user_tz":-120},"id":"UwQY1bogDT6-","outputId":"69357e1b-6ebf-4b95-8f01-db4ccdea23e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing noisy1_SNRdb_0.0_clnsp1_predict in SNR_0: SNR = 14.57 dB\n","Processing noisy2_SNRdb_0.0_clnsp2_predict in SNR_0: SNR = 13.38 dB\n","Processing noisy3_SNRdb_0.0_clnsp3_predict in SNR_0: SNR = 15.43 dB\n","Processing noisy4_SNRdb_0.0_clnsp4_predict in SNR_0: SNR = 18.43 dB\n","Processing noisy5_SNRdb_0.0_clnsp5_predict in SNR_0: SNR = 15.97 dB\n","Processing noisy6_SNRdb_0.0_clnsp6_predict in SNR_0: SNR = 17.95 dB\n","Processing noisy7_SNRdb_0.0_clnsp7_predict in SNR_0: SNR = 16.40 dB\n","Processing noisy8_SNRdb_0.0_clnsp8_predict in SNR_0: SNR = 11.65 dB\n","Processing noisy9_SNRdb_0.0_clnsp9_predict in SNR_0: SNR = 14.46 dB\n","Processing noisy10_SNRdb_0.0_clnsp10_predict in SNR_0: SNR = 13.48 dB\n","Processing noisy11_SNRdb_0.0_clnsp11_predict in SNR_0: SNR = 18.00 dB\n","Processing noisy12_SNRdb_0.0_clnsp12_predict in SNR_0: SNR = 14.77 dB\n","Processing noisy13_SNRdb_0.0_clnsp13_predict in SNR_0: SNR = 16.15 dB\n","Processing noisy14_SNRdb_0.0_clnsp14_predict in SNR_0: SNR = 11.85 dB\n","Processing noisy15_SNRdb_0.0_clnsp15_predict in SNR_0: SNR = 13.76 dB\n","Processing noisy16_SNRdb_0.0_clnsp16_predict in SNR_0: SNR = 17.04 dB\n","Processing noisy17_SNRdb_0.0_clnsp17_predict in SNR_0: SNR = 12.18 dB\n","Processing noisy18_SNRdb_0.0_clnsp18_predict in SNR_0: SNR = 12.31 dB\n","Processing noisy19_SNRdb_0.0_clnsp19_predict in SNR_0: SNR = 16.56 dB\n","Processing noisy20_SNRdb_0.0_clnsp20_predict in SNR_0: SNR = 12.81 dB\n","Processing noisy21_SNRdb_0.0_clnsp21_predict in SNR_0: SNR = 16.96 dB\n","Processing noisy22_SNRdb_0.0_clnsp22_predict in SNR_0: SNR = 14.26 dB\n","Processing noisy23_SNRdb_0.0_clnsp23_predict in SNR_0: SNR = 17.90 dB\n","Processing noisy24_SNRdb_0.0_clnsp24_predict in SNR_0: SNR = 12.19 dB\n","Processing noisy1_SNRdb_5.0_clnsp1_predict in SNR_5: SNR = 17.82 dB\n","Processing noisy2_SNRdb_5.0_clnsp2_predict in SNR_5: SNR = 15.42 dB\n","Processing noisy3_SNRdb_5.0_clnsp3_predict in SNR_5: SNR = 16.25 dB\n","Processing noisy4_SNRdb_5.0_clnsp4_predict in SNR_5: SNR = 13.56 dB\n","Processing noisy5_SNRdb_5.0_clnsp5_predict in SNR_5: SNR = 18.49 dB\n","Processing noisy6_SNRdb_5.0_clnsp6_predict in SNR_5: SNR = 14.97 dB\n","Processing noisy7_SNRdb_5.0_clnsp7_predict in SNR_5: SNR = 12.94 dB\n","Processing noisy8_SNRdb_5.0_clnsp8_predict in SNR_5: SNR = 14.32 dB\n","Processing noisy9_SNRdb_5.0_clnsp9_predict in SNR_5: SNR = 17.74 dB\n","Processing noisy10_SNRdb_5.0_clnsp10_predict in SNR_5: SNR = 14.85 dB\n","Processing noisy11_SNRdb_5.0_clnsp11_predict in SNR_5: SNR = 16.96 dB\n","Processing noisy12_SNRdb_5.0_clnsp12_predict in SNR_5: SNR = 12.46 dB\n","Processing noisy13_SNRdb_5.0_clnsp13_predict in SNR_5: SNR = 18.47 dB\n","Processing noisy14_SNRdb_5.0_clnsp14_predict in SNR_5: SNR = 15.16 dB\n","Processing noisy15_SNRdb_5.0_clnsp15_predict in SNR_5: SNR = 15.69 dB\n","Processing noisy16_SNRdb_5.0_clnsp16_predict in SNR_5: SNR = 15.90 dB\n","Processing noisy17_SNRdb_5.0_clnsp17_predict in SNR_5: SNR = 16.58 dB\n","Processing noisy18_SNRdb_5.0_clnsp18_predict in SNR_5: SNR = 15.39 dB\n","Processing noisy19_SNRdb_5.0_clnsp19_predict in SNR_5: SNR = 16.97 dB\n","Processing noisy20_SNRdb_5.0_clnsp20_predict in SNR_5: SNR = 16.76 dB\n","Processing noisy21_SNRdb_5.0_clnsp21_predict in SNR_5: SNR = 17.04 dB\n","Processing noisy22_SNRdb_5.0_clnsp22_predict in SNR_5: SNR = 18.77 dB\n","Processing noisy23_SNRdb_5.0_clnsp23_predict in SNR_5: SNR = 17.06 dB\n","Processing noisy24_SNRdb_5.0_clnsp24_predict in SNR_5: SNR = 17.95 dB\n","Processing noisy25_SNRdb_5.0_clnsp25_predict in SNR_5: SNR = 17.57 dB\n","Processing noisy1_SNRdb_10.0_clnsp1_predict in SNR_10: SNR = 17.90 dB\n","Processing noisy2_SNRdb_10.0_clnsp2_predict in SNR_10: SNR = 16.93 dB\n","Processing noisy3_SNRdb_10.0_clnsp3_predict in SNR_10: SNR = 18.98 dB\n","Processing noisy4_SNRdb_10.0_clnsp4_predict in SNR_10: SNR = 17.67 dB\n","Processing noisy5_SNRdb_10.0_clnsp5_predict in SNR_10: SNR = 14.31 dB\n","Processing noisy6_SNRdb_10.0_clnsp6_predict in SNR_10: SNR = 18.21 dB\n","Processing noisy7_SNRdb_10.0_clnsp7_predict in SNR_10: SNR = 18.12 dB\n","Processing noisy8_SNRdb_10.0_clnsp8_predict in SNR_10: SNR = 13.27 dB\n","Processing noisy9_SNRdb_10.0_clnsp9_predict in SNR_10: SNR = 15.82 dB\n","Processing noisy10_SNRdb_10.0_clnsp10_predict in SNR_10: SNR = 16.52 dB\n","Processing noisy11_SNRdb_10.0_clnsp11_predict in SNR_10: SNR = 16.95 dB\n","Processing noisy12_SNRdb_10.0_clnsp12_predict in SNR_10: SNR = 14.86 dB\n","Processing noisy13_SNRdb_10.0_clnsp13_predict in SNR_10: SNR = 17.32 dB\n","Processing noisy14_SNRdb_10.0_clnsp14_predict in SNR_10: SNR = 18.92 dB\n","Processing noisy15_SNRdb_10.0_clnsp15_predict in SNR_10: SNR = 16.83 dB\n","Processing noisy16_SNRdb_10.0_clnsp16_predict in SNR_10: SNR = 17.46 dB\n","Processing noisy17_SNRdb_10.0_clnsp17_predict in SNR_10: SNR = 15.92 dB\n","Processing noisy18_SNRdb_10.0_clnsp18_predict in SNR_10: SNR = 16.90 dB\n","Processing noisy19_SNRdb_10.0_clnsp19_predict in SNR_10: SNR = 17.06 dB\n","Processing noisy20_SNRdb_10.0_clnsp20_predict in SNR_10: SNR = 16.99 dB\n","Processing noisy21_SNRdb_10.0_clnsp21_predict in SNR_10: SNR = 17.06 dB\n","Processing noisy22_SNRdb_10.0_clnsp22_predict in SNR_10: SNR = 17.45 dB\n","Processing noisy23_SNRdb_10.0_clnsp23_predict in SNR_10: SNR = 21.38 dB\n","Processing noisy24_SNRdb_10.0_clnsp24_predict in SNR_10: SNR = 16.18 dB\n","Results for SNR_0:\n","Mean SNR: 14.93602121869723 dB\n","Max SNR: 18.42773199081421 dB\n","Min SNR: 11.649466753005981 dB\n","Median SNR: 14.67245101928711 dB\n","\n","\n","Results for SNR_5:\n","Mean SNR: 16.203747510910034 dB\n","Max SNR: 18.77019762992859 dB\n","Min SNR: 12.464462518692017 dB\n","Median SNR: 16.582494974136353 dB\n","\n","\n","Results for SNR_10:\n","Mean SNR: 17.04232782125473 dB\n","Max SNR: 21.381378173828125 dB\n","Min SNR: 13.271970748901367 dB\n","Median SNR: 17.021881341934204 dB\n","\n","\n"]}],"source":["predict_base_path = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Predict\"\n","test_base_path = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset\"\n","snr_values = ['SNR_0', 'SNR_5', 'SNR_10']\n","\n","snr_results = process_SNR_calculation(predict_base_path, test_base_path, snr_values)\n","\n","# Print the results\n","for snr, results in snr_results.items():\n","    print(f\"Results for {snr}:\")\n","    print(f\"Mean SNR: {results['mean_SNR']} dB\")\n","    print(f\"Max SNR: {results['max_SNR']} dB\")\n","    print(f\"Min SNR: {results['min_SNR']} dB\")\n","    print(f\"Median SNR: {results['median_SNR']} dB\")\n","    print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"Kgux1svLllL6"},"source":["## Experiment Two: Verifying Specific Background Noises\n","\n","Isolating and comparing model performance on different background noises"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Hb_FLSLFlOT"},"outputs":[],"source":["# generate files with typing as a background noise only\n","!python3 \"/content/drive/MyDrive/SoundAI - Final/MS-SNSD/noisyspeech_synthesizer_typing.py\" --cfg noisyspeech_synthesizer_typing.cfg\n","\n","# generate files with airport announcements as a background noise only\n","!python3 \"/content/drive/MyDrive/SoundAI - Final/MS-SNSD/noisyspeech_synthesizer_airport.py\" --cfg noisyspeech_synthesizer_airport_announcement.cfg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r2boo5Wwm8hh"},"outputs":[],"source":["destination_base_path = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset\"\n","data_base_path = \"/content/drive/MyDrive/SoundAI - Final/MS-SNSD\"\n","\n","background_noise_dirs = [os.path.join(data_base_path, background_noise) for background_noise in ['AirportAnnouncement', 'Typing']]\n","destination_paths = [os.path.join(destination_base_path, background_noise) for background_noise in ['AirportAnnouncement', 'Typing']]\n","\n","# Generate background noise paths\n","background_noise_paths = [[os.path.join(background_noise_dirs, dir) for dir in os.listdir(background_noise_dirs)] + [destination] for background_noise_dirs, destination in zip(background_noise_dirs, destination_paths)]\n","\n","# Creating a dictionary to map background noises to their respective paths\n","background_noise_dict = dict(zip(['AirportAnnouncement', 'Typing'], background_noise_paths))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":572,"status":"ok","timestamp":1710867317207,"user":{"displayName":"Omri Newman","userId":"03939389287432353270"},"user_tz":-120},"id":"iglrwIlOo2xW","outputId":"e9baad18-2880-4f41-e026-18b6a144a340"},"outputs":[{"data":{"text/plain":["{'AirportAnnouncement': ['/content/drive/MyDrive/SoundAI - Final/MS-SNSD/AirportAnnouncement/NoisySpeech_training',\n","  '/content/drive/MyDrive/SoundAI - Final/MS-SNSD/AirportAnnouncement/CleanSpeech_training',\n","  '/content/drive/MyDrive/SoundAI - Final/MS-SNSD/AirportAnnouncement/Noise_training',\n","  '/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset/AirportAnnouncement'],\n"," 'Typing': ['/content/drive/MyDrive/SoundAI - Final/MS-SNSD/Typing/NoisySpeech_training',\n","  '/content/drive/MyDrive/SoundAI - Final/MS-SNSD/Typing/CleanSpeech_training',\n","  '/content/drive/MyDrive/SoundAI - Final/MS-SNSD/Typing/Noise_training',\n","  '/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset/Typing']}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["background_noise_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WLCDAqFpUo2"},"outputs":[],"source":["def run_process_samples_for_background_noise(background_noise_dict, dataset_type):\n","    for background_noise, paths in background_noise_dict.items():\n","        # Extract paths for the current SNR level\n","        noisy_speech_path, clean_speech_path, noise_path, dataset_path = paths\n","\n","        print(f\"Processing {background_noise}: clean_path={clean_speech_path}, noisy_path={noisy_speech_path}, noise_path={noise_path}, dataset_path={dataset_path}\")\n","\n","        # Call process_samples with the extracted paths and the dataset type for the current SNR level\n","        process_samples(dataset_path, clean_speech_path, noisy_speech_path, noise_path, dataset_type)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24968,"status":"ok","timestamp":1710867506242,"user":{"displayName":"Omri Newman","userId":"03939389287432353270"},"user_tz":-120},"id":"JIuH77IHo3zp","outputId":"5135ae1c-4605-4e35-c3c7-a6f5ab9b856c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing AirportAnnouncement: clean_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/AirportAnnouncement/CleanSpeech_training, noisy_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/AirportAnnouncement/NoisySpeech_training, noise_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/AirportAnnouncement/Noise_training, dataset_path=/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset/AirportAnnouncement\n","Total Number of Samples (N): 24\n","Number of Samples Processed: 1\n","Number of Samples Processed: 2\n","Number of Samples Processed: 3\n","Number of Samples Processed: 4\n","Number of Samples Processed: 5\n","Number of Samples Processed: 6\n","Number of Samples Processed: 7\n","Number of Samples Processed: 8\n","Number of Samples Processed: 9\n","Number of Samples Processed: 10\n","Number of Samples Processed: 11\n","Number of Samples Processed: 12\n","Number of Samples Processed: 13\n","Number of Samples Processed: 14\n","Number of Samples Processed: 15\n","Number of Samples Processed: 16\n","Number of Samples Processed: 17\n","Number of Samples Processed: 18\n","Number of Samples Processed: 19\n","Number of Samples Processed: 20\n","Number of Samples Processed: 21\n","Number of Samples Processed: 22\n","Number of Samples Processed: 23\n","Number of Samples Processed: 24\n","Processing Typing: clean_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/Typing/CleanSpeech_training, noisy_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/Typing/NoisySpeech_training, noise_path=/content/drive/MyDrive/SoundAI - Final/MS-SNSD/Typing/Noise_training, dataset_path=/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset/Typing\n","Total Number of Samples (N): 25\n","Number of Samples Processed: 1\n","Number of Samples Processed: 2\n","Number of Samples Processed: 3\n","Number of Samples Processed: 4\n","Number of Samples Processed: 5\n","Number of Samples Processed: 6\n","Number of Samples Processed: 7\n","Number of Samples Processed: 8\n","Number of Samples Processed: 9\n","Number of Samples Processed: 10\n","Number of Samples Processed: 11\n","Number of Samples Processed: 12\n","Number of Samples Processed: 13\n","Number of Samples Processed: 14\n","Number of Samples Processed: 15\n","Number of Samples Processed: 16\n","Number of Samples Processed: 17\n","Number of Samples Processed: 18\n","Number of Samples Processed: 19\n","Number of Samples Processed: 20\n","Number of Samples Processed: 21\n","Number of Samples Processed: 22\n","Number of Samples Processed: 23\n","Number of Samples Processed: 24\n","Number of Samples Processed: 25\n"]}],"source":["run_process_samples_for_background_noise(background_noise_dict, 'test')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6QVpxEjupf4L"},"outputs":[],"source":["model_path = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/checkpoints/FT_ALL/model\"\n","base_dataset_dir = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset\"\n","base_output_dir = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Predict\"\n","path_to_predict = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/predict.py\"\n","background_noises = ['AirportAnnouncement', 'Typing']\n","\n","process_all_snr_levels(path_to_predict, model_path, background_noises, base_dataset_dir, base_output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99lpt5Hfp5JE"},"outputs":[],"source":["def process_SNR_calculation_background_noise(predict_base_path, test_base_path, background_noises):\n","    background_noises_results = {}\n","\n","    for background_noise in background_noises:\n","        predict_folder_path = os.path.join(predict_base_path, background_noise)\n","        test_folder_path = os.path.join(test_base_path, background_noise, 'test')\n","\n","        total_SNR, max_SNR, min_SNR = 0, float('-inf'), float('inf')\n","        SNR_values, progress = [], 0\n","\n","        for folder_name in os.listdir(predict_folder_path):\n","            predicted_file_path = os.path.join(predict_folder_path, folder_name, 'mixture_vocals.wav')\n","            test_file_path = os.path.join(test_folder_path, folder_name.replace('_predict', ''), 'vocals.wav')\n","\n","            if os.path.exists(predicted_file_path) and os.path.exists(test_file_path):\n","                predicted_speech, _ = librosa.load(predicted_file_path, sr=None)\n","                test_clean_speech, _ = librosa.load(test_file_path, sr=None)\n","\n","                est_noise = predicted_speech - test_clean_speech\n","                curr_SNR = calc_SNR(test_clean_speech, est_noise)\n","\n","                total_SNR += curr_SNR\n","                max_SNR = max(max_SNR, curr_SNR)\n","                min_SNR = min(min_SNR, curr_SNR)\n","                SNR_values.append(curr_SNR)\n","\n","                progress += 1\n","                print(f\"Processing {folder_name} in {background_noise}: SNR = {curr_SNR:.2f} dB\")\n","\n","        # Aggregate results for current SNR\n","        mean_SNR = total_SNR / progress if progress else 0\n","        median_SNR = np.median(SNR_values) if SNR_values else 0\n","\n","        # Store results in dictionary\n","        background_noises_results[background_noise] = {\n","            'mean_SNR': mean_SNR,\n","            'max_SNR': max_SNR,\n","            'min_SNR': min_SNR,\n","            'median_SNR': median_SNR\n","        }\n","\n","    return background_noises_results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2892,"status":"ok","timestamp":1710869493011,"user":{"displayName":"Omri Newman","userId":"03939389287432353270"},"user_tz":-120},"id":"FABCzSwIw0CI","outputId":"d6b6d8eb-5ee6-4c22-d753-cb32b98b48dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing noisy1_SNRdb_20.0_clnsp1_predict in AirportAnnouncement: SNR = 19.81 dB\n","Processing noisy2_SNRdb_20.0_clnsp2_predict in AirportAnnouncement: SNR = 16.36 dB\n","Processing noisy3_SNRdb_20.0_clnsp3_predict in AirportAnnouncement: SNR = 17.53 dB\n","Processing noisy4_SNRdb_20.0_clnsp4_predict in AirportAnnouncement: SNR = 18.37 dB\n","Processing noisy5_SNRdb_20.0_clnsp5_predict in AirportAnnouncement: SNR = 18.84 dB\n","Processing noisy6_SNRdb_20.0_clnsp6_predict in AirportAnnouncement: SNR = 17.48 dB\n","Processing noisy7_SNRdb_20.0_clnsp7_predict in AirportAnnouncement: SNR = 18.93 dB\n","Processing noisy8_SNRdb_20.0_clnsp8_predict in AirportAnnouncement: SNR = 18.18 dB\n","Processing noisy9_SNRdb_20.0_clnsp9_predict in AirportAnnouncement: SNR = 17.85 dB\n","Processing noisy10_SNRdb_20.0_clnsp10_predict in AirportAnnouncement: SNR = 18.32 dB\n","Processing noisy11_SNRdb_20.0_clnsp11_predict in AirportAnnouncement: SNR = 18.68 dB\n","Processing noisy12_SNRdb_20.0_clnsp12_predict in AirportAnnouncement: SNR = 17.94 dB\n","Processing noisy13_SNRdb_20.0_clnsp13_predict in AirportAnnouncement: SNR = 18.10 dB\n","Processing noisy14_SNRdb_20.0_clnsp14_predict in AirportAnnouncement: SNR = 14.50 dB\n","Processing noisy15_SNRdb_20.0_clnsp15_predict in AirportAnnouncement: SNR = 18.34 dB\n","Processing noisy16_SNRdb_20.0_clnsp16_predict in AirportAnnouncement: SNR = 20.26 dB\n","Processing noisy17_SNRdb_20.0_clnsp17_predict in AirportAnnouncement: SNR = 18.95 dB\n","Processing noisy18_SNRdb_20.0_clnsp18_predict in AirportAnnouncement: SNR = 18.72 dB\n","Processing noisy19_SNRdb_20.0_clnsp19_predict in AirportAnnouncement: SNR = 19.53 dB\n","Processing noisy20_SNRdb_20.0_clnsp20_predict in AirportAnnouncement: SNR = 16.52 dB\n","Processing noisy21_SNRdb_20.0_clnsp21_predict in AirportAnnouncement: SNR = 17.39 dB\n","Processing noisy22_SNRdb_20.0_clnsp22_predict in AirportAnnouncement: SNR = 19.15 dB\n","Processing noisy23_SNRdb_20.0_clnsp23_predict in AirportAnnouncement: SNR = 17.53 dB\n","Processing noisy24_SNRdb_20.0_clnsp24_predict in AirportAnnouncement: SNR = 18.89 dB\n","Processing noisy1_SNRdb_20.0_clnsp1_predict in Typing: SNR = 18.29 dB\n","Processing noisy2_SNRdb_20.0_clnsp2_predict in Typing: SNR = 20.04 dB\n","Processing noisy3_SNRdb_20.0_clnsp3_predict in Typing: SNR = 18.31 dB\n","Processing noisy4_SNRdb_20.0_clnsp4_predict in Typing: SNR = 19.14 dB\n","Processing noisy5_SNRdb_20.0_clnsp5_predict in Typing: SNR = 16.96 dB\n","Processing noisy6_SNRdb_20.0_clnsp6_predict in Typing: SNR = 15.08 dB\n","Processing noisy7_SNRdb_20.0_clnsp7_predict in Typing: SNR = 16.82 dB\n","Processing noisy8_SNRdb_20.0_clnsp8_predict in Typing: SNR = 20.58 dB\n","Processing noisy9_SNRdb_20.0_clnsp9_predict in Typing: SNR = 19.25 dB\n","Processing noisy10_SNRdb_20.0_clnsp10_predict in Typing: SNR = 16.90 dB\n","Processing noisy11_SNRdb_20.0_clnsp11_predict in Typing: SNR = 19.45 dB\n","Processing noisy12_SNRdb_20.0_clnsp12_predict in Typing: SNR = 17.43 dB\n","Processing noisy13_SNRdb_20.0_clnsp13_predict in Typing: SNR = 16.32 dB\n","Processing noisy14_SNRdb_20.0_clnsp14_predict in Typing: SNR = 16.66 dB\n","Processing noisy15_SNRdb_20.0_clnsp15_predict in Typing: SNR = 19.86 dB\n","Processing noisy16_SNRdb_20.0_clnsp16_predict in Typing: SNR = 19.58 dB\n","Processing noisy17_SNRdb_20.0_clnsp17_predict in Typing: SNR = 19.40 dB\n","Processing noisy18_SNRdb_20.0_clnsp18_predict in Typing: SNR = 19.33 dB\n","Processing noisy19_SNRdb_20.0_clnsp19_predict in Typing: SNR = 18.38 dB\n","Processing noisy20_SNRdb_20.0_clnsp20_predict in Typing: SNR = 18.77 dB\n","Processing noisy21_SNRdb_20.0_clnsp21_predict in Typing: SNR = 17.74 dB\n","Processing noisy22_SNRdb_20.0_clnsp22_predict in Typing: SNR = 18.87 dB\n","Processing noisy23_SNRdb_20.0_clnsp23_predict in Typing: SNR = 18.18 dB\n","Processing noisy24_SNRdb_20.0_clnsp24_predict in Typing: SNR = 19.14 dB\n","Processing noisy25_SNRdb_20.0_clnsp25_predict in Typing: SNR = 20.65 dB\n","Results for AirportAnnouncement:\n","Mean SNR: 18.173568844795227 dB\n","Max SNR: 20.259876251220703 dB\n","Min SNR: 14.504196643829346 dB\n","Median SNR: 18.33100438117981 dB\n","\n","\n","Results for Typing:\n","Mean SNR: 18.44564094543457 dB\n","Max SNR: 20.65002679824829 dB\n","Min SNR: 15.084694623947144 dB\n","Median SNR: 18.773479461669922 dB\n","\n","\n"]}],"source":["predict_base_path = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Predict\"\n","test_base_path = \"/content/drive/MyDrive/SoundAI - Final/Wave-U-Net-Pytorch/Dataset\"\n","background_noises = ['AirportAnnouncement', 'Typing']\n","\n","background_noises_results = process_SNR_calculation_background_noise(predict_base_path, test_base_path, background_noises)\n","\n","# Print the results\n","for background_noise, results in background_noises_results.items():\n","    print(f\"Results for {background_noise}:\")\n","    print(f\"Mean SNR: {results['mean_SNR']} dB\")\n","    print(f\"Max SNR: {results['max_SNR']} dB\")\n","    print(f\"Min SNR: {results['min_SNR']} dB\")\n","    print(f\"Median SNR: {results['median_SNR']} dB\")\n","    print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"Macygvi-DqjE"},"source":["# Compare Results on Pre-Trained U-NET Model\n","Predict results for fine tuning on Pre-trained model, and evaluate using SNR."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ixiGbE_vny9","outputId":"a49621f1-af84-48fa-f9bb-5a710226dca4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n","Using valid convolutions with 41641 inputs and 32089 outputs\n","Loading model from checkpoint /content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\n","Step 132065\n"]}],"source":["# Define the model path\n","model_path = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/checkpoints/waveunet/model\"\n","\n","# Define the test directory path\n","test_dir = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/Dataset/test\"\n","\n","# Define the output directory path\n","output_dir = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/Predict-pre-trained\"\n","\n","# Check if the output directory is empty\n","if not os.listdir(output_dir):\n","    # Iterate over the test files in the test directory\n","    for test_file in os.listdir(test_dir):\n","\n","        test_file_name = 'mixture.wav'\n","\n","        # Extract the test file name\n","        test_file_path = os.path.join(test_dir, test_file, test_file_name)\n","\n","        # Create directory for the current test file's predictions\n","        test_output_dir = os.path.join(output_dir, test_file + \"_predict\")\n","        os.makedirs(test_output_dir, exist_ok=True)\n","\n","        # Construct the command\n","        command = f\"python \\\"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/predict.py\\\" --load_model \\\"{model_path}\\\" --input \\\"{test_file_path}\\\" --output \\\"{test_output_dir}\\\" --sr 16000\"\n","\n","        # Execute the command\n","        !{command}\n","\n","else:\n","    print(\"Predict directory is not empty. Please clear the directory before running predictions.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SC3DEDAvny_","outputId":"d93c9b39-fe0d-4ab1-dc97-4b499fde53fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Progress: 1/192 (0.52%)\n","Progress: 2/192 (1.04%)\n","Progress: 3/192 (1.56%)\n","Progress: 4/192 (2.08%)\n","Progress: 5/192 (2.60%)\n","Progress: 6/192 (3.12%)\n","Progress: 7/192 (3.65%)\n","Progress: 8/192 (4.17%)\n","Progress: 9/192 (4.69%)\n","Progress: 10/192 (5.21%)\n","Progress: 11/192 (5.73%)\n","Progress: 12/192 (6.25%)\n","Progress: 13/192 (6.77%)\n","Progress: 14/192 (7.29%)\n","Progress: 15/192 (7.81%)\n","Progress: 16/192 (8.33%)\n","Progress: 17/192 (8.85%)\n","Progress: 18/192 (9.38%)\n","Progress: 19/192 (9.90%)\n","Progress: 20/192 (10.42%)\n","Progress: 21/192 (10.94%)\n","Progress: 22/192 (11.46%)\n","Progress: 23/192 (11.98%)\n","Progress: 24/192 (12.50%)\n","Progress: 25/192 (13.02%)\n","Progress: 26/192 (13.54%)\n","Progress: 27/192 (14.06%)\n","Progress: 28/192 (14.58%)\n","Progress: 29/192 (15.10%)\n","Progress: 30/192 (15.62%)\n","Progress: 31/192 (16.15%)\n","Progress: 32/192 (16.67%)\n","Progress: 33/192 (17.19%)\n","Progress: 34/192 (17.71%)\n","Progress: 35/192 (18.23%)\n","Progress: 36/192 (18.75%)\n","Progress: 37/192 (19.27%)\n","Progress: 38/192 (19.79%)\n","Progress: 39/192 (20.31%)\n","Progress: 40/192 (20.83%)\n","Progress: 41/192 (21.35%)\n","Progress: 42/192 (21.88%)\n","Progress: 43/192 (22.40%)\n","Progress: 44/192 (22.92%)\n","Progress: 45/192 (23.44%)\n","Progress: 46/192 (23.96%)\n","Progress: 47/192 (24.48%)\n","Progress: 48/192 (25.00%)\n","Progress: 49/192 (25.52%)\n","Progress: 50/192 (26.04%)\n","Progress: 51/192 (26.56%)\n","Progress: 52/192 (27.08%)\n","Progress: 53/192 (27.60%)\n","Progress: 54/192 (28.12%)\n","Progress: 55/192 (28.65%)\n","Progress: 56/192 (29.17%)\n","Progress: 57/192 (29.69%)\n","Progress: 58/192 (30.21%)\n","Progress: 59/192 (30.73%)\n","Progress: 60/192 (31.25%)\n","Progress: 61/192 (31.77%)\n","Progress: 62/192 (32.29%)\n","Progress: 63/192 (32.81%)\n","Progress: 64/192 (33.33%)\n","Progress: 65/192 (33.85%)\n","Progress: 66/192 (34.38%)\n","Progress: 67/192 (34.90%)\n","Progress: 68/192 (35.42%)\n","Progress: 69/192 (35.94%)\n","Progress: 70/192 (36.46%)\n","Progress: 71/192 (36.98%)\n","Progress: 72/192 (37.50%)\n","Progress: 73/192 (38.02%)\n","Progress: 74/192 (38.54%)\n","Progress: 75/192 (39.06%)\n","Progress: 76/192 (39.58%)\n","Progress: 77/192 (40.10%)\n","Progress: 78/192 (40.62%)\n","Progress: 79/192 (41.15%)\n","Progress: 80/192 (41.67%)\n","Progress: 81/192 (42.19%)\n","Progress: 82/192 (42.71%)\n","Progress: 83/192 (43.23%)\n","Progress: 84/192 (43.75%)\n","Progress: 85/192 (44.27%)\n","Progress: 86/192 (44.79%)\n","Progress: 87/192 (45.31%)\n","Progress: 88/192 (45.83%)\n","Progress: 89/192 (46.35%)\n","Progress: 90/192 (46.88%)\n","Progress: 91/192 (47.40%)\n","Progress: 92/192 (47.92%)\n","Progress: 93/192 (48.44%)\n","Progress: 94/192 (48.96%)\n","Progress: 95/192 (49.48%)\n","Progress: 96/192 (50.00%)\n","Progress: 97/192 (50.52%)\n","Progress: 98/192 (51.04%)\n","Progress: 99/192 (51.56%)\n","Progress: 100/192 (52.08%)\n","Progress: 101/192 (52.60%)\n","Progress: 102/192 (53.12%)\n","Progress: 103/192 (53.65%)\n","Progress: 104/192 (54.17%)\n","Progress: 105/192 (54.69%)\n","Progress: 106/192 (55.21%)\n","Progress: 107/192 (55.73%)\n","Progress: 108/192 (56.25%)\n","Progress: 109/192 (56.77%)\n","Progress: 110/192 (57.29%)\n","Progress: 111/192 (57.81%)\n","Progress: 112/192 (58.33%)\n","Progress: 113/192 (58.85%)\n","Progress: 114/192 (59.38%)\n","Progress: 115/192 (59.90%)\n","Progress: 116/192 (60.42%)\n","Progress: 117/192 (60.94%)\n","Progress: 118/192 (61.46%)\n","Progress: 119/192 (61.98%)\n","Progress: 120/192 (62.50%)\n","Progress: 121/192 (63.02%)\n","Progress: 122/192 (63.54%)\n","Progress: 123/192 (64.06%)\n","Progress: 124/192 (64.58%)\n","Progress: 125/192 (65.10%)\n","Progress: 126/192 (65.62%)\n","Progress: 127/192 (66.15%)\n","Progress: 128/192 (66.67%)\n","Progress: 129/192 (67.19%)\n","Progress: 130/192 (67.71%)\n","Progress: 131/192 (68.23%)\n","Progress: 132/192 (68.75%)\n","Progress: 133/192 (69.27%)\n","Progress: 134/192 (69.79%)\n","Progress: 135/192 (70.31%)\n","Progress: 136/192 (70.83%)\n","Progress: 137/192 (71.35%)\n","Progress: 138/192 (71.88%)\n","Progress: 139/192 (72.40%)\n","Progress: 140/192 (72.92%)\n","Progress: 141/192 (73.44%)\n","Progress: 142/192 (73.96%)\n","Progress: 143/192 (74.48%)\n","Progress: 144/192 (75.00%)\n","Progress: 145/192 (75.52%)\n","Progress: 146/192 (76.04%)\n","Progress: 147/192 (76.56%)\n","Progress: 148/192 (77.08%)\n","Progress: 149/192 (77.60%)\n","Progress: 150/192 (78.12%)\n","Progress: 151/192 (78.65%)\n","Progress: 152/192 (79.17%)\n","Progress: 153/192 (79.69%)\n","Progress: 154/192 (80.21%)\n","Progress: 155/192 (80.73%)\n","Progress: 156/192 (81.25%)\n","Progress: 157/192 (81.77%)\n","Progress: 158/192 (82.29%)\n","Progress: 159/192 (82.81%)\n","Progress: 160/192 (83.33%)\n","Progress: 161/192 (83.85%)\n","Progress: 162/192 (84.38%)\n","Progress: 163/192 (84.90%)\n","Progress: 164/192 (85.42%)\n","Progress: 165/192 (85.94%)\n","Progress: 166/192 (86.46%)\n","Progress: 167/192 (86.98%)\n","Progress: 168/192 (87.50%)\n","Progress: 169/192 (88.02%)\n","Progress: 170/192 (88.54%)\n","Progress: 171/192 (89.06%)\n","Progress: 172/192 (89.58%)\n","Progress: 173/192 (90.10%)\n","Progress: 174/192 (90.62%)\n","Progress: 175/192 (91.15%)\n","Progress: 176/192 (91.67%)\n","Progress: 177/192 (92.19%)\n","Progress: 178/192 (92.71%)\n","Progress: 179/192 (93.23%)\n","Progress: 180/192 (93.75%)\n","Progress: 181/192 (94.27%)\n","Progress: 182/192 (94.79%)\n","Progress: 183/192 (95.31%)\n","Progress: 184/192 (95.83%)\n","Progress: 185/192 (96.35%)\n","Progress: 186/192 (96.88%)\n","Progress: 187/192 (97.40%)\n","Progress: 188/192 (97.92%)\n","Progress: 189/192 (98.44%)\n","Progress: 190/192 (98.96%)\n","Progress: 191/192 (99.48%)\n","Progress: 192/192 (100.00%)\n","\n","\n","THE MEAN SNR IS:  18.4106986100475\n","THE MAX SNR IS:  23.724594116210938\n","THE MIN SNR IS:  9.22860324382782\n","THE MEDIAN SNR IS:  19.2257821559906\n"]}],"source":["# Define the folder path containing audio files\n","predict_folder_path = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/Predict-pre-trained\"\n","test_dataset_path = \"/content/drive/MyDrive/Reichman University /AI Audio/Final Project/Wave-U-Net-Pytorch/Dataset/test\"\n","\n","# Initialize total SNR, max SNR, min SNR, and length of the folder\n","total_SNR = 0\n","max_SNR = float('-inf')\n","min_SNR = float('inf')\n","SNR_values = []\n","\n","length = len(os.listdir(predict_folder_path))\n","progress = 0\n","\n","# Iterate through each audio file in the folder\n","for folder_name in os.listdir(predict_folder_path):\n","    # Remove the \"_predict\" part\n","    folder_name_without_predict = folder_name[:-8]\n","\n","    try:\n","\n","      # Load the audio files\n","      predicted_speech, sample_rate_predict = librosa.load(os.path.join(predict_folder_path, folder_name, 'mixture.wav_vocals.wav'), sr=None)\n","      test_clean_speech, sample_rate_test = librosa.load(os.path.join(test_dataset_path, folder_name_without_predict, 'vocals.wav'), sr=None)\n","\n","      # Calculate estimated noise by subtracting original clean speech from predicted speech\n","      est_result_noise = predicted_speech - test_clean_speech\n","\n","      # Calculate SNR for the current audio file\n","      curr_SNR = calc_SNR(test_clean_speech, est_result_noise)\n","\n","      # Accumulate SNR for mean calculation\n","      total_SNR += curr_SNR\n","\n","      # Update max and min SNR\n","      max_SNR = max(max_SNR, curr_SNR)\n","      min_SNR = min(min_SNR, curr_SNR)\n","\n","      # Append current SNR to the list\n","      SNR_values.append(curr_SNR)\n","\n","      progress += 1  # Increment progress counter\n","      progress_percentage = (progress / length) * 100  # Calculate progress percentage\n","      print(f\"Progress: {progress}/{length} ({progress_percentage:.2f}%)\")  # Print progress\n","\n","    except FileNotFoundError:\n","      print(f\"File not found: {folder_name}\")\n","      continue\n","\n","# Calculate mean SNR\n","mean_SNR = total_SNR / length\n","\n","# Calculate median SNR\n","median_SNR = np.median(SNR_values)\n","\n","# Print the mean, max, min, and median SNR\n","print('\\n')\n","print('THE MEAN SNR IS: ', mean_SNR)\n","print('THE MAX SNR IS: ', max_SNR)\n","print('THE MIN SNR IS: ', min_SNR)\n","print('THE MEDIAN SNR IS: ', median_SNR)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}